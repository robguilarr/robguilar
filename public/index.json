[{"categories":["Projects"],"content":"RAG application leveraging the Langchain framework and OpenAI's GPT-4, designed to enhance the onboarding experience for Pok√©mon Go players by providing a GenAI-powered index for improved game understanding and user retention","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"Abstract: This article introduces an E2E application aimed at enhancing the onboarding experience for new Pok√©mon Go players through a Langchain-framework-based app that integrates with the OpenAI API, leveraging GPT-4‚Äôs capabilities. Central to the application is a GenAI-powered Pok√©mon index, designed as a companion to reduce new user friction and deepen understanding of the game‚Äôs universe. By employing a Retrieval-Augmented Generation system with FAISS, Meta‚Äôs vector database, the app ensures information retrieval. With a workflow controlled entirely by a LLM, it offers functionalities such as Pok√©mon information, squad creation, and defense strategies. The app is aiming to improve user retention, widen the game‚Äôs demographic reach, and ease the learning curve. The project shows the potential of linking AI with mobile gaming to craft intuitive, engaging platforms to improve user retention and game understanding among newcomers. Looking for an interactive experience?\r\rüöÄ Clone the source code from Github, available here, then follow the instructions on the README file to install the Application on your local environment\r\r ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:0:0","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"‚ö†Ô∏è Introduction to problem Introducing a GenAI companion app for guiding new users in Pok√©mon Go makes a lot of sense when we look at the challenges the community faces. Pok√©mon Go isn‚Äôt just a game; it‚Äôs a phenomenon that has captured the attention of millions, combining engaging gameplay with real-world exploration. In 2023, the game boasted 145 million MAU, showcasing its massive appeal. Despite experiencing a dip in revenue in 2022, dropping to $0.77 billion from its peak, Pok√©mon Go continues to draw in a vast number of players as mentioned by Mansoor Iqbal. One key aspect of Pok√©mon Go‚Äôs success is its broad and diverse player base. The demographics are varied, with a significant number of young adults playing the game. Additionally, there‚Äôs a pretty balanced gender distribution among players, with 45.1% male and 54.9% female according to Player Counter. This diversity speaks volumes about the game‚Äôs ability to attract and engage a wide range of people. The latest graph from Statista reveals an intriguing trend: a significant drop in the application‚Äôs usage among users aged between 18 and 54, who previously used the app but now do not. Conversely, users above 55 have shown the opposite behavior, demonstrating increased engagement. This information is particularly crucial given that most users are young adults; however, daily engagement is something that should be measured using at least D1 to D7 Retention for a clearer diagnosis. Understanding DAU behavior is vital for a game generating $4 million in daily revenue, as reported by SensorTower. However, the graph does not suggest that Niantic may need to do more to help new users learn the game and keep them engaged. Drawing on these insights, it‚Äôs clear that a GenAI companion app could fulfill several key roles: Onboarding and Education: This function would help newcomers grasp the fundamental aspects of the game, including basic rules and strategic approaches. By demystifying the initial learning curve, the app ensures that new players can quickly become competent and enjoy their gaming experience. Adaptation Support: As games evolve, introducing new features or temporary changes can sometimes confuse even the most experienced players. Here, the GenAI companion app steps in to offer timely updates and tutorials, helping players adapt and continue to enjoy the game without interruption. Guidance for New Joiners in the Pokemon Saga: Specifically tailored to those new to the Pokemon series, this feature of the app would act as a QA support system. It addresses common queries and challenges that newcomers face, ensuring they feel supported as they embark on their Pokemon journey. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:1:0","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"Hypothesis Following the perspective provided by the previous data, and given the limited availability of real data, it will be assumed as a base hypothesis that the creation of a complementary application will serve to substantially improve the key roles mentioned in the following hypothesis. H_0 : The companion app will significantly support onboarding, and offer adaptive assistance, thereby facilitating a smoother introduction for new members to the Pok√©mon series. Now, under the premise that there must be enough statistical evidence to reject the null hypothesis, the following technical article will be explained, which will show the architecture and development of this GenAI application, but is not intended to be used as professional advice or consultation. Source: Remco Braas ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:1:1","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"Potential Stakeholders Programmers: Tasked with developing the technical infrastructure for GenAI integration, augmented reality features, and ensuring smooth app functionality across devices. Testers: Essential for identifying bugs and ensuring the GenAI interactions, with augmented reality features work as intended across various devices and real-world scenarios. Publisher: Interested in the market potential of combining Pok√©mon Go‚Äôs popularity with cutting-edge GenAI technology to attract new players and retain existing ones. Note:¬†To facilitate the understanding of the roles of the development team, I invite you to take a look at¬†this¬†diagram that I designed. ‚ö†Ô∏è¬†Audience message: If the theoretical and technical details of this article are not of your interest, feel free to jump directly to the Application Workflow section. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:1:2","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üì• About the model This GenAI app uses OpenAI‚Äôs GPT-4 by default for its advanced NLP capabilities. However, understanding the various needs of users, I incorporated a feature that allows easy replacement of the AI model through a simple parameter adjustment, during the initialization of the OpenAI Chatbot instance. The customization option ensures that users can align application performance with their specific requirements, balancing factors such as computational efficiency, cost, and task specificity, this one can be modified in the global_conf.yml file of the repository. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:2:0","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üèóÔ∏è Architecture components To grasp the application workflow, it‚Äôs important to first familiarize with the components that make up the architecture and understand their functionalities. To aid in this explanation, let‚Äôs refer to the following architecture diagram, which provides a comprehensive overview. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:3:0","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üì¶ Setup Loader When initializing the application or invoking the LLM, the first step is crucial but might not be showed in the last diagram. This step ensures the consistent return of the same LLM model instance, along with essential components like the environment configuration, the logger, a callback handler for agent‚Äôs monitoring, and the prompt library. This consistency is achieved through the Singleton design pattern. Essentially, this pattern ensures that every call to SetupLoader() results in the return of the identical instance configuration. By doing so, it guarantees the model‚Äôs settings, such as temperature and other configurations remain uniform and constant across the server. class SetupLoader: \"\"\" -- Singleton design pattern to instantiate the application -- Validation: Ensures that subsequent calls to SetupLoader() will return the same instance configuration. \"\"\" _instance = None _is_initialized = False def __new__(cls, *args, **kwargs): if cls._instance is None: cls._instance = super(SetupLoader, cls).__new__(cls) return cls._instance def __init__(self, new_model=False): load_dotenv() if not self.__class__._is_initialized: # Ensure __init__ is only executed once self.logger = self._setup_logging() self._setup_environment() self.prompt_template_library = self._setup_prompt_library() self.global_conf = self._setup_global_conf() self.chat_openai = self._setup_chat_openai( callbacks=self._setup_callbacks() ) self.__class__._is_initialized = True elif new_model: # After the first initialization, a new model can be created self.chat_openai = self._setup_chat_openai( callbacks=self._setup_callbacks() ) def _setup_logging(self): logging.basicConfig(level=logging.INFO) return logging.getLogger(__name__) def _setup_prompt_library(self): return prompt_template_library def _setup_global_conf(self): return global_conf def _setup_callbacks(self): return [AgentCallbackHandler()] def _setup_environment(self): \"\"\"Set the OpenAI API key from global_conf.yml if the user defined it.\"\"\" if global_conf.get(\"OPENAI_API_KEY\", None): os.environ[\"OPENAI_API_KEY\"] = global_conf[\"OPENAI_API_KEY\"] def _setup_chat_openai(self, callbacks=None): openai.api_key = os.environ.get(\"OPENAI_API_KEY\") return ChatOpenAI( temperature=global_conf[\"MODEL_CREATIVITY\"], model_name=global_conf[\"MODEL_NAME\"], callbacks=callbacks, ) It‚Äôs worth noting there‚Äôs a special parameter, new_model, designed for scenarios where a new instance creation is explicitly required. This option adds flexibility for use cases that demand refreshing the LLM settings. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:3:1","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üì¶ Tagging Components: Detect Intent When examining the diagram‚Äôs structure, we can identify a step that plays a pivotal role: evaluating user input and deciding the subsequent path for redirecting requests or questions. All this will happen on the first step of the IntentHandler. This task is achieved through the implementation of a Tagging strategy, which is built upon a base Pydantic model. Put simply, this process is similar to a Named Entity Recognition system. However, a difference is the limitation to a predefined set of categories for text classification. from langchain.output_parsers.openai_functions import PydanticOutputFunctionsParser from langchain_core.pydantic_v1 import BaseModel, Field class IntentTagger(BaseModel): \"\"\"Tag the piece of text with particular intent type, and detect the text structure describing the intent\"\"\" intent_type: str = Field( description=\"Intent type expressed in the text, must take one of the \" \"functionalities as: 'defense_suggestion', 'information_request' or \" \"'squad_build'\", default=None, ) intent_structure: str = Field( description=\"The type of sentence structure used to detect the intent must \" \"take one of the values: 'pokemon_names', 'natural_language_question' or \" \"'natural_language_description'\", default=None, ) intent_parser = PydanticOutputFunctionsParser(pydantic_schema=IntentTagger) The intent_type can be classified include: defense_suggestion: Prompt mentions that an unknown Pok√©mon has appeared and the user needs a suggestion to use a Pok√©mon against it. The user must provide the name of the Pok√©mon as part of the input. information_request: The user wants to know more about a Pok√©mon. In the input, the user must provide a request for information on one or multiple entities. squad_build: The user wants to build a squad of Pok√©mon based on the opponent‚Äôs Pok√©mon types. The user must provide a list of Pok√©mon names as part of the input. And, the intent_structure, which describe the syntactic composition of the text, can be classified in: pokemon_names: Sentence with the name of one or more Pok√©mon explicitly mentioned in the text with an expressed request. natural_language_description: Sentence with a description of a Pok√©mon in natural language without explicitly mentioning the name of the Pok√©mon with the intention of guessing what the Pok√©mon is. natural_language_question: Sentence with a question about the Pok√©mon whose name is mentioned explicitly, asking for a specific attribute(s) belonging to the Pok√©mon entity. Note that intent_parser helps structure the LLM response, meaning that the output will be consistently returned as a Pydantic object, and based on this attributes, the system will define where to redirect the request. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:3:2","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üì¶ Named Entity Recognition (NER) Components: Gather Name Entity | Gather Desc. Entities Also known as Multiple Extraction approach, this phase involves a general procedure that incorporates Pydantic models, which will be used for various intents presented in the architecture. In this specific instance, a Pydantic model functions as an entity object. It is characterized by defined attributes that are essential for the Named Entity Recognition extraction process. Let‚Äôs refer to the following code example: from typing import List from langchain.output_parsers.openai_functions import PydanticOutputFunctionsParser from langchain.pydantic_v1 import BaseModel, Field class PokemonEntity(BaseModel): \"\"\"Extract the piece of text with the Pok√©mon name\"\"\" name: str = Field(description=\"Name of the Pok√©mon mentioned in text\", default=None) class PokemonEntityList(BaseModel): \"\"\"Extract list of pieces of text with Pok√©mon names\"\"\" name_list: List[PokemonEntity] = Field( description=\"List of names of the Pok√©mon mentioned in text\", default=[] ) pokemon_entity_parser = PydanticOutputFunctionsParser(pydantic_schema=PokemonEntityList) Here, the PokemonEntity will represent a Pokemon named in the prompt, and name is the identifiable entity attribute with its respective data type. However, the real challenge arises when we introduce multiple names into the prompt. This is where the PokemonEntityList comes into play. The advantage of the PokemonEntityList is quite significant. It allows for the addition of more attributes to the PokemonEntity class seamlessly. This feature ensures that the LLM can accurately determine which entity each attribute belongs to. To put this into perspective, consider a scenario where the prompt given is ‚ÄúCharizard and Treecko are fire and grass type Pok√©mon respectively‚Äù. The output structure generated from this prompt is both efficient and logical, demonstrating the system‚Äôs capability to assign entities, simply great: PokemonEntityList.name_list = [ PokemonEntity(name=\"Charizard\", pokemon_type=\"Fire\"), PokemonEntity(name=\"Treecko\", pokemon_type=\"Grass\") ] This way, a chain is created and invoked by converting the Pydantic class into an OpenAI function. def get_pokemon_entity_chain() -\u003e RunnableSequence: \"\"\"Create a chain that can be used to identify the Pok√©mon entities of a user input by using an Extraction approach. Returns: RunnableSequence: Language model chain structured as RunnableSequence. \"\"\" pokemon_template = dedent( prompt_template_library[\"stage_1_pokemon_entity_template\"] ) pokemon_entity_template = ChatPromptTemplate.from_messages( [(\"system\", pokemon_template), (\"human\", \"{input}\")] ) model = base_llm.bind( functions=[convert_pydantic_to_openai_function(PokemonEntityList)], function_call={\"name\": \"PokemonEntityList\"}, ) return pokemon_entity_template | model | pokemon_entity_parser ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:3:3","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üì¶ API Tooling Components: Gather Information Langchain provides two solutions to augment the output of LLMs through two primary methods: ‚ÄúAgents‚Äù and ‚ÄúChains.‚Äù The ‚ÄúAgents‚Äù method involves calling an agent that intelligently selects and utilizes the necessary tools as often as needed to fulfill a request. This process is automatic, leveraging the agent‚Äôs ability to determine the most suitable tool for the task at hand. On the other hand, ‚ÄúChains‚Äù offers a more tailored approach, which is particularly beneficial for our specific needs. By employing ‚ÄúChains‚Äù, we can make a customized Pokemon API wrapper that functions as an OpenAI tool. Source: Langchain Documentation In the previous illustration, we outlined a process flow. Here, we can go deeper into each step: Input: The system begins by reading the natural language input provided by the user. Parser: Next in the interpretation, the system employs a parser to analyze the type of input received. This analysis is essential for identifying the most appropriate tool within the model that is suited for processing the given input. The parser acts as a bridge, connecting the input with its relevant processing tool. Create Arguments: Once the appropriate tool is identified, the system then proceeds to formulate the necessary arguments required to utilize the tool effectively. These arguments, along with the names of the selected tools, are temporarily stored in an output. Tool Mapping: In the final step, the prepared arguments are forwarded to the actual function designated for the task. The system then executes the function, extracting the outputs generated. Following this structured approach, the API tool code is designed as shown below: @tool(\"pokemon_api_wrapper\", args_schema=ToolingEntry, return_direct=True) def pokemon_api_wrapper(name_list: List[str]) -\u003e Dict: \"\"\"Useful for when you need to request information from the Pok√©mon API, considering a single Pok√©mon Entity or several of them as input.\"\"\" client = pokepy.V2Client() pokemon_info_collection = {} for pokemon_name in name_list: try: logger.info(\" PokemonAPIWrapper: Information Search \") pokemon_data = client.get_pokemon(pokemon_name.lower()) pokemon = pokemon_data[0] info = { \"id\": pokemon.id, \"stats\": {stat.stat.name: stat.base_stat for stat in pokemon.stats}, \"height\": pokemon.height, \"weight\": pokemon.weight, \"types\": [type_slot.type.name for type_slot in pokemon.types], \"abilities\": [ ability_slot.ability.name for ability_slot in pokemon.abilities ], \"sprites\": pokemon.sprites.__dict__, } logger.info(\" PokemonAPIWrapper: Types Search \") if info[\"types\"]: try: # Extract Damage Relations info[\"damage_relations\"] = [ client.get_type(type_slot) for type_slot in info[\"types\"] ] # Overwrite Damage Relations damage_relations = info[\"damage_relations\"][0][0].damage_relations info[\"damage_relations\"] = { damage_type: damage_relations.__dict__.get(damage_type)[0].name for damage_type in damage_relations.__dict__.keys() if damage_type != \"_subresource_map\" and damage_relations.__dict__.get(damage_type) != [] } except Exception as e: logger.info(f\"No 'damage_relations' were extracted: {e}\") info[\"damage_relations\"] = {} pass pokemon_info_collection[pokemon_name] = info logger.info( f\" PokemonAPIWrapper: Information of '{pokemon_name}' was extracted \" ) except Exception as e: raise ToolException(f\"Tool Error handling '{pokemon_name}': {e}\") return pokemon_info_collection ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:3:4","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üì¶ Retrieval-Augmented Generation - Retrieval QA Components: Gather Answer | Search Sim. Desc | Gather Description | Gather Defense Suggestion | Gather Squad Suggestion For better understanding, first let‚Äôs break down this kind of component into two key parts, focusing on how we handle data and answer questions using Large Language Models: Retrieval-Augmented Generation (RAG): This method uses a two-step process. First, it finds relevant information files (e.g.: Pokemon Wiki as PDF) related to the input. Then, it combines this information with the original request to create a response. This approach is great for generating detailed and enhanced descriptions. RetrievalQA: This strategy is all about answering questions. It searches for and retrieves information related to the question from a knowledge base (e.g.: Vector Store). Then, it uses this information to either directly provide an answer or to create one. In such a system, when a question is asked, the model retrieves relevant documents or passages from a knowledge base and then either extracts or generates an answer based on this chunk of information. ‚òëÔ∏è Vector Store To implement these methods, we‚Äôre using a tool Vector Store FAISS, created by Meta AI Research. Faiss is designed for quickly finding information that‚Äôs similar to your search query, even in huge databases. We chose to use FAISS for its features, explained simply: It can search multiple vectors simultaneously, even without a GPU enabled, thanks to its great multithreading capabilities. It finds not just the closest match but also the second closest, third closest, and so on, using a Euclidean distance (L2) to search for its similarity. It trades precision for speed, e.g.: give an incorrect result 10% of the time with a method that‚Äôs 10x faster or uses 10x less memory. It saves data on the disk instead of using RAM. It works with binary vectors, which are simpler than the usual floating-point vectors. More details can be found at the Meta‚Äôs official wiki. ‚òëÔ∏è Process After loading the files using Langchain Document Loaders. Source: Langchain Documentation Split: The first steps is to define the splitting strategy using Text Splitters. The decision between recursive or character-based text splitting is a preprocessing step to divide the document into manageable chunks. This is important because FAISS operates on vectors, and the text must first be converted into a numerical format (vector representation) that FAISS can work with. The splitting helps in managing document size and ensuring each chunk is meaningful for the vectorization process. Embed: Here are two process relevant to this step: Vectorization: While FAISS itself doesn‚Äôt handle the text-to-vector conversion, this step is essential for preparing data for similarity search. The choice of embedding method (like OpenAIEmbeddings) significantly affects the quality of the search results, as it determines how well semantic similarities are captured in the vector space. Embeddings are high-dimensional vectors that represent the semantic content of the text. Each word, sentence, or document is mapped to a point in a high-dimensional space, where the distance between points corresponds to semantic similarity. Indexing: involves creating a structured representation of the files that allows for similarity searches. Store: Saving the vector store locally means storing the indexed representation of the document chunks. This allows for persistent access to the indexed data for future similarity searches without needing to re-process the original documents. We chose to keep this index in the same repository (.faiss \u0026 .pkl) for simplicity, but it‚Äôs also possible to store it in a artifact repository manager like JFrog. def _index_vector_store() -\u003e None: \"\"\"Void Function to create an RAG index inside a vector store from the source file.\"\"\" logger.info(\"Loading \u0026 Splitting Source File\") pdf_path = f\"{global_conf['SOURCE_PDF_PATH']}/pokedex_tabletop_content.pdf\" loader ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:3:5","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üì¶ API endpoints The entire conditional process is consolidated into a single API endpoint, distinct from the UI‚Äôs URL, ensuring the UI‚Äôs independence from the computational resources required for the IntentHandler process. The infrastructure relies on FastAPI, with a Pydantic schema handling the I/O object. class Query(BaseModel): \"\"\"Query model to handle the user query\"\"\" user_query: str = Field(description=\"User query to process\", default=None) @app.post(\"/intent_query/\") async def process_query(query: Query): \"\"\"Endpoint to handle the intent execution and return the response to the user.\\nAll the responses will be structured same as defined by the corresponding agent in charge or the intent execution by using the `IntentHandler` class.\\nFinally the response will be returned with the format defined by the `ResponseTemplate` object.\\n**Note**: The response will be in JSON format which will be used by the streamlit app to display the response. \"\"\" try: intent_handler = IntentHandler() intent_handler.user_input = query.user_query raw_response = intent_handler.run() final_response = raw_response.template_structure return {\"response\": final_response} except Exception as e: return {\"error\": str(e)} To separately test the API independently, start the server with the command make api_server, and then access the Swagger UI. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:3:6","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üì¶ User Interface The UI dynamically adapts based on the response format, utilizing a custom template generator. This generator feeds data into the Streamlit UI, tailoring the display to match both the intent type and the syntactic structure of the user‚Äôs prompt. The core of this functionality is a large Python @dataclass, which serves as the response template generator and is accessible for reference. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:3:7","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"‚ö° Application Workflow The application simplifies setting up and deploying APIs built with FastAPI. The README file includes a series of Make targets with a range of functions, from environment setup to API configuration and UI deployment. This section focuses on explaining the workflow and functionalities, rather than the technical details behind them. The E2E process is guided by the prompt‚Äôs intent (type) and syntax (structure). Using these, the application with assistance from the LLM, directs the workflow to the appropriate tools. If the response is not recognized, it is redirected to GPT-4‚Äôs general knowledge, with a banner citing the source on the final response. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:4:0","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"UI Features The first step involves familiarizing the user with the response feature. After launching the UI on a local port, a side panel appears on the left (see image below, labeled as 1), featuring a toggle for activating or deactivating JSON mode, set to off by default. This mode presents responses in JSON format (labeled as 2), facilitating debugging or analysis of the response‚Äôs target within the ‚ÄúIntent Handler‚Äù workflow or ‚ÄúResponse Formatter‚Äù (core components). This by default is set to False. Once we‚Äôve defined the output format, the next step involves sending a request to the API. However, the API‚Äôs response can vary significantly based on the type of intent. As previously discussed in the ‚ÄúTagging‚Äù component, the Intent Handler can detect multiple combinations of intents. We will detail each of these intents in the following section: ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:4:1","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"Information Retrieval Intent This intent primarily seeks information on one or more Pok√©mon, with the process adapting to the syntactic structure of the request. There are 3 types of syntactic structures: Natural Language Question: A question about a Pok√©mon initiates a RetrievalQA process through the Vector Database to find an answer. The Pok√©mon‚Äôs name is identified as an entity, triggering an API wrapper request to deliver the final response. E.g.: ‚ÄúDo you know in what kind of habitats I can find a Psyduck?‚Äù. Natural Language Description: This involves inputting a description used at identifying a Pok√©mon. The process selects the most similar document chunk from the Vector Database, including the Pok√©mon‚Äôs name. Next, a NER process identifies the Pok√©mon name within the text, using this entity to make an API request. This retrieves information and structures a description from the Vector Database to formulate the answer. E.g.: ‚ÄúCan you guess which Pok√©mon is a dual-type Grass/Poison Pok√©mon known for the plant bulb on its back, which grows into a large plant as it evolves‚Äù. Single Named Entity: The simplest approach, it directly uses NER to detect the Pok√©mon‚Äôs name. This entity then prompts an API request to obtain information, which is used to form a structured description from the Vector Database to provide the answer. E.g.: ‚ÄúAlright, Pok√©dex! It‚Äôs time to find out everything about Snorlax and Pikachu!‚Äù. The expected structured output look as follows. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:4:2","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"Defense Suggestion Intent This intent aims to offer players advice on defending against Pok√©mon, useful for capturing Pok√©mon or winning battles against other players. When a user encounters an unidentified Pok√©mon and requests a recommendation for a counter Pok√©mon, they must specify the name of the Pok√©mon. E.g.: ‚ÄúI stumbled upon a wild Grovyle lounging in the park! Which Pokemon should I choose for an epic battle to defeat it?‚Äù. Initially, we identify the Pok√©mon‚Äôs name as a Named Entity using a Pydantic Class for extraction. This entity is then processed by the system, which aligns the input schema with the corresponding API entry. Subsequently, we create an embedding request that highlights the opposing Pok√©mon‚Äôs vulnerabilities based on its type. Leveraging this information, the LLM augment the response using Vector Database context to suggest the most effective Pok√©mon to use in battle. The expected structured output look as follows. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:4:3","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"Squad Builder Intent This intent facilitates the creation of a Pok√©mon squad tailored to counter the types of an opponent‚Äôs Pok√©mon, in simple words it builds a squad. Users must input a list of the opponent‚Äôs Pok√©mon names as part of the prompt. E.g.: ‚ÄúTime to challenge the Fire Gym Leader! He‚Äôs got a tough team with a Ninetales and Combusken, but I need your help to build a squad‚Äù. Initially, the language model identifies each Pok√©mon in the opponent‚Äôs squad and creates a vulnerability template for each, similar to the approach in the ‚Äúdefensive‚Äù intent. Subsequently, it identifies suggested Pok√©mon entities and submits them to an API, which returns a structured response for each entity. The expected structured output look as follows. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:4:4","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"Sprites Design Last but not least, each of the answers has a structure with the same format, which provides as part of the API wrapper the sprites of the classic designs of the series. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:4:5","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"üóíÔ∏è Final thoughts \u0026 takeaways What can the stakeholders understand and take in consideration? Implementing a GenAI-driven chatbot for user onboarding is beneficial, but it‚Äôs important to also focus on the quality of supplementary resources, such as wikis. These play a significant role in enhancing response augmentation process. Additionally, scalability demands may require evaluating various Vector Database services, including Pinecone, elasticsearch, Cassandra, among others. To assess the viability of this approach, it‚Äôs essential to first evaluate the token consumption for all LLM calls related to each intent. What could the stakeholders do to take action? Prior to adopting such an approach, a thorough analysis of user needs is crucial. Examining user retention from D1 to D7 across different cohorts (e.g.: demography-based), supported by a robust segmentation strategy, provides valuable insights into user behavior and needs. The current integration serves as a preliminary demonstration. Developing a companion app will require addressing a distinct set of integration requirements. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:5:0","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"‚ÑπÔ∏è Additional Information Here you have a list of preferred materials for you to explore if you‚Äôre interested in similar topics: Related Content ‚Äî About Langchain. ‚Äî OpenAI models Pricing page. ‚Äî Meta‚Äôs FAISS tutorials. ‚Äî User Segmentation Strategies by IronSource from Unity. ‚Äî Pok√©mon Go release notes by Niantic. Augmentation Assets The RetrievalQA system was augmented using an unofficial Tabletop Pok√©mon Guide. An official resource is suggested to improve response quality and avoid any bias. ","date":"2024-01-15","objectID":"/posts/genai_pokemon_strategy_assistant/:6:0","tags":["GenAI","LLM","FastAPI","Vector DB","Langchain","RAG","GPT-4"],"title":"GenAI-Driven Pok√©mon Go Companion App: Augmenting onboarding experience with RAG","uri":"/posts/genai_pokemon_strategy_assistant/"},{"categories":["Projects"],"content":"Production-Ready pipeline to extract logs from the mobile game Brawl Stars, using a KMeans model for player classification and cohorts creation, to produce parametrized bounded retention metrics","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"Abstract: This study presents a comprehensive investigation into the development of a player retention pipeline and the establishment of user cohorts for the game Brawl Stars through the application of unsupervised machine learning models. The primary aim is to enhance the analysis of user retention by implementing advanced cohort classification techniques and the generation of retention metrics. To achieve this, the research utilizes data obtained from the Brawl Stars public API, applies KMeans clustering for the categorization of players, and employs PySpark for data processing and analysis, with an emphasis on the use of Kedro for the efficient assembly of the data processing pipeline. The outcomes of this analysis underscore the pipeline‚Äôs proficiency in handling vast datasets and augmenting the production of retention metrics. The results suggest that this automated strategy provides clear insights into player behaviors and retention rates, endorsing its integration into game development practices and its potential adaptability for Google Cloud Deployments. ¬†Important: Any views, material or statements expressed are mines and not those of my employer, read the disclaimer to learn more.1 Looking for an interactive experience?\r\rüöÄ Access the source code from GitHub\r\r ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:0:0","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"‚ö†Ô∏è Introduction to the problem ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:1:0","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"Hypothesis The analytics team of a mobile gaming developer is facing a problem while producing bounded retention metrics for player cohorts based on the game modes offered. They need a fully parameterized way to produce these metrics, instead of hardcoded methods per iteration. Currently, the analytics team mines data batches of over 20,000 players in a single run to create unbounded retention metrics, and the job is processed every 56 days. However, the team need to track user retention within a given time frame and qualify player preferences based on the game modes offered and their game-experience. Also, they require a model that can define cohorts based on the installation date or any other classification parameter and includes a feature store to keep track of the features used. Additionally, they would like to store the hyperparameters, evaluation scores, and estimators used for each experiment on a Cloud Service to reuse specific models later. $H_0:$ The pipeline is sufficient to produce retention metrics for player cohorts based on the game modes offered, and there is no need for a fully parameterized tool to create these metrics with bounded approaches. $H_1:$ The current pipeline is not sufficient, and a fully parameterized tool is required to track user retention within a given time frame and qualify player preferences based on the game modes offered. Additionally, a Cloud Service-based feature store and model registry are needed to keep track of experiment versions. ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:1:1","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"Potential Stakeholders Designers: Game Designers are interested in user retention metrics to understand how users are interacting with the game and to identify any areas that need to be improved to keep users engaged and coming back to the game. Product / Project Managers: Responsible for defining the goals and objectives for the game and ensuring that those goals are met, they need to follow user retention metrics to understand how well the game is performing in terms of retaining users and whether any changes need to be made to the game to improve retention. Marketer: Understand how effective their marketing campaigns are in ‚Äúkeeping users engaged‚Äù with the game, and whether any changes need to be made to the campaigns to improve retention. Programmers: Interested in identifying any technical issues that may be causing users to leave the game and make improvements to the game to improve retention. Publisher: Need to understand the financial performance of the game and whether it is generating sufficient revenue to justify the investment. They are also interested in understanding the potential for future growth and the impact of retention on the game‚Äôs long-term sustainability. Note:¬†To facilitate the understanding of the roles of the development team, I invite you to take a look at¬†this¬†diagram that I designed. ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:1:2","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"üì• About the data ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:2:0","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"Source data The Brawl Stars Public API is the source of the data, which is accessible to anyone and completely free to use. By entering an identifier in the request function, the API gathers data on the player associated with that identifier, also known as the ‚ÄúPlayer Tag‚Äù. Each user account has a unique player tag, which I collected from a sample shared by @SirWerto on Kaggle. To supplement our tag list, also utilized the .ranking() function of Brawlstats, an open-source Python library. The dataset contains a total of 23841 Player Tags, which are saved in a .txt file and stored in a Google Cloud Bucket folder labeled as /01_player_tags. ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:2:1","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"Data Catalog structure Versioning plays a vital role in maintaining organized and traceable files. To accomplish this, we utilize a hierarchical storage approach, incorporating a timestamp in EPOCH format as a reference point. In order to leverage this functionality, we will employ the Kedro versioning feature. To ensure a comprehensive record of models, estimators, and features used in each run, the catalog offers eight distinct storage endpoints, including /06_viz_data, /07_feature_store, and /08_model_registry, all of which are versioned. By using versioning to manage and maintain these files, teams can easily track data and model changes, to ensure that each iteration is saved for future reference. gcp:# ----- Bucket Parent Locations -----player_tags:gs://animus-memory-bucket/brawlstars/01_player_tagsraw_battlelogs:gs://animus-memory-bucket/brawlstars/02_raw_battlelogsraw_metadata:gs://animus-memory-bucket/brawlstars/03_raw_metadataenriched_data:gs://animus-memory-bucket/brawlstars/04_enriched_datacurated_data:gs://animus-memory-bucket/brawlstars/05_curated_dataviz_data:gs://animus-memory-bucket/brawlstars/06_viz_datafeature_store:gs://animus-memory-bucket/brawlstars/07_feature_storemodel_registry:gs://animus-memory-bucket/brawlstars/08_model_registry# ----- Main GCP Project ID -----project_id:abstergo-corp-id To easily locate the endpoint of each GCP Bucket output, simply refer to the conf/gcp/catalog.yml file. This file defines each destination using Dataset Groups, as shown in the following example: _pyspark:\u0026pysparktype:spark.SparkDataSetfile_format:parquetload_args:header:truesave_args:mode:overwritesep:','header:True_pandas:\u0026pandastype:pandas.CSVDataSetload_args:sep:\",\"save_args:index:Falsefs_args:project:${gcp.project_id}cohort_activity_data@pyspark:\u003c\u003c:*pysparkfilepath:${gcp.curated_data}/cohort_activity_data.parquetlayer:\"primary\"player_metadata@pandas:\u003c\u003c:*pandasfilepath:${gcp.raw_metadata}/player_metadata.csvlayer:\"raw\" The outputs cohort_activity_data@pyspark and player_metadata@pandas are specifically mapped to the _pyspark and _pandas dataset groups, respectively. Moreover, we can assign Layers to these outputs, providing users with a clear understanding of their utility as intermediate data sources, for analysis, or as input for machine learning models. Seven layers were defined in the catalog: raw, primary, model input, feature, models, model output, and reporting. To go deeper into the Layer definitions and best practices, I suggest reading the following article by Joel Schwarzmann (2021). ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:2:2","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"‚ö°Pipelines workflow The pipeline architecture consists of five modular pipelines, with a total of 14 nodes. These pipelines will be constructed utilizing the open-source framework, Kedro. Two of these pipelines, battlelogs_request_preprocess and metadata_request_preprocess, will use asynchronous coroutines to request data from the API and Spark for preprocessing. The requested data will be processed in batches of over 400 MB using only Spark to avoid communication overhead while minimizing the number of actions taken to take advantage of the worker‚Äôs process fluency. The events_activity_segmentation pipeline will use Spark SQL API to produce retention metrics. This pipeline will use the data gathered from the requesting nodes. It is important to consider the data dependency between functions, which is why multithreading won‚Äôt be used. Another pipeline, the player_cohorts_classifier, will perform a multiprocessing step over a Grid Search and cross-validation across multiple cores to evaluate the best quantity of clusters in which we can classify the players based on the game modes‚Äô behaviors. This depends on the distortion rate and the hyperparameters defined by the user. The last modular pipeline, player_activity_clustering_merge, will use the curated data to produce retention plots. It will use a single Python processing since it uses filtered data, which is a smaller subset. This pipeline was designed to speed up processing by parallelizing. We can calculate the theoretical speedup by applying Amdahl‚Äôs Law. $$S = \\frac{1}{1-P+\\frac{P}{N}} = \\frac{1}{1-0.64286+\\frac{0.64286}{10}} = 2.52$$ The theoretical speedup will be approximately 2.52 for a single-machine cluster. Here, S represents theoretical speedup, P represents the fraction of the workload that can be parallelized, and N represents the number of processors. If you want to learn more about the appropriate use cases for multithreading and multiprocessing, I highly recommend watching this conference video by Chin Hwee Ong (Pycon Taiwan, 2020). It‚Äôs one of the best resources to get started. ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:3:0","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"‚öîÔ∏è Battlelogs and Metadata Request Pipelines Pipeline battlelogs_request_preprocess and metadata_request_preprocess Both pipelines follow the same structure, the only difference is the request function used to gather the data from the API, one uses .get_battle_logs(), and the other uses the .get_player() function. And from these functions come two different data models battle logs and players. Node: battlelogs_request_node and player_metadata_request_node The inconsistent batch sizes of our data make it unsuitable to rely on an O(n) frequency expectation, due factors such as: Product cycle (e.g., new feature or season launch). Player longevity can affect the data (e.g., newer players with less than 25 sessions). To extract the logs, I opted to use low-level APIs, specifically Python‚Äôs asynchronous coroutines. This approach allows executing one batch while waiting for a response from another, which is particularly helpful when dealing with variable batch sizes or waiting for processing, such as querying from a SQL database. However, the API wrapper used limits the multithreading pool size to 10, which means we had to separate the tags into batches of that size. The main advantage of this approach is that it makes the code independent of the processor‚Äôs clock speed, avoiding waiting times between requests. Although Python‚Äôs Global Interpreter Lock (GIL) restricts running multiple threads simultaneously, we overcame this limitation by using coroutines to process future objects. If the pool size weren‚Äôt limited, we could have used multithreading instead. Overall, this strategy allowed us to process our data more efficiently and achieve better results. def battlelogs_request( player_tags_txt: str, parameters: Dict[str, Any] ) -\u003e pd.DataFrame: \"\"\" Extracts battlelogs from Brawlstars API by asynchronously executing a list of futures objects, each of which is made up of an Async thread task due to blocking call limitations of the api_request submodule. Args: player_tags_txt: A list of player tags. parameters: Additional parameters to be included in the API request. Returns: A structured DataFrame that concatenates all the battlelogs of the players. \"\"\" # Get key and validate it exists API_KEY = conf_credentials.get(\"brawlstars_api\", None).get(\"API_KEY\", None) try: assert API_KEY is not None except AssertionError: log.info( \"No API key has been defined. Request one at https://developer.brawlstars.com/\" ) # Create a BrawlStats client object to interact with the API. Note: the # \"prevent_ratelimit\" function in the source code can be used this behavior client = brawlstats.Client(token=API_KEY) # Split the player tags text by commas to create a list of player tags. player_tags_txt = player_tags_txt.split(\",\") def api_request(tag: str) -\u003e pd.DataFrame: \"\"\"Requests and structures the 25 most recent battle logs from the Brawl Stars API for a given player.\"\"\" try: # Retrieve raw battle logs data from API player_battle_logs = client.get_battle_logs(tag).raw_data # Normalize data into structured format player_battle_logs_structured = pd.json_normalize(player_battle_logs) # Add player ID to DataFrame player_battle_logs_structured[\"player_id\"] = tag except: log.info(f\"No battle logs extracted for player {tag}\") player_battle_logs_structured = pd.DataFrame() pass return player_battle_logs_structured async def api_request_async(tag: str) -\u003e pd.DataFrame: \"\"\" Transform non-sync request function to async coroutine, which creates a future object by API request. The Coroutine contains a blocking call that won't return a log until it's complete. So, to run concurrently, await the thread and not the coroutine by using this method. \"\"\" return await asyncio.to_thread(api_request, tag) async def spawn_request(player_tags_txt: list) -\u003e pd.DataFrame: \"\"\" This asynchronous function takes a list of player tags as input and creates a list of coroutines that request the player's battlelogs data. It then schedules their execution and waits for them to complete by u","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:3:1","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"üï∞Ô∏è Events and Activity Segmentation Pipeline Pipeline events_activity_segmentation Our goal is to transform raw data into valuable insights. To achieve this, we will focus on processing and filtering battle logs to produce ‚Äúactivity‚Äù data. This includes segmentations per event type, retention metrics, and ratios based on retention. With these metrics, we can gain a deeper understanding of user behavior and make informed decisions that drive growth. Node: battlelogs_deconstructor_node def battlelogs_deconstructor( battlelogs_filtered: pyspark.sql.DataFrame, parameters: Dict[str, Any] ) -\u003e Tuple[ pyspark.sql.DataFrame, pyspark.sql.DataFrame, pyspark.sql.DataFrame, pyspark.sql.DataFrame, ]: \"\"\" Extract player and brawler combinations from the same team or opponents, grouped in JSON format, based on user-defined event types. Each of the 'exploders' is parameterized by the user, according to the number of players needed for each type of event. Args: battlelogs_filtered: a filtered Pyspark DataFrame containing relevant cohorts and features. parameters: a dictionary of event types defined by the user to include in the subset process. Returns: A tuple of four Pyspark DataFrames, each containing only one event type \"\"\" # Call | Create Spark Session spark = SparkSession.builder.getOrCreate() log.info(\"Deconstructing Solo Events\") if parameters[\"event_solo\"] and isinstance(parameters[\"event_solo\"], list): event_solo_data = battlelogs_filtered.filter( f.col(\"event_mode\").isin(parameters[\"event_solo\"]) ) event_solo_data = _group_exploder_solo( event_solo_data, parameters[\"standard_columns\"] ) else: log.warning( \"Solo Event modes not defined or not found according to parameter list\" ) event_solo_data = spark.createDataFrame([], schema=t.StructType([])) log.info(\"Deconstructing Duo Events\") if parameters[\"event_duo\"] and isinstance(parameters[\"event_duo\"], list): event_duo_data = battlelogs_filtered.filter( f.col(\"event_mode\").isin(parameters[\"event_duo\"]) ) event_duo_data = _group_exploder_duo( event_duo_data, parameters[\"standard_columns\"] ) else: log.warning( \"Duo Event modes not defined or not found according to parameter list\" ) event_duo_data = spark.createDataFrame([], schema=t.StructType([])) log.info(\"Deconstructing 3 vs 3 Events\") if parameters[\"event_3v3\"] and isinstance(parameters[\"event_3v3\"], list): event_3v3_data = battlelogs_filtered.filter( f.col(\"event_mode\").isin(parameters[\"event_3v3\"]) ) event_3v3_data = _group_exploder_3v3( event_3v3_data, parameters[\"standard_columns\"] ) else: log.warning( \"3 vs 3 Event modes not defined or not found according to parameter list\" ) event_3v3_data = spark.createDataFrame([], schema=t.StructType([])) log.info(\"Deconstructing Special Events\") if parameters[\"event_special\"] and isinstance(parameters[\"event_special\"], list): event_special_data = battlelogs_filtered.filter( f.col(\"event_mode\").isin(parameters[\"event_special\"]) ) event_special_data = _group_exploder_special( event_special_data, parameters[\"standard_columns\"] ) else: log.warning( \"Special Event modes not defined or not found according to parameter list\" ) event_special_data = spark.createDataFrame([], schema=t.StructType([])) return event_solo_data, event_duo_data, event_3v3_data, event_special_data To understand the basic game modes of this mobile game, an extensive research was required. All assumptions made in the event deconstruction process are based on the information provided by the Brawl Stars Fandom Wiki. The deconstruction process will classify the battle_teams column and divide all players in each session into groups based on the number of team members defined by the event type. These event types are categorized according to specific parameters, enabling users to easily classify events based on the distribution of players' teams per session. # ----- Parameters to deconstruct the data by event ----battlelogs_deconstructor:event_solo:['soloShowdown']event_duo:['duoShowdown']event_3v3:['gemGrab','brawlBall','bounty','he","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:3:2","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"üíÄ Player Classification and Cohorts Creation Pipeline Pipeline player_cohorts_classifier This pipeline is a parametrized method for classifying players. It saves the model in the registry and the artifacts for each experiment. The main objective of this pipeline is to make it easy for data scientists to set hyperparameters and save their models. This will help to prevent the spread of models across a repository into random Jupyter notebooks, without versioning. We know this is a common scenario that many data scientists are familiar with. It is worth mentioning that each node in this pipeline was planned using an experimental Jupyter Notebook. If you want to check the raw version, you can download it from here. Node: feature_scaler_node def feature_scaler(metadata_prepared: pd.DataFrame) -\u003e pd.DataFrame: \"\"\" Applies MaxAbsScaler on the input data and returns the transformed data. Args: metadata_prepared: Dataframe containing player metadata with 'player_id' column and numeric features. Returns: Dataframe with scaled numeric features Raises: AssertionError: If the length of 'player_id' column and the transformed data are not equal \"\"\" # Drop the 'player_id' column and use it as the target variable X = metadata_prepared.drop(\"player_id\", axis=1) y = metadata_prepared[\"player_id\"] # Initialize the MaxAbsScaler object, fit it to the data, and transform the data scaler = MaxAbsScaler() X_scaled = scaler.fit_transform(X) # Create a new DataFrame with the transformed data and original column names metadata_scaled = pd.DataFrame(X_scaled, columns=X.columns) # Check if the length of 'y' matches the transformed data and concatenate them try: assert len(y) == len(metadata_scaled) metadata_scaled = pd.concat([y, metadata_scaled], axis=1) except AssertionError: log.info(\"Scaler instance or data is corrupted, try debugging your input data\") # Return the scaled DataFrame return metadata_scaled This node will use as input the player metadata, which includes descriptive data about the lifetime statistics for each player per battle-type score, to get more detail on the data specs you can refer to the ‚ÄúPlayer‚Äù data model. To train a KMeans model effectively, it‚Äôs crucial to prepare the data by scaling it. Let‚Äôs first review one of my recent experiments before delving into the scaling methods. The scaling method we choose depends on the nature of the data and our analysis requirements. To choose the best scaling method for your project, here‚Äôs a brief overview of each option: StandardScaler: This scaling method scales each feature to have a mean of 0 and a standard deviation of 1. It‚Äôs a good choice if our data follow a normal distribution and if we want to emphasize the differences between data points. Normalizer: This scaling method scales each sample to have a Euclidean length of 1. It‚Äôs a good choice if we want to emphasize the direction of the data points and we‚Äôre not interested in their magnitude. MaxAbsScaler: This scaling method scales each feature to have a maximum absolute value of 1. It‚Äôs a good choice if we have sparse data or features with very different scales and we want to preserve the sparsity and relative magnitudes of the data, like when we have justified outliers. So, as we have sparse data and features with very different scales, and we want to preserve the sparsity and relative magnitudes of the data I used¬†MaxAbsScaler. Node: feature_selector_node def feature_selector( metadata_scaled: pd.DataFrame, parameters: Dict[str, Any] ) -\u003e Tuple[pd.DataFrame, Dict[str, List]]: \"\"\" Select top K features from a scaled metadata dataframe. Here the module select n_features_to_select to consider the selection of features to represent: - Player capacity of earning credits (representation of trophies). - Capacity of being a team player (representation of 3v3 or Duo victories). - Solo skills (representation of Solo victories). Args: metadata_scaled: The input metadata as a Pandas DataFrame, with player ID in one column and feature values i","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:3:3","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"üìä Player activity data and Cluster labels merge Pipeline Pipeline player_activity_clustering_merge def player_cluster_activity_concatenator( user_activity_data: pyspark.sql.DataFrame, players_metadata_clustered: pd.DataFrame, params_ratio_register: Dict[str, Any], params_activity_transformer: Dict[str, Any], ) -\u003e pd.DataFrame: \"\"\" Concatenates player clusters with their respective activity record activity data and calculates retention ratios based on provided analytical ratios. Args: user_activity_data: Spark DataFrame containing user retention data. players_metadata_clustered: DataFrame containing player data and cluster labels. params_ratio_register: Dictionary with ratio register parameters. params_activity_transformer: Dictionary with activity transformer parameters. Returns: Pandas DataFrame containing concatenated player cluster activity data and retention ratios \"\"\" # Call | Create Spark Session spark = SparkSession.builder.getOrCreate() # Create Spark DataFrame from Pandas DataFrame players_metadata_clustered = spark.createDataFrame( data=players_metadata_clustered ).select(\"player_id\", \"cluster_labels\") # Join user activity data with player metadata and cluster labels player_clustered_activity = user_activity_data.join( players_metadata_clustered, on=\"player_id\", how=\"left\" ) # Select retention metric columns and dates of first log records ret_columns = player_clustered_activity.select( player_clustered_activity.colRegex(\"`^D\\d+R$`\") ).columns player_clustered_activity = player_clustered_activity.select( *[\"player_id\", \"cluster_labels\", \"first_log\", *ret_columns] ) # Group by cluster labels and first log date, and aggregate retention metrics player_clustered_activity = player_clustered_activity.groupBy( [\"cluster_labels\", \"first_log\"] ).agg(*[f.sum(col).alias(f\"{col}\") for col in ret_columns]) # Get analytical ratios, normal ratios and days available from parameters, # and validate that all day labels are present analytical_ratios = [ literal_eval(ratio) for ratio in params_ratio_register.get(\"analytical_ratios\") ] ratios = [literal_eval(ratio) for ratio in params_ratio_register[\"ratios\"]] days_available = params_activity_transformer.get(\"retention_days\") # Convert retention metrics to percentages for ratio in ratios: num, den = ratio ratio_name = f\"D{num}R\" ret_num = f\"D{num}R\" ret_den = f\"D{den}R\" player_clustered_activity = player_clustered_activity.withColumn( ratio_name, ratio_agg(f.col(ret_num), f.col(ret_den)) ) # Obtain analytical ratios if them were defined in the parameters (ratio register) if params_ratio_register[\"analytical_ratios\"] and isinstance( params_ratio_register[\"analytical_ratios\"], list ): # Validate day labels are present in parameters _ratio_days_availabilty(analytical_ratios, days_available) # Calculate retention ratios for each analytical ratio (inputs) for ratio in analytical_ratios: num, den = ratio ratio_name = f\"D{num}/D{den}\" ret_num = f\"D{num}R\" ret_den = f\"D{den}R\" player_clustered_activity = player_clustered_activity.withColumn( ratio_name, ratio_agg(f.col(ret_num), f.col(ret_den)) ) # Reorder dataframe, reformat the cluster labels and send to data to driver player_clustered_activity = ( player_clustered_activity.orderBy(f.asc(\"first_log\"), f.asc(\"cluster_labels\")) .dropna(subset=[\"cluster_labels\"]) .withColumn(\"cluster_labels\", f.col(\"cluster_labels\").cast(\"integer\")) .toPandas() ) # Present all installations as 100% retention player_clustered_activity[\"D0R\"] = float(1.0) return player_clustered_activity The purpose of the upcoming pipeline is to streamline the process of augmenting the retention data generated by the activity_transformer_node. Specifically, this involves incorporating the tags and cluster labels obtained from the output produced by the kmeans_inference_node. The ultimate objective is to utilize this enriched dataset to generate compelling visualizations through the user_retention_plot_gen_node. To illustrate the capabilities of this pipeline, let‚Äôs examine an examp","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:3:4","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"üåê¬†Deployment Demo The deployment will be structured in a simple way to define an architecture that can run the job without complex services. In this case, I used a stateless service to run the job. The tech stack used is composed of the following elements: GitHub repository:¬†The code repository containing the code assembled as a Kedro project framework. This repository has two branches: one for development (feature/develop) and another for deployments (main). Docker container:¬†A container created with a Dockerfile that installs Java v8, Hadoop 3.1.1, and Spark 3.3.1 on top of a base image in Python 3.9. The container then executes the main¬†CMD¬†command to run the job. Google Cloud Bucket:¬†A place to store the raw data, staging data, and model artifacts. The bucket is structured to provide more traceability to the experiments. Google Cloud Build:¬†A service used for continuous development. When a user makes a pull request from the¬†feature/develop¬†branch to the¬†main¬†branch, an action is triggered in Google Cloud Build to rebuild the image based on the new code version. Google Cloud Run:¬†A computing platform with scalable infrastructure where the execution command is pointing, using the code image provided by the Cloud Build service. Note that the execution of the code is pointing to the visualization command, that‚Äôs why the outputs are not loaded. This is because the main purpose of this deployment is to showcase this project. However, for a deployment on GCP, it is simple to change the CMD command at the end of the Dockerfile. Additionally, the deployment was made public for this showcase using Google‚Äôs Free Program, and a VPC was not set up to let you access the pipeline view. ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:4:0","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"üóíÔ∏è Final thoughts \u0026 takeaways What can the stakeholders understand and take into consideration? Implementing an automated and parametrized pipeline for bounded retention metrics allows for focused analysis of player engagement and behavior, providing actionable insights for game improvement. Real-time and reliable data enables designers to identify trends, patterns, and potential issues affecting customer retention, facilitating the implementation of targeted strategies and interventions. What could the stakeholders do to take action? Planning the data processing method is crucial to assess scalability and long-term parallelization technologies, minimizing risks and optimizing resource allocation. Establishing a robust monitoring system helps track key metrics and performance indicators before and after new season releases, aiding in the identification of potential data drift sources. Implementing good practices in Continuous Development allows for seamless productionalization of new changes and facilitates testing and deployment. To compare different modeling strategies, A/B testing can be used by assembling new nodes to alternative versions of the same pipeline. What can stakeholders keep working on? Centralizing the solution into a single cloud provider, like Google Cloud Services, minimizes tech debt, simplifies management, and optimizes resource allocation and cost management. Leveraging tools like Kedro, MLflow, and Neptune.ai for experiment tracking helps keep artifacts versioned and trackable over time, enhancing reproducibility and collaboration ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:5:0","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"‚ÑπÔ∏è Additional Information Related Content Here you have a list of preferred materials for you to explore if you‚Äôre interested in similar topics: ‚Äî Elite Game Developers Podcast by Joakim Achren. ‚Äî Joakim Achren‚Äôs newsletter and blog. ‚Äî Game Data Science book and additional info at the¬†Oxford University Press. ‚Äî Getting started with Kedro. ‚Äî Brawlstats homepage, by overwolf. Datasets The extracted datasets will be uploaded to Kaggle as part of its public source intention. Article disclaimer: The information presented in this article is solely intended for learning purposes and serves as a tool for the author‚Äôs personal development. The content provided reflects the author‚Äôs individual perspectives and does not rely on established or experienced methods commonly employed in the field. Please be aware that the practices and methodologies discussed in this article do not represent the opinions or views held by the author‚Äôs employer. It is strongly advised not to utilize this article directly as a solution or consultation material, as the process of data collection and utilization of Public APIs may introduce biases and inaccuracies. Users are cautioned to exercise caution and discretion when considering its use.¬†‚Ü©Ô∏é ","date":"2023-05-20","objectID":"/posts/brawlstars_retention_pipeline/:6:0","tags":["Kedro","Spark","Unsupervised Models","Retention Metrics","KMeans","Google Cloud"],"title":"Optimizing User Retention in Brawl Stars: Unsupervised Machine Learning Approach to Cohorts Retention Metrics","uri":"/posts/brawlstars_retention_pipeline/"},{"categories":["Projects"],"content":"Inferential analysis with Kruskal‚ÄìWallis ANOVA to justify the Balance of Power 6.86 patch logs","date":"2022-07-24","objectID":"/posts/anova_dota/","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"Abstract: This study examines the effect of a game patch on hero balance in Multiplayer Online Battle Arena (MOBA) games, focusing on changes in hero damage output. Utilizing ANOVA tests and statistical analyses, including Kruskal‚ÄìWallis ANOVA, Mann-Whitney U test, Kolmogorov-Smirnov test, and Bootstrapping in Python, data from ranked matchmaking sessions were analyzed to assess the statistical significance of damage output changes according to the official changelog. Findings reveal Shadow Fiend as the standout hero with increased gold collection, tower damage, and victories due to the ‚ÄòRequiem of Souls‚Äô ability‚Äôs enhancement with the ‚ÄúAghanim‚Äôs Scepter‚Äù in patch 6.86. In contrast, Zeus was identified as having the highest damage output, attributed to a character remodel, its role as Soft Support, and its straightforward playability. The study offers insights into the implications of game patches on hero performance, providing a foundation for further game design and balance discussions. Looking for an interactive experience?\r\rüöÄ Download the Jupyter Notebook, available here\r\r ","date":"2022-07-24","objectID":"/posts/anova_dota/:0:0","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"‚ö†Ô∏è Introduction to problem ","date":"2022-07-24","objectID":"/posts/anova_dota/:1:0","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"Hypothesis For online multiplayers, sustainability mirrors the voice of customers. In the case of DOTA, the platform‚Äôs exclusivity due to the ownership of its developer, Valve Corporation, make it more reachable to gather community reviews on time from the Steam store. This, in comparison to other games' developers, who might require to access an endpoint to connect to¬†Steamworks API¬†to perform the same task. The Consumer Insights team at Valve gathered all the internal reviews and external‚Äôs from social networks of high traffic in this niche, like Reddit. After an exhaustive NLP analysis, the team has found that players tend to comment about Nerfing and Buffing heroes, which is very common in MOBA games when every patch is released. And, since patch 6.86 was released on December 16 of 2015, also known as¬†Balance of Power, the team has found many comments about this topic, so they require to generate actionable insights to let the Game Designers take action on time. After carefully collecting player logs since the released date of this patch, they need to test if effectively the heroes‚Äô stats are unbalanced or not. As the main interaction between enemy players is measured by the damage dealt, this will be defined as the core KPI of this study and with this, we will define our hypothesis as: $H_0:$ After the release date of patch 6.86, the heroes have recorded a statistically equal damage. $H_1:$ After the release date of patch 6.86, the heroes haven‚Äôt recorded a statistically equal damage. Designed by Roberto Aguilar, 2022\"\rDesigned by Roberto Aguilar, 2022\r ","date":"2022-07-24","objectID":"/posts/anova_dota/:1:1","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"Potential Stakeholders Level Designers: if the heroes are balanced in terms of damage, the damage dealt by the creeps will require an investigation, which depends on the design of the map lanes. They must hit a balance between creating easy or hard experiences, and this will rely on the creeps per lane. Another mapping issue that would need to address is the jungle, which is typically filled with¬†creeps¬†that players can kill for¬†rewards, which the designers require to define how challenging they should be to defeat. Character Designer and Engineers: they evaluate the design of the heroes to be certain that each team is balanced, and confirm there is no side having an unfair benefit. In such cases will have to rebalance the hero stats with the help of the Gameplay Engineer, and adjust the HUD with the help of a Systems Engineer. User Retention team: by working around the Data Analysts, they will support delivering the final report to the Lead Game Designer, about the potential effect on the players' retention in case of not taking quick actions, in the scenario of having a true Null Hypothesis. PR team and Community Manager: they must be aligned to deliver an accurate¬†Patch note¬†with an accurate summary of the next patch to be released. Players community: as the main stakeholder, the core sustainability of this IP comes from the community engagement, so they expect to have a more balanced experience at least for patch 6.86b. Note: To facilitate the understanding of the roles of the development team, I invite you to take a look at this diagram that I designed. ","date":"2022-07-24","objectID":"/posts/anova_dota/:1:2","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"üì• About the data This dataset was collected on December 17th of 2015 from a sample of¬†Ranked Matchmaking¬†mode, which holds information from 49,855 different contests, where 149,557 different players formed part of them. As we mentioned in the preprocessing notebook, the raw extracted data can be found in the¬†YASP.CO¬†collection from Academic Torrents. This dataset was uploaded and gathered by¬†Albert Cui¬†et. al (2015). However, you can extract your sample from the¬†OpenDota API¬†query interface for the comfort of use. ","date":"2022-07-24","objectID":"/posts/anova_dota/:2:0","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"Collection process and structure According to Albert Cui et. al (2014), OpenDota is a volunteer-developed, open-source service to request DOTA 2 semistructured data gathered from the Steam WebAPI, which has the advantage of retrieving a vast amount of replay files in comparison to the source API. The service also provides a web interface for casual users to browse through the collected data, as well as an API to allow developers to build their applications with it. The present sample can be found in an archive of¬†YASP.CO, which was uploaded for the same developers of the OpenDota project,¬†Albert Cui,¬†Howard Chung, and¬†Nicholas Hanson-Holtry. Due to the size of the data (99 GB), we won‚Äôt find the raw data in the Github repository of this project, however, you will find a random sample with its respective preprocessing explanation in a Jupyter Notebook.1 As a suggestion, to preprocess data of this size, the most accurate solution is to use parallel computing frameworks like Apache Spark to do the transformations. Web services like Databricks with its Lakehouse Architecture, let us process semi-structured data with the capabilities of a Data Warehouse and the flexibility of a Data Lake, and to do this you can use the Data Science and Engineering environment of the¬†Community Edition¬†for free. Without diving deeper, let‚Äôs import the initially needed packages and the preprocessed data. # Basic packages import pandas as pd import numpy as np from math import sqrt, ceil, floor # Plotly packages import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots # Plotly static config static = {'staticPlot': True} # Own package for layout of plots from vizformatter.standards import layout_plotly # Load layout base objects sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) # Import preprocessed data data = pd.read_csv(\"DOTA_data/data.csv\", index_col = 0, na_values=[\"N/A\",\"\", \" \", \"nan\"]) data.head() As we can notice, in our data we have a total of 28 attributes, however, not all of them will be used in this study but for future investigations of extra user metrics, we see that all the assembled data could be a useful asset. In such case you require the description of one of these attributes, please guide yourself with the¬†Appendix at the end of the¬†preprocess notebook, where you will find a punctual explanation. ","date":"2022-07-24","objectID":"/posts/anova_dota/:2:1","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"üîß Data Preprocessing Almost every transformation was made on the preprocessing notebook, there you will encounter different blocs like the joins of hero descriptions, items used per session, and match outcomes to the main table of players' registries. In the end, also will be a distribution study, to exclude outliers based on irregularities recorded on sessions (impossible values). Knowing this, we can say that the data is ready to go, so we can skip the Data Cleaning section. Here we are going to do an overall inspection. ","date":"2022-07-24","objectID":"/posts/anova_dota/:3:0","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"Data Consistency Considering conceivable errors in the telemetry logs, we require to validate the lack of missing values on each one of the features. One of the attributes that required a substantial transformation to infer and extract descriptive values for each player was the ‚Äúplayer_slot‚Äù, which is no more in the present dataset. This value was an 8-bit unsigned integer, where the first digit defined the faction (Radiant | Dire), the next four digits were empty and the last three defined the lane role of the player within the team. The encoding is part of the reason why there were present some missing data that wasn‚Äôt describing the record. After the pruning was executed, the sample remain significant in size with 290,000 records, and by the next table, we can verify it. def nullCounter(data): # Create empty lists columns = [] nulls = [] perc = [] total_rows = data.shape[0] for i in range(len(data.columns)): # Null values null_counter = data.loc[:,data.columns[i]].isnull().sum() # Append only columns with null values if null_counter \u003e 0: # Append values to lists columns.append(data.columns[i]) nulls.append(null_counter) # Generate percentage perc_value = null_counter/total_rows perc.append(\"{:.2%}\".format(perc_value)) # Create DF from lists nullsTable = pd.DataFrame({'Null Values':nulls, 'Fraction':perc}).T nullsTable.columns = columns return nullsTable nullCounter(data= data) The columns with more NA values are the ones relative to the player position. In case of any comparison needed in terms of lane position, we will take another sample ensuring that we have a significant size to run our tests. ","date":"2022-07-24","objectID":"/posts/anova_dota/:3:1","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"Data Distribution Validation Every column is already transformed into its respective data type, and depending on the analysis needed we should require to retransform it or even apply a linear scaling method to be interpreted. data.describe(include= [\"number\"]) We can see each value defined by the percentiles, however, is preferable to have a straightforward vision to conclude about each feature. # Load a new layout for grid axis signGrid, layoutGrid = layout_plotly(height= 900, width= 1200, font_size= 12) # Function to generate grid of boxplots def gridDist(data, layout, sign): # Validate not using ids in the grid for i in data.columns: if (i.endswith('_id')) or (i == 'id'): data[i] = data[i].astype(str) # Extract columns with numeric values numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64'] df = data.select_dtypes(include=numerics) # Calculate grid space, for columns and rows in the layout num_col = len(df.columns) cols = ceil(sqrt(num_col)) rows = cols # Generate subplots grid grid = make_subplots(rows=rows, cols=cols) # Populating spaces with Graphic objects counter = 0 for i in range(1,rows+1): for j in range(1,cols+1): column_name = df.columns[counter] trace = go.Box(x= df[column_name], name= column_name) grid.append_trace(trace, i, j) if counter == num_col-1: break else: counter +=1 # Adding layout conf grid.update_layout(layout) grid.update_layout(showlegend=False) grid.add_annotation(sign) # Disable interactivity by transforming image to bytes static = {'staticPlot': True} return grid.show(config = static) gridDist(data= data, layout= layoutGrid, sign= signGrid) From the dispersal of each attribute, we can notice that most of the data is right-skewed. We don‚Äôt have any information bonded to system anomalies during data collection, so from the variables that we are going to use in our analysis, we can notice descriptive behaviors like: Per session players usually report, on average, a total of 11 assists and 6 kills with a median hero damage score of 10,500. There is a big segment of outliers from players who healed allies, with a value of over 233 points. Making general assumptions over its distribution wouldn‚Äôt be accurate, because that segment could be linked with the Hard Support role. The same situation happens with the gold collected and the tower damage, where there is a great number of scores above the 75% percentile, which can be due to lane role differences like Soft Support which is usually a role used for¬†Jungling. In our exploratory analysis, we will have to study these metrics by lane, because each lane has different roles, and some aren‚Äôt directly comparable. Nevertheless, that‚Äôs also something that we‚Äôll need to demonstrate with a hypothesis test between lanes. ","date":"2022-07-24","objectID":"/posts/anova_dota/:3:2","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"üîç Exploratory Analysis \u0026 In-game interpretations Before going through the hypothesis test, let‚Äôs explore the behavior within the categorical variables to check the consistency within the descriptive attributes for the players on each lane. data.describe(include= [\"object\"]) At first sight, the most common role in our data corresponds to the Offlaner and the most used hero is Windranger, and this makes sense since is one of the heroes with a higher Win Rate on the Offlaner role according to¬†DOTABUFF. Also, the two most used shop items are the¬†Town Portal Scroll¬†and the¬†Blink Dagger. But since we have numerous combinations between heroes and items, this might not be the most accurate way to make inferences. So let‚Äôs look at the combinations for the Radiant and Dire factions. # Get combination of items and heroes def combItemHero(df, side): # Subset Faction/Side data= df[df.side == side] # Subset categorical variables itemHero = data.select_dtypes(include=\"object\") # Exclude unused columns itemHero = itemHero.loc[:,~itemHero.columns.isin(['lane','role','side'])] # Pivot Table itemHero = itemHero.set_index(\"hero_name\").stack()\\ .reset_index() # Add counter itemHero['counter'] = np.ones(itemHero.shape[0], dtype=int) # Drop items column itemHero = itemHero.drop(labels= [\"level_1\"], axis= 1) # Rename columns itemHero.columns = ['hero_name','item', 'value'] # Get combinations DF itemHero = itemHero.groupby(['hero_name','item'])['value'].sum().reset_index()\\ .sort_values('value', ascending= False, ignore_index= True) # Create combination column itemHero['stage'] = itemHero.hero_name + ' with ' + itemHero.item # Reorder columns itemHero = itemHero.loc[:,sorted(itemHero.columns)].drop(columns = ['hero_name','item']) # Add side column itemHero['side'] = side return itemHero # Drop empty hero names data = data[data.hero_name.isnull() == False] # Set new dataframes itemHeroRad= combItemHero(df= data, side= 'Radiant') itemHeroDire= combItemHero(df= data, side= 'Dire') # Concatenate dataframes allItemHero = pd.concat([itemHeroRad, itemHeroDire], axis=0).sort_values('value', ascending= False, ignore_index= True) # Generate Visuals allItemHero = px.funnel(allItemHero.head(17), x='value', y='stage', color='side', color_discrete_sequence= ['#E25822','#2C538F']) allItemHero.update_layout(layout) allItemHero.update_layout(title = {'text':'Combinations recorded of Hero and Items from 50K sessions'}, yaxis_title=\"Combinations\") allItemHero.add_annotation(sign) allItemHero.show(config = static) Now we can verify the initial statement that the Windranger is the most played hero, but at the same time, we can check that the Town Portal Scroll and the Blink Dagger are not the most used for this hero, meaning that their use is because of commonness and not because it‚Äôs a good fit for that hero. Windranger¬†it‚Äôs a ranged hero that can buff her stats by 3.75% in¬†movement speed. According to¬†Tan Guan, these boots supply the most damage out of any boots choice, and the six armor allows an Intelligence hero to have a mana-less option to chase the enemy down. Also, the Phase active brings you up to max movement speed with Windrun (another physical ability for movement speed). The same case is for¬†Shadow Fiend¬†using Power Treads, where some advanced strategies are possible for players who like to micromanage or wish to maximize their efficiency in a competitive setting. Like switching the Power Treads to Intelligence before a fight to allow this hero to cast one more spell. While examining patches, it‚Äôs always important to examine the performance of the character by sampling the player statistics. In such case of DOTA, it would let the designers decide which heroes need to be nerfed and which need to be buffed, by delivering changes in a patch log attached to the bottom side of the¬†documentation website. To put you in context,¬†nerfed¬†means a negative effect on the hero‚Äôs stats, while¬†buffed¬†means the opposite, given an update (patch) on the game. ","date":"2022-07-24","objectID":"/posts/anova_dota/:4:0","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"üó∫Ô∏è Overview of Metrics by Lane Roles We saw that generalizing about all the heroes and extracting insights, it‚Äôs not the best way to produce findings, because many times the perspective can bias our interpretations, especially while dealing with Big Data. As we move on to the next analysis, the main intent is to narrow down our scope of analysis (data sample), so to start let‚Äôs compare the core metrics by role lane. First, let‚Äôs see the scores for how much a player heals their allies' heroes, how much the player damages the enemys' heroes, and how many times each player dies on average, taking as a group the role assigned in the lane. # Function to scale variables using \"Linear Scaling\" def linearScale(array): scaled = (array - array.min()) / (array.max() - array.min()) scaled = round(scaled, 5) return scaled # Subset required columns dataHeal = data.loc[:,data.columns.isin(['role','deaths','hero_healing','hero_damage'])] # Reorder columns dataHeal = dataHeal.loc[:,sorted(dataHeal.columns, reverse= True)] # Extract only the subset of players that healed during session dataHeal = dataHeal.groupby('role').mean().reset_index() # Load a new layout for parallel axis signPar, layoutPar = layout_plotly(height= 700, width= 1500, font_size= 14) # Generate Parallel Coordinates plot parallel = go.Figure(data= go.Parcoords( line= dict(color= [1,2,3,4,5], colorscale= [[0,'#3BE8B0'],[0.25,'#1AAFD0'],[0.5,'#6A67CE'],[0.75,'#FFB900'],[1,'#FC636B']]), dimensions = list([ dict(range = [1,len(dataHeal.role)], tickvals = [1,2,3,4,5], label = 'Role', values = [1,2,3,4,5], ticktext = dataHeal.role), dict(range = [floor(min(dataHeal.hero_healing)), ceil(max(dataHeal.hero_healing))], #tickvals = [0,0.25,0.5,0.75,1], label = 'Healing', values = dataHeal.hero_healing), dict(range = [floor(min(dataHeal.hero_damage)), ceil(max(dataHeal.hero_damage))], #tickvals = [0,0.25,0.5,0.75,1], label = 'Damage', values = dataHeal.hero_damage), dict(range = [floor(min(linearScale(dataHeal.deaths))), ceil(max(linearScale(dataHeal.deaths)))], #tickvals = [0,0.25,0.5,0.75,1], label = 'Scaled Deaths', values = linearScale(dataHeal.deaths)) ]) ) ) # Assign layout for parallel axis parallel.update_layout(layoutPar) parallel.update_layout(margin={\"t\": 125, \"l\": 100}) parallel.update_layout(title = {'text':\"User Metrics comparison by Role Lane\"}) parallel.add_annotation(signPar) parallel.show(config = static) From the last visual we can drag the next insights: Soft support¬†is the role where they dedicate less effort to healing their teammates (due to their freedom is the role used for Jungling), while¬†Midlaners¬†dedicate more time to healing teammates. This is probably due to their high mobility during the session, and because they have the major responsibility to be the team‚Äôs playmakers, according to Zach the DOTA¬†coach. In terms of Damage dealt, we can say that by far Soft Support and Midlaners are the positions that dealt more damage, while¬†Hard Carry¬†is the opposite because they require a clear vision of the momentum between staying in the Safelane with the Hard Support or moving through the Offlane to attack enemy buildings. As being a¬†Hard Support¬†is the less complex role to start, heal more, and damage less. According to Zach the DOTA¬†coach, they need to farm a little and be actively healing their teammates. At the same time, they need to know where to be positioned, cause if one core is farming a lot, you will need to be actively doing the same; if one core is always in action they need to be healing. We saw that Soft Support and the Midlaners have a very similar average of Damage Dealt per session, so we can analyze by checking the Kills logs by hero. ","date":"2022-07-24","objectID":"/posts/anova_dota/:4:1","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"‚ò†Ô∏è Kills logs comparison grouped by Role and Hero The core interaction between enemy players is measured by Kills. We‚Äôll compare the logs by groups; one being grouped by Role and another by Hero, keeping in mind that the main intent is to discover the more buffed heroes since the release of the last patch. # Function to aggregate variables by group def groupGen(data, grouper, variable, top): # Aggregate new dataframe dataCoord = data[[grouper,variable]] dataCoord = dataCoord[dataCoord[grouper].isnull() == False] dataCoord = dataCoord.groupby(grouper).mean().reset_index() # Normalize the data with Linear Scaling dataCoord[variable] = linearScale(dataCoord[variable]) # Resort DF randomly dataCoord = dataCoord.sort_values(by= variable, ascending=False).head(top) dataCoord = dataCoord.sample(frac=1).reset_index(drop=True) return dataCoord # Generate aggregated data for heros and roles heroGroup = groupGen(data= data, grouper= 'hero_name', variable= 'kills', top= 30) roleGroup = groupGen(data= data, grouper= 'role', variable= 'kills', top= 5) # Load a new layout for polar axis signPolar, layoutPolar = layout_plotly(height= 700, width= 1750, font_size= 13) # Create subplot layout, with 2 subplots of polar type polars = make_subplots(rows=1, cols=2, specs=[[{'type': 'polar'}]*2]) # Add subplot graphic objects polars.add_trace(go.Scatterpolar(name = \"Kills by role\", r = roleGroup.kills, theta = roleGroup.role, line_color = \"#007fff\", mode= 'lines'), 1, 1).update_traces(fill='toself') polars.add_trace(go.Barpolar(name = \"Kills by hero\", r = heroGroup.kills, theta = heroGroup.hero_name), 1, 2) # Add layout format polars.update_layout(template=\"plotly_dark\") polars.update_layout(layoutPolar) polars.update_layout(title = {'text':\"Linearly scaled kills log by role and hero\"}) polars.add_annotation(signPolar) polars.show(config= static) In terms of kills, we can justify these stats by looking at the updates given in the changelogs of patch 6.86: Riki: since this hero was introduced with the new ‚ÄòCloak and Dagger‚Äô ability, he can become invisible each time attacks an enemy from behind, which is a perfect combination with ‚ÄòBlink Strike‚Äô which was transformed into basic agility. This second gives the user bonus damage for attacks from behind and also does a quick teleport to the enemy‚Äôs back, giving more to use this combination. Huskar: This hero received a hard magic resistance and attack bonus speed mechanical change with bonus changes from 5% to 50%. Ursa: known as one of the heroes most used to Jungling, in this patch the ‚ÄòAghanim‚Äôs Scepter‚Äô was added to this hero which also let him cast the ‚ÄòEnrage‚Äô ability while being stunned, slept, taunted, hidden, or during forced movement. The main use of ‚ÄòEnrage‚Äô is to reduce the income damage by ~80%. Midlaner and Soft Support are the roles with more kills registered, which are the 2 positions in which Riki, Huskar, Ursa, and Queen of Pain are used because they are in the¬†Carry¬†heroes category. From the last global perspective of heroes and lanes, the purpose of the analysis is to let it evolve as granular as possible to handle detailed deductions, so let‚Äôs compare the Soft Support and Midlaner roles to decide on which to concentrate our study. ","date":"2022-07-24","objectID":"/posts/anova_dota/:4:2","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"üßùüèª‚Äç‚ôÇÔ∏è Two Sample Mann-Whitney Test for Kills logs by Role Lane for Carries We still doubt which of the two roles is the one preferred to use Carry heroes, in other words, those that are used to rank up by logging as many kills as possible. To do this we can run a two-sample t-test, so first let‚Äôs check the normality since this is a must-have assumption. It‚Äôs important to mention that to respect the¬†independence¬†assumption, in the last code we extracted one record per player from a random¬†resampled without replacement¬†data frame. This is in case the player count with multiple sessions logged. For this test, we prefer to have a randomized sample where the groups are¬†mutually exclusive, so a player can only belong to one role group. from scipy import stats # Normality check function def normality_two_sample(data1, data2): stat_1, p_1 = stats.normaltest(data1) stat_2, p_2 = stats.normaltest(data2) # Validate p_value if p_1 \u003c 0.5: if p_2 \u003c 0.5: print('At least one sample is not normal,\\nwhere ' 'the p values are: {p_1:.6f}and {p_2:.6f}'.format(p_1= p_1,p_2= p_2)) else: print('Only Soft_Support sample is normal,\\nwhere ' 'the p values are: {p_1:.6f}and {p_2:.6f}'.format(p_1= p_1,p_2= p_2)) else: if p_2 \u003c 0.5: print('Only Mid sample is normal,\\nwhere ' 'the p values are: {p_1:.6f}and {p_2:.6f}'.format(p_1= p_1,p_2= p_2)) else: print('At least one sample is not normal,\\nwhere ' 'the p values are: {p_1:.6f}and {p_2:.6f}'.format(p_1= p_1,p_2= p_2)) # List of heroes for Carry role carries = ['Shadow_Fiend', 'Ember_Spirit', 'Alchemist', 'Meepo', 'Queen_of_Pain', 'Windranger', 'Spectre', 'Terrorblade', 'Faceless_Void', 'Brewmaster', 'Viper', 'Doom', 'Spirit_Breaker', 'Slardar', 'Juggernaut', 'Phantom_Assassin', 'Slark', 'Templar_Assassin', 'Necrophos', 'Invoker', 'Luna', 'Tidehunter', 'Gyrocopter', 'Clinkz', 'Silencer', 'Sven', 'Lina', 'Drow_Ranger', 'Night_Stalker', 'Zeus', 'Chaos_Knight', 'Weaver', 'Tinker', 'Huskar', 'Troll_Warlord', 'Anti-Mage', 'Axe', 'Wraith_King', 'Kunkka', 'Lycan', 'Bristleback', 'Outworld_Devourer', 'Riki', 'Sniper', 'Magnus', 'Leshrac', 'Phantom_Lancer', 'Ursa', 'Bloodseeker', 'Tiny', 'Medusa', 'Morphling', 'Storm_Spirit', 'Lone_Druid', 'Death_Prophet', 'Razor', 'Naga_Siren'] # Subset data from players in carry roles data_carry = data[data.hero_name.isin(carries)] # To repect independence assumption data_carry = data_carry.sample(frac= 1) data_carry = data_carry.groupby('account_id').first().reset_index() # Extract samples of Mid Laners and Soft Support data_carry_Mid = data_carry[data_carry.role == 'Mid'] data_carry_Soft = data_carry[data_carry.role == 'Soft_Support'] # Call function for normality test normality_two_sample(data1= data_carry_Mid.kills, data2= data_carry_Soft.kills) At least one sample is not normal, where the p values are: 0.000000 and 0.000000 It seems that both distributions are not normal, so we can‚Äôt proceed with a Two Sample t-test. For this reason, we‚Äôll use a two-sample¬†Mann-Whitney U test where our hypothesis will be defined by: $H_0: P(x_{kills_{mid}} \u003e x_{kills_{soft}}) = 0.5$ (The probability that a randomly drawn player in Midlane will record more kills than a Soft Support player, is 50%) $H_1: P(x_{kills_{mid}} \u003e x_{kills_{soft}}) \u003e 0.5$ (The probability that a randomly drawn player in Midlane will record more kills a Soft Support player, is more than 50%) This test makes the following assumptions: The two samples data groups are independent The data elements in respective groups are continuous and not-normal Let‚Äôs start by plotting the distributions. # Load layout base objects sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) # Generate plot for dual distributions def dual_dist_plot(data1, data2, title, xaxis_title, group_names): # Subset group masks group1_name = group_names[0] group2_name = group_names[1] # Generate array of lists and append them data_grouped = [data1, data2] all_data = data1.append(data2).reset_index(drop= True) # Calculate mean mean_group1 = ","date":"2022-07-24","objectID":"/posts/anova_dota/:4:3","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"üßùüèªüëπ Two Sample Mann-Whitney Test for Kills of Mid Lanes and Soft Support vs Rest of Roles Let‚Äôs start by defining the new hypothesis: $H_0: P(x_{kills_{mid \\cup soft}} \u003e x_{kills_{rest}}) = 0.5$ (The probability that a randomly drawn player from Midlane or Soft Support will record more kills than any player, is 50%) $H_1: P(x_{kills_{mid \\cup soft}} \u003e x_{kills_{rest}}) \u003e 0.5$ (The probability that a randomly drawn player from Midlane or Soft Support will record more kills than any player, is more than 50%) Using the same process as before, our first assumption to check is the non-normality of the same random sample for each group. # Subset and create kills ample for carry role in Soft Support and Midlane positions data_soft_mid = data_carry[data_carry.role.isin(['Soft_Support', 'Mid'])] # Subset and create kills sample for carry role in the rest positions data_not_soft_mid = data_carry[data_carry.role.isin(['Hard_Carry', 'Hard_Support', 'Offlaner'])] # Call function for normality test normality_two_sample(data1= data_soft_mid.kills, data2= data_not_soft_mid.kills) At least one sample is not normal, where the p values are: 0.000000 and 0.000000 Now that we know the sample are not normal, we can proceed with the Mann-Whitney Test. dist_groups = dual_dist_plot(data1= data_soft_mid.kills, data2= data_not_soft_mid.kills, title= 'Carry roles Kills log by Position ', xaxis_title= 'Kills log', group_names= ['Mid \u0026 Soft Sppt Kills','Rest. Positions Kills']) dist_groups.update_layout(layout) dist_groups.add_annotation(sign) dist_groups.show(config= static) And as well, to avoid incurring in normality bias due to the samples sizes, we are going to rely on an Asymptotic P-value calculation. print('Mann-Whitney Test P-value: '+ str(mannwhitneyu_est(data1= data_soft_mid.kills, data2= data_not_soft_mid.kills, alt= 'greater', method= 'asymptotic'))) Mann-Whitney Test P-value: 0.015672996714278714 We can see that our P-Value is very close to the rejection section. However, since this is taken each time from a random sample, we can‚Äôt conclude yet. The most accurate way to test this is to bootstrap the value and make judgments over the percentage of p-values that accept or reject our hypothesis. The next function will return one array of bootstrapped estimates for the P-value, which we will describe the next: p_value_mannwhitney:¬†Proportion of¬†P-values measured from the Mann-Whitney Test, showing the probability that a randomly drawn Midlaner or Soft Support will record more or equal quantity of kills than any player. boot_mannwhitney_replicates:¬†Array of bootstrapped p-values of Mann-Whitney Test. Also, let‚Äôs plot the bootstrapped replicates. # Bootstrapping Mann-Whitney U Test, and generate array of replicates def draw_bs_test(data1, data2, iterations): boot_arr = [] for i in range(iterations): estimate = mannwhitneyu_est(np.random.choice(data1, len(data1)), np.random.choice(data2, len(data2)), alt= 'greater', method= 'asymptotic') boot_arr.append(estimate) # Compute and print p-value of estimate with different distributions/medians p_value_test = np.sum([1 if i \u003c= 0.05 else 0 for i in boot_arr]) / len(boot_arr) return boot_arr, p_value_test # Generates bootstrapped estimates boot_mannwhitney_replicates, p_value_mannwhitney = draw_bs_test(data1= data_soft_mid.kills, data2= data_not_soft_mid.kills, iterations= 2000) # ECDF Generator function def ecdf(data): # Generate ECDF (Empirical Cumulative Distribution Function) for one dimension arrays n = len(data) # X axis data x = np.sort(data) # Y axis data y = np.arange(1, n+1) / n return x, y # ECDF plot generator def ecdf_plot(data, var_label): # Generate ECDF data x, y = ecdf(data) # Generate percentile makers percentiles = np.array([5,25,50,75, p_value_mannwhitney*100, 95]) #Plot the percentile where the bootstrapped p-value is located ptiles = np.percentile(data, percentiles) # ECDF plot plot = go.Figure() # Add traces plot.add_trace(go.Scatter(x=x, y=y, mode='markers', name=var_label)) plot.add","date":"2022-07-24","objectID":"/posts/anova_dota/:4:4","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"üßô‚Äç‚ôÇÔ∏è Kruskal‚ÄìWallis One-Way ANOVA First, we are going to try to test the difference between Carry heroes groups for Soft Support and Midlaners using a common One-Way ANOVA. Which is a parametric test which is used to test the statistically significant variance difference between 3 or more groups. In this case, our categorical variable will be the hero name, which will be used to segment the groups, to compare the hero damage factor between levels. To select the hero groups, we can simply add the hero damage metric to use as a decision parameter, for this reason, let‚Äôs see the cumulative metric in a Pareto chart for the Carry heroes. # Load layout for Pareto chart sign_pareto, layout_pareto = layout_plotly(height= 700, width= 1400, font_size= 12) def pareto_builder(data, var_value, var_cat, title, title_xaxis, title_yaxis): # Aggregate data by variable data = data.groupby([var_cat])[var_value].sum().reset_index(drop=False) # Sort data data = data.sort_values(var_value, ascending= False).reset_index(drop=True) # Generate CumSum Percentage cumulative_per = data[var_value] / data[var_value].sum() * 100 cumulative_per = cumulative_per.cumsum() # Append cumulative percentages column data = pd.concat([data, cumulative_per], axis= 1) # Fix column names and remove last repeated value error data.columns = data.columns.insert(-1, 'Cum_Sum')[:-1] # Pareto Chart pareto = make_subplots(specs=[[{\"secondary_y\": True}]]) # Bar plot graphic object pareto.add_trace(go.Bar(x = data[var_cat], y = data[var_value], name = \"Hero\", marker_color= \"#007FFF\"), secondary_y = False) # Scatter+Lines plot graphic object pareto.add_trace(go.Scatter(x = data[var_cat], y = data['Cum_Sum']/100, mode='lines+markers', name = \"Percentage\", marker_color= \"#FF5A5F\"), secondary_y = True) # Layout pareto.update_layout(title = {'text':title}, xaxis = {\"title\":title_xaxis}, yaxis = {\"title\":title_yaxis}) pareto.update_yaxes(tickformat = \"0%\", title = \"Cumulative Percentage\", secondary_y = True) pareto.add_hline(y=0.8, line_dash = \"dash\", line_color=\"red\", opacity=0.5, secondary_y = True) return pareto pareto = pareto_builder(data= data_soft_mid, var_value= 'hero_damage', var_cat= 'hero_name', title= 'Pareto Chart of Hero Damage Logs', title_xaxis= 'Heroes', title_yaxis= 'Total Damage recorded') pareto.update_layout(layout_pareto) pareto.update_layout(showlegend=False) pareto.show() From the 110 different heroes, we‚Äôll consider that 20% of them approximately allocated 80% of the total damage recorded. For this reason, we are going to subset those top 28 heroes that will be under study for our ANOVA test. In the last visual, due to the size of the displayed area, we are not seeing the 110 heroes, but we can see the cumulative sum of damage recorded on the right Y axis of the Pareto Chart. # Aggregate data by group list_sub_heroes = data_soft_mid.groupby(['hero_name'])['hero_damage'].sum().reset_index(drop=False) # Sort data list_sub_heroes = list_sub_heroes.sort_values('hero_damage', ascending= False).reset_index(drop=True) # Extract heroes list list_sub_heroes = list_sub_heroes.head(28).hero_name # Subset data from players for analysis data_heroes = data_soft_mid[data_soft_mid.hero_name.isin(list_sub_heroes)] As we mentioned before, the specificity parameter is our focus point to make the right deductions, so we‚Äôll still narrow our analysis to Carry heroes. The situation is that an ANOVA will tell us only if there is a hero Mean Damage is different from the rest. To clarify the context of why we are going to use an ANOVA test, we have two reasons; the first is to check for potential nerfed heroes in later patches, that‚Äôs why our core KPI will be the damage, which is a continuous attribute; and the second one is that the comparison is made by a hero and fortunately we count with 28 heroes after the subset was made. This way we can define the elements of the ANOVA as: Levels: Hero Names Factor: Hero Damage logs And our Hypothesis will be defined by: $H_0:¬†\\overline{X}_{Wind","date":"2022-07-24","objectID":"/posts/anova_dota/:4:5","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"üóíÔ∏è Final thoughts \u0026 takeaways What can the stakeholders understand and take in consideration? At the beginning we were looking that the most buffed heroes, in terms of kills were different heroes like Riki, Huskar, and Ursa, however, unless we start using statistical tests applied to a behavioral metric like the damage dealt to enemies, we began noticing contrasts, where some heroes were registering higher victory values like Shadow Fiend and others high damage ranges without having superiority in term of sessions succeeded. For this reason, is preferable to base the conclusion upon statistical techniques to study subjective behaviors. What could the stakeholders do to take action? For now, the major concern of the Game Designer, should not be to take immediate action once they see this kind of insight. Instead, they can start collecting more granular data about the registries of damage logs changes, especially when abilities were introduced. Then can study two samples, one after the patch and another before, using the data archived, to run an A/B test to prove its veracity. What can stakeholders keep working on? MOBA games tend to rely their sustainability on their community engagement and this can be studied by the player‚Äôs user experience. Usually, when buffed heroes are used for Jungling, which among DOTA 2 community sometimes is a frowned upon activity, this will make the users feel uncomfortable because of unfair mechanics, so the main solution is to build a Gameplay Metrics dashboard segmented by characters to track closely its behavior and take prompter actions each time a patch is released. ","date":"2022-07-24","objectID":"/posts/anova_dota/:5:0","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"‚ÑπÔ∏è Additional Information About the article This article was developed from the content explained in the Inferential statistics section of Chapter 3 of the ‚ÄòGame Data Science book' and part of the statistical foundation was based on the book ‚ÄòProbability \u0026 Statistics for Engineers \u0026 Scientists‚Äô by Walpole et. al (2016). All the assumptions and the whole case scenario were developed by the author of this article, for any suggestion I want to invite you to go to my about section and contact me. Thanks to you for reading as well. Related Content ‚Äî Dota 2 Protracker Role Assistant by Stratz ‚Äî Game Data Science book and additional info at the¬†Oxford University Press ‚Äî Anders Drachen personal¬†website ‚Äî DOTA 2 Balance of Power Patch Notes Datasets This project was developed with a dataset provided by Albert Cui,¬†Howard Chung, and¬†Nicholas Hanson-Holtry, which also can be found at YASP Academic Torrents or you can experiment with this great OpenDota API interface to make queries. Footnote: We won‚Äôt find the raw data in the Github repository of this project due the storage capabilities, instead you will find a random sample with its respective preprocessing explanation in a Jupyter Notebook.¬†‚Ü©Ô∏é ","date":"2022-07-24","objectID":"/posts/anova_dota/:6:0","tags":["Kruskal‚ÄìWallis ANOVA","Mann-Whitney t-test","Kolmogorov-Smirnov","Bootstrapping","Python"],"title":"Assessing the Impact of Patch Releases on Digital Assets in DOTA 2","uri":"/posts/anova_dota/"},{"categories":["Projects"],"content":"Data Analysis with two sample t-tests and Mann Whitney U, to test a change implemented from a game patch of control mechanics","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Abstract: This publication shows a comprehensive statistical examination of control mechanics within Action RPGs, exploring the hypothesis that alterations in control mechanics substantially affect gameplay outcomes, specifically in terms of quest completion and enemy kills, across varying player experience levels. Utilizing data extracted from the VPAL mod, the research employs a methodological approach that includes data preprocessing, and the application of inferential analysis via t-tests and Mann-Whitney U tests, executed within a Python environment. The findings reveal disparities in gameplay metrics between new and veteran players, attributing these differences to the modifications in control mechanics and their pronounced impact on player experience. This study not only highlights the relationship between control mechanics and gameplay experience but also provides a framework for future research in game design and player interaction analysis. Looking for an interactive experience?\r\rüöÄ Download the Jupyter Notebook, available here\r\r ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:0:0","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"‚ö†Ô∏è Introduction to problem ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:1:0","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Hypothesis The Producers are concerned for some comments about a change made in the¬†control mechanics¬†since the last patch was released. The UX research team has communicated to them that the QA testers think that some of them found the quest‚Äôs difficulty too easy and others very leveraged, especially for veteran players of the saga, reason why we can start inferring about changes made in the gameplay experience, which the major changes made were on the mechanics. The problem is that the QA team is very diversified between experienced and non-experienced testers, so it won‚Äôt be easy to make conclusions. For this reason, we need to look for statistical evidence to validate a hypothesis based on the profile of those testers. Now let‚Äôs establish the main hypothesis for the analysis: $H_0:$ Experienced players/testers will have completed¬†more¬†quests and more kills (the usual), the mechanical changes were not significant. $H_1:$ Inexperienced players/testers will have completed¬†more or an equal¬†quantity of quests and kills (not usual), the mechanical changes were significant. From all the KPIs that you will see next, it‚Äôs important to mention that we‚Äôll bring kills as a secondary validation measure since Fallout it‚Äôs a survival-RPG-like where you have to kill radioactive NPCs in almost every quest. We won‚Äôt count with a difficulty category for each player, so the kills attribute will be one of the few not affected by the difficulty categorization, also because it‚Äôs an end measure, not a progressive one, like shots for example. ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:1:1","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Potential Stakeholders There is an unconformity in the changes made in the control mechanics, the team requires the assistance of a Data Analyst to perform a test and validate if the QA tester‚Äôs statements make sense. On the other side of the table we have some stakeholders on the development and testing side: Gameplay Engineers: They need to know if the control mechanics are affecting the interactions of the NPCs with the progress of the players since the release of the last patch. Level Designer: As the whole RPG is designed as quests on an isolated map, they need to know if these are balanced in the sequence or chapter located, or if there is a significant change in the patch that can explain the unconformity. QA Testers: They are a key source of information while communicating the QA statement to the UX Designer. UX Designer: They are working side by side with the consumer insights team and the testers to find a clear current situation and take immediate actions if required, by giving the final report to the level designers and the programmers. Note: To facilitate the understanding of the roles of the development team, I invite you to take a look at this diagram that I designed. ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:1:2","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üì• About the data \u0026 preprocessing reference The data was obtained from a mod developed at the PLAIT (Playable Interactive Technologies) by Northeastern University. The mod is called VPAL: Virtual Personality Assessment Lab, and also can be accessed to the raw telemetry data in the Game Data Science book. ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:2:0","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Collection process and structure Before start let‚Äôs import the libraries we‚Äôre going to need for the preprocess. import numpy as np import pandas as pd # Visuals import plotly.express as px import plotly.figure_factory as ff # Own package from vizformatter.standards import layout_plotly # Load layout base objects sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) The instrumentation of the data and storage of all game actions within the game were done through TXT files that are stored on the client side per player, with an unstructured format. For each one, we created a file named [participantNumber].txt, which includes all session data for that participant. If the participant has more than one session, a folder is created, and multiple files are created for that participant. The transfer process can be made by using a product like Databricks, to run an entire pipeline over a PySpark Notebook. As a demonstration we made an example in a Jupyter notebook, where we got several methods to target, extract, process singles labeled activities, and merge into activity-designated CSVs, which were originally parsed from TXT files by player record. In reality the best practices show that the data can be in a JSON file, stored in a MongoDB, or an SQL database. The collection was an extensive process so you can access the simulation right¬†here¬†with a complete explanation of the bad practices made and how we tackled them. Also, it‚Äôs important to mention that the data was parsed into a single data frame with counted activities per player. After this, we applied a Likert scale to an experience metric according to Anders Drachen et al. (2021), which helped us to separate the data into Experienced and Inexperienced players. First let‚Äôs check our data. # Inexperienced group of players dataframe InExperienced = pd.read_csv(\"data/ExpNeg_GameData.csv\") # Experienced group of players dataframe Experienced = pd.read_csv(\"data/ExpPos_GameData.csv\") Experienced.head() Both data frames have 17 attributes and, the Experienced data have 28 rows, while the Inexperienced data has 42 rows. In our case the attributes in use will be ‚ÄúQuest Completed‚Äù and ‚ÄúKills‚Äù will be the variables to test. Quest Completed: Integer number counting the number of quests the tester completed during the session Kills: Number of kills registered by the player during the session ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:2:1","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üîß Data Preprocessing Before moving to our analysis it‚Äôs important to validate that our data is usable in order to make correct conclusions for the analysis. ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:3:0","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Data Cleaning \u0026 Consistency First let‚Äôs validate the data types and the ranges for both groups. InExperienced.loc[:,[\"User ID\",\"Kills\", \"Quest Completed\"]].describe() Experienced.loc[:,[\"User ID\",\"Kills\", \"Quest Completed\"]].describe() As we saw there is no problems linked to the data types, because in the data parsing noted above we took care of it. And from the ranges we infer that the maximum number of Kills registered is from an Inexperienced player with 44, but at the same time 75% of the Experienced players register a higher number in comparison to the Inexperienced ones, the same case apply to the number of Quests Completed. ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:3:1","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üîç Inferential Analysis \u0026 In-game interpretations ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:4:0","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Descriptive statistics Now, we got the next conclusions about their distribution and measurement: User ID Interpretation: Unique and counts with 70 distinct values which make sense since there is a player associated to each one of the data-row parsed Data type: Nominal transformed to numerical Measurement type: Discrete/String transformed to integer Quest Completed Interpretation: Not unique and counts the quests registries per player, and at first sight make sense that the Experienced players show higher numbers than the Inexperienced ones Data type: Numerical Measurement type: Integer Kills Interpretation: Not unique and counts the quests registries per player, and at first sight make sense that the Experienced players show higher numbers than the Inexperienced ones Data type: Numerical Measurement type: Integer For the next test plots, we will plot a simulated graph showing how fitted is the actual distribution to an ideal normal curve from the same data, that‚Äôs why we are going to create two functions, one for the new distribution calculation from the median and another to plot it over the original data. This will be our general approach because this plot will also let us see more clearly the existence of skewing elements. import math # Generate simulated-std based on the median def median_std(data): # Number of observations n = len(data) # Median of the data median = data.median() # Square deviations deviations = [(x - median) ** 2 for x in data] # Variance variance = sum(deviations) / n return math.sqrt(variance) # Function to generate a plot of a vector to compare it vs a fitted normal distribution def normalplot(data, var_name): # Generate subset of dataframe variable = data[var_name] # Max and min from variable max_var = max(variable) min_var = min(variable) # Create a random array of normally distributed number with the same parameter as the original data y_fit = np.random.normal(loc= variable.median(), scale= median_std(variable), size= len(variable)) # Max and min from simulated curve max_fit = max(y_fit) min_fit = min(y_fit) # Group data together hist_data = [variable, y_fit] group_labels = [var_name, 'Median-Simulated \u003cbr\u003e Normal Curve'] colors = [\"#007FFF\",\"#FF5A5F\"] # Create distplot with custom bin_size plot = ff.create_distplot(hist_data, group_labels, bin_size=[0.75,0.75], show_rug=False, show_hist=True, colors= colors) # Update layout plot.update_layout(layout) plot.update_layout(title = {'text':'Density Plot of '+var_name+' vs a Normal Distribution'}, xaxis = {\"title\":var_name}, yaxis = {\"title\":\"Density\"}) plot.update_xaxes(range=[min_var, max(max_var,max_fit)]) plot.add_annotation(sign) return plot ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:4:1","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"‚öîÔ∏è Two Sample T-Test for Quests Completed by Experience Group A two sample t-test is used to test whether there is a significant difference between two groups means, where the hypothesis will be: $H_0: \\mu_{ex} = \\mu_{inex}$ (population mean of ‚ÄúExperienced Players Completed Quests‚Äù is equal to ‚ÄúInexperienced Players Completed Quests‚Äù) $H_1: \\mu_{ex} \\ne \\mu_{inex}$ (population mean of ‚ÄúExperienced Players Completed Quests‚Äù is different from ‚ÄúInexperienced Players Completed Quests‚Äù) This test makes the following assumptions: The two samples data groups are independent The data elements in respective groups follow any normal distribution The given two samples have similar variances (homogeneity assumption) In this case, both groups are independent since none of them provide information about the other and vice versa. Normality Check First let‚Äôs visualize the distribution for the Experienced players. expQuestnorm = normalplot(data= Experienced, var_name= 'Quest Completed') expQuestnorm.update_layout(title = {'text':'Density Plot of Quest Completed by Experienced Players'}) expQuestnorm.show() And the distribution for Inexperienced players. inQuestnorm = normalplot(data= InExperienced, var_name= 'Quest Completed') inQuestnorm.update_layout(title = {'text':'Density Plot of Quest Completed by Inexperienced Players'}) inQuestnorm.show() Notice that the data is normal for both cases and in terms of the quantity of ‚Äúquests completed‚Äù by the experienced player‚Äôs data is slightly higher than the inexperienced ones. Variance Homogeneity First let‚Äôs see the variance for each group of testers. # Print the variance of both data groups print(\"Quest Completed variance for Experienced players: \" + str(np.var(Experienced[\"Quest Completed\"])) + \"\\n\", \"Quest Completed variance for Inexperienced players: \" + str(np.var(InExperienced[\"Quest Completed\"])) + \"\\n\") Quest Completed variance for Experienced players: 3.25 Quest Completed variance for Inexperienced players: 2.283446712018141 In the good practice the correct way to do this validation is by a Homogeneity Test, so let‚Äôs make a Barlett test since our data is normal (for non-normal is preferred Levene‚Äôs). So let‚Äôs start from the next hypothesis. $H_0: \\sigma^2_{ex} = \\sigma^2_{inex}$ (The variances are equal across in both groups) $H_1: \\sigma^2_{ex} \\ne \\sigma^2_{inex}$ (The variances are not equal across in both groups) # Import the bartlett method from scipy.stats import bartlett # Bartlett's test in Python with SciPy: bartlett(Experienced[\"Quest Completed\"], InExperienced[\"Quest Completed\"]) BartlettResult(statistic=1.0905002879637673, pvalue=0.29636038782360125) Our P-value is above 0.05, so we have enough statistical evidence to accept the hypothesis that the variances are equal between the Experienced testers and the Inexperienced players. Now that we checked that we have normal data and homogeneity in our variances, let‚Äôs continue with our two sample t-test. # Import the stats module import scipy.stats as stats # Perform the two sample t-test with equal variances stats.ttest_ind(a=Experienced[\"Quest Completed\"], b=InExperienced[\"Quest Completed\"], equal_var=True) Ttest_indResult(statistic=3.8261587063084392, pvalue=0.0002855175095244425) We have enough statistical evidence to reject the null hypothesis, the population mean of Quests Completed by ‚ÄúExperienced Players‚Äù is significantly different from the ‚ÄúInexperienced Players‚Äù. As a supposition, we can say that the difficulty selected by the player can be a factor affecting the Completion of the Quest. Considering that the difficulty levels in Fallout New Vegas can vary by two variables which are ‚ÄúDamage taken from Enemy‚Äù and ‚ÄúDamage dealt with Enemy‚Äù, without mentioning the Hardcore mode which was excluded from the samples. The difficulty is an important factor to consider since an experienced player can know what is the level of preference, while for an inexperienced player is a ‚Äútry and fail situation‚Äù, so for a future study it‚Äôs imp","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:4:2","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üíÄ Two Sample Mann-Whitney Test for Kills by Experience Group A two sample Mann-Whitney U test is used to test whether there is a significant difference between two groups distributions by comparing medians, where the hypothesis will be: $H_0: P(x_{ex} \u003e x_{inex}) = 0.5$ (population median of ‚ÄúExperienced Players Kills‚Äù is equal to ‚ÄúInexperienced Players Kills‚Äù, same distribution) $H_1: P(x_{ex} \u003e x_{inex}) \\ne 0.5$ (population median of ‚ÄúExperienced Players Kills‚Äù is different from ‚ÄúInexperienced Players Kills‚Äù, different distribution) In other words, the null hypothesis is that, ‚Äúthe probability that a randomly drawn member of the first population will exceed a member of the second population, is 50%‚Äù. This test makes the following assumptions: The two samples data groups are independent The data elements in respective groups are continuous and not-normal It‚Äôs the equivalent of a two sample t-test without the normality assumption. Also, both groups are independent since none of them provide information about the other and vice-versa. Normality Check First let‚Äôs visualize the distribution for the Experienced players. expKillsnorm = normalplot(data= Experienced, var_name= 'Kills') expKillsnorm.update_layout(title = {'text':'Density Plot of Kills by Experienced Players'}) expKillsnorm.show() And for Inexperienced players. inKillsnorm = normalplot(data= InExperienced, var_name= 'Kills') inKillsnorm.update_layout(title = {'text':'Density Plot of Kills by Inexperienced Players'}) inKillsnorm.show() Both groups show a clear violation of the normality principle, where the data is highly skewed to the right. For the Inexperienced players, despite considering excluding the players with 35 or more kills registered, it wouldn‚Äôt be significant, it will still remain skewed to the right. In cases where our data don‚Äôt have a normal distribution, we need to consider¬†non-parametric¬†alternatives to make assumptions. That is the main reason why we are going to use a¬†Mann Whitney U Test. from scipy.stats import mannwhitneyu def mannwhitneyu_est(data1, data2): statistic, p = mannwhitneyu(data1, data2, alternative = 'two-sided') return p print(mannwhitneyu_est(Experienced.Kills, InExperienced.Kills)) 0.0020247429887724775 Finally we reject the null hypothesis, since our p-value is less than 0.05 with a 95% of confidence. This means that population median of ‚ÄúExperienced Player Kills‚Äù is different from ‚ÄúInexperienced Player Kills‚Äù, the same we can conclude from the distribution. ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:4:3","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üîÉ Two-sample bootstrap hypothesis test for Kills metric After some meetings with the Level Designers, they aren‚Äôt convinced to take action upon the recommendations given from a sample size of 28 and 42 for the Experienced and Inexperienced groups respectively. So they require us to take another type of validation to ensure the confidence of our conclusion. Unfortunately, the Programmers require to get that insight as soon as possible to start fixing the patch. So, the only alternative to satisfy both parts is to perform a bootstrapping with the means and the p-value of the past test. Bootstrapping is one of the best choices to perform better results in sample statistics when we count with few logs. This Statistical Inferential Procedure works by resampling the data. We will take the same values sampled by group, from a resampling with replacement, to an extensive dataset to compare the means between the Kills metric of Experienced players and Inexperienced players. On the other side, we¬†can‚Äôt¬†use a permutation of the samples, since we can‚Äôt conclude that both groups' distributions are the same. Before this let‚Äôs create our bootstrapping replicates generator function. # Bootstrapping Means Function def draw_bs_reps(data,func,iterations): boot_arr = [] for i in range(iterations): boot_arr.append( func(data = np.random.choice(data, len(data))) ) return boot_arr # Bootstrapping Mann-Whitney Test def draw_bs_test(data1, data2, iterations): boot_arr = [] for i in range(iterations): estimate = mannwhitneyu_est(np.random.choice(data1, len(data1)),np.random.choice(data2, len(data2))) boot_arr.append(estimate) return boot_arr And with the next function we will create our new estimates for the empirical means and the values of the Mann-Whitney U Test. This function will return two p-values and two arrays of bootstrapped estimates, which we will describe next: p_emp: P-value measured from the estimated difference in means between both groups, showing the probabilities of randomly getting a ‚ÄúKills‚Äù value from the Experienced that will be greater than one of the Inexperienced group bs_emp_replicates: Array of bootstrapped difference in means p_test: P-value measured from the estimated probability of getting a statically significant p-value from the Mann-Whitney Test (p-value \u003c 0.05), in other words how many tests present different distribution for both groups bs_test_replicates: Array of bootstrapped p-values of Mann-Whitney Test from statistics import mean # Bootrapping probability estimates (p-value) def twosampleboot(data1, data2, operation, iterations): # Compute n bootstrap replicates from the groups arrays bs_replicates_1 = draw_bs_reps(data1, operation, iterations= iterations) bs_replicates_2 = draw_bs_reps(data2, operation, iterations= iterations) # Get replicates of empirical difference of means bs_emp = pd.Series(bs_replicates_1) - pd.Series(bs_replicates_2) # Compute empircal difference in means for p-value, where of Experienced-Kills \u003e InExperienced-Kills p_emp = np.sum(bs_emp \u003e= 0) / len(bs_emp) # Get replicates of Mann-Whitney U Test bs_test = draw_bs_test(data1= Experienced.Kills, data2= InExperienced.Kills, iterations= iterations) # Compute and print p-value of estimate with different distributions/medians p_test = np.sum([1 if i \u003c= 0.05 else 0 for i in bs_test]) / len(bs_test) return p_emp, bs_emp, p_test, bs_test # Return p-values and bs_replicates p_emp, bs_emp_replicates, p_test, bs_test_replicates = twosampleboot(Experienced.Kills, InExperienced.Kills, operation= mean, iterations= 10000) print('Diff. Means p-value =', p_emp, '\\n', 'Mann-Whitney Test p-value =',p_test) Diff. Means p-value = 0.9852 Mann-Whitney Test p-value = 0.8991 After making a sample of 10,000 replicates, we can conclude that 98% of the time, the disparity in the Kills average will be higher in Experienced Players than in Inexperienced players, and from that replicates 90% of them will present a significant difference between distributions. Now let‚Äôs ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:4:4","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üóíÔ∏è Final thoughts \u0026 takeaways What can the stakeholders understand and take in consideration? There is a significant advantage of veteran players over the new ones, giving us the conclusion that the changes in mechanics made from the last Fallout New Vegas patch still very similar and don‚Äôt present meaningful changes, reason why veteran players are finding it easy and have a clear advantage like it‚Äôs usual, so for now the programmers should not be concerned about fixing the patch. What could the stakeholders do to take action? The Level Designers can consider working side by side with the UX team, to make recurrent revisions before launching a new patch, because always the community will expect an experience-centric work of the whole development team in terms of the quality delivered. Also for a future study we could consider to gather the difficulty category and merge it into the dataset and see if this variable is producing a significant variability in our final output. What can stakeholders keep working on? For significant changes in game mechanics, is recommended to do it just under a critical situation, like when is a significant bug affecting the player experience, otherwise it‚Äôs preferable to leave it until the launch of the next title reveal and test the audience reaction before the final implementation, if it‚Äôs possible. ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:5:0","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"‚ÑπÔ∏è Additional Information About the article This article was developed from the content explained in the Inferential statistics section of Chapter 3 of the Game Data Science book. All the conclusions made were inspired by a player-profiling from in-game metrics by using deductive reasoning where we assumed, and then we proved it using significance, confidence and variance, through Inferential analysis. All the assumptions and the whole case scenario were developed by the author of this article, for any suggestion I want to invite you to go to my about section and contact me. Thanks to you for reading as well. Related Content ‚Äî Game Data Science book and additional info at the Oxford University Press ‚Äî Anders Drachen personal¬†website Datasets This project was developed with a dataset provided by Anders Drachen et. al (2021), respecting the author rights of this book the entire raw data won‚Äôt be published, however, you can access the transformed data in my¬†Github repository. ","date":"2022-06-22","objectID":"/posts/control_mechanics_fallout/:6:0","tags":["Mann-Whitney t-test","Bootstrapping","Python"],"title":"Exploring the Impact of Control Mechanics on User Experience in Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Exploration of difficulty levels in a sweet episode of Candy Crush Saga, by making use of Bernoulli Principles","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Abstract: This study explores the impact of game difficulty on player engagement within mobile Puzzle Games, positing that the level of challenge significantly influences player retention and satisfaction. Utilizing Python and Plotly for data preprocessing and visualization, the study analyzes player attempts and success rates across various levels, based on a dataset provided by Rasmus Baath. The findings reveal a generally well-balanced difficulty spectrum, advocating for the implementation of dynamic difficulty adjustments to optimize engagement. The conclusion underscores the importance of maintaining diverse difficulty levels to minimize player attrition, highlighting a subset of levels that pose exceptional challenges. Looking for an interactive experience?\r\rüöÄ Download the Jupyter Notebook, available here\r\r ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:0:0","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"‚ö†Ô∏è Introduction to problem ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:1:0","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Hypothesis We‚Äôll review a game that potentially can lead any developer to many unseen problems, considering the abundance of levels. From the perspective of a customer, there can be several points of view that can emerge and, at the same time, can become unnoticed. That‚Äôs why our diagnosis will start from 2 potential hypothesis: $H_0:$ The game is too easy so it became boring over time. $H_1:$ The game is too hard so the players leave it and become frustrated. ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:1:1","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Potential Stakeholders None of the past hypotheses are the main intentions of the developers. So they require a Data Analyst to help with this task since the developers are seeing only the backend factors affecting the game, but it‚Äôs also critical to consider those external ones that affect the experience for the player and the sustainability of this game for the company. Among the main stakeholders could be: Level Designers: They work aligned with the rest of the Engineering Team because they still have a backend perspective and their next patch release needs to be aligned with the insights given by the analyst. Mobile Designer \u0026 User Retention Expert: This is a game whose main income input is in-game purchases because it‚Äôs a F2P, the main source of income is centered in retain the engagement in the game and keeping the consumers on the platform. Gameplay Engineer: They require to start working on the difficulty adjustment patch as soon as they receive the final statement. Executive Producer: Besides Candy Crush being an IP with internal producers since it‚Äôs developed and published by King, the parent company will expect to have an ROI aligned with their expectations. Players' community: They expect to have an endurable and great experience with a brief response in case of disconformities. Note: To facilitate the understanding of the roles of the development team, I invite you to take a look at this diagram that I designed. ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:1:2","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"üì• About the data ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:2:0","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Collection process and structure Before start let‚Äôs import the libraries we‚Äôre going to need import pandas as pd import numpy as np # For visualization import matplotlib.pyplot as plt %matplotlib inline import plotly.express as px import plotly.graph_objects as go # Own layout design library from vizformatter.standards import layout_plotly # Load layout base objects sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) Due to the extensive size of possible episodes to analyze, we‚Äôll limit the analysis to just one of them, which exactly will have data available for 15 levels. To do this, the analysts need to request a sample from the telemetry systems to get data related to the number of attempts per player in each episode level. Also, it‚Äôs important to mention that in terms of privacy, this analysis requires importing the data with the player_id codified for privacy reasons. Fortunately, in this case, Rasmus Baath, Data Science Lead at¬†castle.io, provided us with a Dataset with a sample gathered in 2014. df = pd.read_csv(\"datasets/candy_crush.csv\") df.head() We can see that our data is structured and consists of 5 attributes: player_id: a unique player id dt: the date level: the level number within the episode, from 1 to 15 num_attempts: number of level attempts for the player on that level and date num_success: number of level attempts that resulted in a success/win for the player on that level and date ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:2:1","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"üîß Data Preprocessing Before starting the analysis we need to do some validations on the dataset. # Count and display the number of unique players print(\"Number of players: \\n\", df.player_id.nunique(), '\\n', \"Number of records: \\n\", len(df.player_id),'\\n') # Display the date range of the data print(\"Period for which we have data: \\nFrom: \", min(df.dt), ' To:', max(df.dt)) Number of players: 6814 Number of records: 16865 Period for which we have data: From: 2014-01-01 To: 2014-01-07 ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:3:0","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Data Cleaning The data doesn‚Äôt require any kind of transformation and the data types are aligned with their purpose. print(df.dtypes) player_id object dt object level int64 num_attempts int64 num_success int64 dtype: object ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:3:1","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Data Consistency The usability of the data it‚Äôs rather good, since we don‚Äôt count with ‚ÄúNAN‚Äù (Not A Number), ‚ÄúNA‚Äù (Not Available), or ‚ÄúNULL‚Äù (an empty set) values. # Function the plot the percentage of missing values def na_counter(df): print(\"NaN Values per column:\") print(\"\") for i in df.columns: percentage = 100 - ((len(df[i]) - df[i].isna().sum())/len(df[i]))*100 # Only return columns with more than 5% of NA values if percentage \u003e 5: print(i+\" has \"+ str(round(percentage)) +\"% of Null Values\") else: continue # Execute function na_counter(df) NaN Values per column: None By this way, we can conclude that there were not errors in our telemetry logs during the data collection. ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:3:2","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Normalization Next, we can conclude there were no impossible numbers, except for a player that tried to complete the level 11 in 258 attempts with just 1 success. This is the only registry we exclude since it can be an influential outlier and we don‚Äôt rely on more attributes about him to create conclusions. Noticing the distribution of the quartiles and comprehending the purpose of our analysis, we can validate that the data is comparable and doesn‚Äôt need transformations. df = df[df.num_attempts != 258] df.describe() ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:3:3","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"üîç Exploratory Analysis \u0026 In-game interpretations ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:4:0","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Summary statistics Excluding the outliers we mentioned before, we got the next conclusions about their distribution and measurement: player_id Interpretation: Not unique and counts with 6814 distinct values which make sense since there is a player with records of multiple levels Data type: Nominal Measurement type: Discrete/String dt Interpretation: Only includes data from January 1st to January 7th of 2014. Also, the analysis won‚Äôt consider this as a lapse per player since the records per player are not continuous, so they will be limited as a timestamp Data type: Ordinal Measurement type: Temporal level Interpretation: They‚Äôre registered as numbers, but for further analysis will be transformed as factors. 50% of the records are equal to or less than level 9 Data type: Ordinal Measurement type: Discrete/String num_attempts Interpretation: The registries are consistent, the interquartile range mention that half of the players try between 1 and 7 time to complete each level. Furthermore, there are players with 0 attempts, so we need to evaluate if this is present at level 1, which can explain a problem in retention rate for that episode Data type: Numerical Measurement type: Integer num_success Interpretation: Most of the players are casual gamers because 75% of them complete the level and don‚Äôt repeat it Data type: Numerical Measurement type: Integer ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:4:1","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Levels played in Episode First, let‚Äôs examine the number of registries per player. This will tell us, from the episode how many levels have each player recorded in the lapse of 7 days. from plotly.subplots import make_subplots # Group data of amount of levels recorded by player id countdf = df.groupby('player_id')['level'].nunique().reset_index() # Count the number amount of players according to amount of levels recorded by player countdf = countdf.groupby('level')['player_id'].nunique().reset_index() # Arrange data according to amount of levels countdf.level = [str(i)+'s' for i in countdf.level] countdf = countdf.sort_values('player_id', ascending= False) # Generate CumSum cumulative_per = countdf.player_id / countdf.player_id.sum() * 100 cumulative_per = cumulative_per.cumsum() # Format new DF countdf = pd.concat([countdf, cumulative_per], axis = 1) countdf.columns = [\"levels\",\"players\",\"Cum_per\"] # Pareto Chart linec = make_subplots(specs=[[{\"secondary_y\": True}]]) # Bar plot graphic object linec.add_trace(go.Bar(x = countdf.levels, y = countdf.players, name = \"Players\", marker_color= \"#007FFF\"), secondary_y = False) # Scatter plot graphic object linec.add_trace(go.Scatter(x = countdf.levels, y = countdf.Cum_per/100, mode='lines+markers', name = \"Percentage\", marker_color= \"#FF5A5F\"), secondary_y = True) # Layout linec.update_layout(title = {'text':'Pareto Chart of Number of Levels recorded by player'}, xaxis = {\"title\":\"Number of Levels recorded\"}, yaxis = {\"title\":\"Unique players\"}) linec.update_layout(layout) linec.update_yaxes(tickformat = \"0%\", title = \"Cumulative Percentage\", secondary_y = True) linec.update_layout(showlegend=False) linec.add_hline(y=0.8, line_dash = \"dash\", line_color=\"red\", opacity=0.5, secondary_y = True) linec.add_annotation(sign) linec.show() From the last Pareto chart, we can deduce that 80% of the 6814 players just count with 3 levels recorded of 15. But, since this was extracted from a random sample, this won‚Äôt affect our study. ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:4:2","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Difficulty of completing a level in a single try There is a combination of easier and challenging levels. Chance and skills make the number of attempts required to pass a level different from one player to another. The presumption is that difficult levels demand more tries on average than easier ones. That is, the harder a level is the lower the likelihood to pass that level in a single try. In these circumstances, the¬†Bernoulli process might be useful. As a Boolean result, there are only two possibilities, win or lose. This can be measured by a single parameter: $p_{win} = \\frac{\\Sigma wins }{\\Sigma attempts }$: the probability of completing the level in a single attempt Let‚Äôs calculate the difficulty $p_{win}$ individually for each of the 15 levels. difficulty = df.groupby('level').agg(attempts = ('num_attempts', 'sum'), wins =('num_success', 'sum')).reset_index() difficulty['p_win'] = difficulty.wins / difficulty.attempts difficulty We have levels where 50% of players finished on the first attempt and others that are the opposite. But let‚Äôs visualize it through the episode, to make it clear. # Lineplot of Success Probability per Level line1 = px.line(difficulty, x='level', y=\"p_win\", title = \"Probability of Level Success at first attempt\", labels = {\"p_win\":\"Probability\", \"level\":\"Episode Level\"}) line1.update_layout(layout) line1.update_xaxes(range=[1,15], tick0 = 1, dtick = 1) line1.update_yaxes(range=[0,0.7], tick0 = 0, dtick = 0.1) line1.update_layout(yaxis_tickformat = \"0%\") line1.add_annotation(sign) line1.show() ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:4:3","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Defining hard levels It‚Äôs subjective what we can consider a hard level because not consistently depends on a single factor and for all player profile groups this can be different. So, for the outcomes of this study, we will arbitrarily assume that a difficult level is the one that has a probability to be completed in the first attempt of a 10% ($p_{win} \u003c 10%$). # Lineplot of Success Probability per Level line2 = go.Figure(go.Scatter( x = difficulty.level, y = difficulty.p_win)) line2.update_layout(title = {'text':'Probability of Level Success at first attempt with Hard Levels'}, xaxis = {\"title\":\"Episode Level\"}, yaxis = {\"title\":\"Probability\"}) line2.update_layout(layout) line2.update_xaxes(range=[1,15], tick0 = 1, dtick = 1) line2.update_layout(yaxis_tickformat = \"0%\") line2.update_layout(showlegend=False) line2.add_hrect(y0=0.02, y1=0.1, line_width=0, fillcolor=\"red\", opacity=0.2) line2.add_annotation(sign) line2.show() From our predefined threshold, we see that the level digit is not aligned with its difficulty. While we have hard levels as 5, 8, 9, and 15; others like 13 and 15 are unleveraged and need to be rebalanced by the level designers. ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:4:4","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Measuring the uncertainty of success We should always report some calculation of the uncertainty of any provided numbers. Simply, because another sample will give us little different values for the difficulties measured by level. Here we will simply use the¬†Standard error¬†as a measure of uncertainty: $\\sigma_{error} \\approx \\frac{\\sigma_{sample}}{\\sqrt{n}}$ Here n is the number of datapoints and $\\sigma_{sample}$ is the sample standard deviation. For a Bernoulli process, the sample standard deviation is: $\\sigma_{sample} = \\sqrt{p_{win}(1-p_{win})}$ Therefore, we can calculate the standard error like this: $\\sigma_{error} \\approx \\sqrt{\\frac{p_{win}(1-p_{win})}{n} }$ Consider that every level has been played n number of times and we have their difficulty $p_{win}$. Now, let‚Äôs calculate the standard error for each level of this episode. # Computing the standard error of p_win for each level difficulty['error'] = np.sqrt(difficulty.p_win * (1 - difficulty.p_win) / difficulty.attempts) difficulty We have a measure of uncertainty for each levels' difficulty. As always, this would be more appealing if we plot it. Let‚Äôs use¬†error bars¬†to show this uncertainty in the plot. We will set the height of the error bars to one standard error. The upper limit and the lower limit of each error bar should be defined by: $p_{win} \\pm \\sigma_{error}$ # Lineplot of Success Probability per Level line3 = px.line(difficulty, x='level', y=\"p_win\", title = \"Probability of Level Success at first attempt with Error Bars\", labels = {\"p_win\":\"Probability\", \"level\":\"Episode Level\"}, error_y=\"error\") line3.update_layout(layout) line3.update_xaxes(range=[0,16], tick0 = 1, dtick = 1) line3.update_yaxes(range=[0,0.65], tick0 = 0, dtick = 0.1) line3.update_layout(yaxis_tickformat = \"0%\") line3.add_hrect(y0=0.02, y1=0.1, line_width=0, fillcolor=\"red\", opacity=0.2) line3.add_annotation(sign) line3.show() Looks like the difficulty estimates a very exact. Furthermore, for the hardest levels, the measure is even more precise, and that‚Äôs a good point because from this we can make valid conclusions based on that levels. As a curious fact, also we can measure the probability of completing all the levels of that episode in a single attempt, just for fun. # The probability of completing the episode without losing a single time prob = 1 for i in difficulty.p_win: prob = prob*i # Printing it out print(\"Probability of Success in one single attempt \\nfor whole episode: \", prob*100, \"%\") Probability of Success in one single attempt for whole episode: 9.889123140886191e-10 % ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:4:5","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"üóíÔ∏è Final thoughts \u0026 takeaways What can the stakeholders understand and take into consideration? From the sample extracted we conclude that just 33% of the levels are considered of high difficulty, which it‚Äôs acceptable since each episode counts with 15 levels, so by now the level designer should not worry about leveling the difficulty. What could the stakeholders do to take action ? As a suggestion, in the case that the Publisher decides to invest more in in-game mechanics, a solution for a long-time and reactive engagement could be the use of Machine Learning to generate a DGDB as some competitors have adapted in IPs like EA Sports FIFA, Madden NFL or the ‚ÄúAI Director‚Äù of Left 4 Dead. What can stakeholders keep working on? The way their level difficulty design work today is precise since our first hypothesis was that the game wasn‚Äôt too linear to unengaged the player and churn as consequence. Because as we saw, the game has drastic variations in the levels 5-6 and 8-10, which can help to avoid frustrations in players. ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:5:0","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"‚ÑπÔ∏è Additional Information About the article Based on the dataset provided, we will not proceed with a retention analysis as mentioned above. Because the data is from a random episode, if this were episode one, this type of analysis could be useful as it can explain the pool of players who log in, created an account, or install the game but never start playing, causing traction problems. Therefore, this will be left as a limitation to the scope of the analysis. With acknowledgment to Rasmus Baraath for guiding this project. Which was developed for sharing knowledge while using cited sources of the material used. Thanks to you for reading as well. Related Content ‚Äî Rasmus Baath personal¬†blog ‚Äî Anders Drachen personal¬†website Datasets This project was developed with a dataset provided by Rasmus Baraath, which also can be downloaded at my Github repository. ","date":"2022-05-19","objectID":"/posts/candy_crush_difficulty/:6:0","tags":["Bernoulli","Python"],"title":"Level Balancing: Game Difficulty data-driven adjustments in Candy Crush","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Application of Tactical Analytics to improve user retention in a connect-three style puzzle game ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Abstract: This study explores the influence of adjusting the positioning of a ‚Äútime gate‚Äù within the mobile game Cookie Cats, transitioning it from level 30 to level 40, on player retention rates. Predicated on the hypothesis that repositioning the gate would significantly impact user retention, the study gathered and analyzed data encompassing user IDs, game version details, rounds played, and retention metrics on the first and seventh days post-engagement. Employing exploratory data analysis alongside robust statistical methodologies, such as bootstrapping, to evaluate the collected data, the findings reveal a disparity in the retention rates on the first day, with a slight preference towards the control group, wherein the gate remained at level 30. This study contributes to the understanding of game design‚Äôs impact on player engagement and retention. Looking for an interactive experience?\r\rüöÄ Download the Jupyter Notebook, available here\r\r ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:0:0","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"‚ö†Ô∏è Introduction to problem ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:1:0","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Hypothesis According to Rasmus Baath, Data Science Lead at¬†castle.io, Tactile Entertainment is planning to move Cookie Cats' time gates from level 30 to 40, but they don‚Äôt know by how much the user retention can be impacted by this decision. This sort of ‚Äútime gate‚Äù is usually seen in¬†free-to-play¬†models, and normally contains ads that can be skipped using credits. In this case the player requires to submit a specific number of ‚ÄòKeys‚Äô, which also can be skipped in exchange of¬†in-game purchases. So seeing this viewpoint, a decision like this can impact not only user retention, the expected revenue as well that‚Äôs why we are going to set the initial hypothesis as: $H_0:$ Moving the Time Gate from Level 30 to Level 40 will decrease our user retention. $H_1:$ Moving the Time Gate from Level 30 to Level 40 will increase our user retention. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:1:1","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Potential Stakeholders Mobile Designer \u0026 User Retention Expert: They must be aligned with the final statement of the analyst, and make a final judgment to improve user retention. Level Designer: As the scene of levels is under study, the level designers need to take action on time to guarantee the storyline of the levels has a correct sequence, and to add it in a new patch. System Designer \u0026 System Engineer: If we extend the time gate, the credits should or should not remain at the same quantity required, which also needs to be implemented in the tracking system of the user progress. Executive Game Producer: As we mentioned before, a potential change requires a redesign of the earnings strategy and an alignment in the business expectation like funding, agreements, marketing, and patching deadlines. Players community: This stakeholder can be affected by the theory of hedonic adaptation, which is according to Rasmus Baath is ‚Äúthe tendency for people to get less and less enjoyment out of a fun activity over time if that activity is undertaken continuously‚Äù, meaning that if we prolong the time gate, this can affect the engagement in an unfavorable way, which in this case require an evaluation. Note: To facilitate the understanding of the roles of the development team, I invite you to take a look at this diagram that I designed. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:1:2","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üì• About the data ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:2:0","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Collection process and structure Most of the time game developers work aside of¬†telemetry systems, which according to Anders Drachen et al. (one of the¬†pioneers¬†in the Game Analytics field), from an interview made with Georg Zoeller of Ubisoft Singapore, the Game Industry manages two kinds of telemetry systems: Developer-facing:¬†‚ÄúThe main goal of the system is to facilitate and improve the production process, gathering and presenting information about how developers and testers interact with the unfinished game‚Äù. Like the one mentioned in Chapter 7 of the¬†‚ÄúGame Analytics Maximizing the Value of Player Data‚Äù¬†book, like the one implemented in Bioware‚Äôs production process of¬†Dragon Age: Origins¬†in 2009. User-facing:¬†This one is¬†‚Äúcollected after a game is launched and mainly aimed at tracking, analyzing, and visualizing player behavior‚Äù¬†mentioned in Chapters 4, 5, and 6 of the same book. With the help of this kind of data-fetching system, we can create a responsive gate between the Data Analysts and the Designers. In most cases, these systems collect the data in form of logs (.txt) or dictionaries (.json), but fortunately in this case we will count with a structured CSV file. # Importing pandas import pandas as pd # Reading in the data df = pd.read_csv('datasets/cookie_cats.csv') # Showing the first few rows df.head() This dataset contains around¬†90,189¬†records of players that started the game while the telemetry system was running, according to Rasmus Baath. Among the variables collected are the next: userid¬†: unique identification of the user. version¬†: the group in which the players were measured, for a time gate at level 30 it contains a string called¬†gate_30, or for a time gate at level 40 it contains a string called¬†gate_40. sum_gamerounds¬†: number of game rounds played within the first 14 days since the first session played. retention_1¬†: Boolean that defines if the player came back 1 day after the installation. retention_7¬†: Boolean that defines if the player came back 7 days after the installation. Note:¬†An important fact to keep in mind is that in the game industry one crucial metric is¬†retention_1, since it defines if the game generate a first engagement with the first log-in of the player. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:2:1","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üîß Data Preprocessing Before starting the analysis we need to do some validations on the dataset. # Count and display the number of unique players print(\"Number of players: \\n\", df.userid.nunique(), '\\n', \"Number of records: \\n\", len(df.userid),'\\n') Number of players: 90188 Number of records: 90188 It‚Äôs not common to find this kind of data, cause as we saw the data is almost ideally sampled, where we count just with distinct records. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:3:0","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Data Cleaning The data doesn‚Äôt require any kind of transformation and the data types are aligned with their purpose. df.dtypes userid int64 version object sum_gamerounds int64 retention_1 bool retention_7 bool dtype: object ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:3:1","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Data Consistency The usability of the data it‚Äôs rather good, since we don‚Äôt count with ‚ÄúNAN‚Äù (Not A Number), ‚ÄúNA‚Äù (Not Available), or ‚ÄúNULL‚Äù (an empty set) values. # Function the plot the percentage of missing values def na_counter(df): print(\"NaN Values per column:\") print(\"\") for i in df.columns: percentage = 100 - ((len(df[i]) - df[i].isna().sum())/len(df[i]))*100 # Only return columns with more than 5% of NA values if percentage \u003e 5: print(i+\" has \"+ str(round(percentage)) +\"% of Null Values\") else: continue # Execute function na_counter(df) NaN Values per column: None By this way, we can conclude that there were not errors in our telemetry logs during the data collection. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:3:2","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Normalization Noticing the distribution of the quartiles and comprehending the purpose of our analysis, where we only require sum_gamerounds as numeric, we can validate that the data is comparable and doesn‚Äôt need transformations. df.describe() ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:3:3","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üîç Exploratory Analysis \u0026 In-game interpretations ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:4:0","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Summary statistics We got the next conclusions about their distribution and measurement: userid Interpretation: Player identifier with distinct records in the whole dataset which can be transformed as a factor Data type: Nominal Measurement type: Discrete/String version Interpretation: Just two possible values to evaluate, time gate at level 30 or level 40 Data type: Ordinal Measurement type: Discrete/String sum_gamerounds Interpretation: Number of game rounds played by the user, where 50% of the users played between 5 and 51 sessions Data type: Numerical Measurement type: Integer retention_1 Interpretation: Boolean measure to verify that the player retention was effective for 1 day at least Data type: Nominal Measurement type: Discrete/String retention_7 Interpretation: Boolean measure to verify that the player retention was effective for 7 days at least Data type: Nominal Measurement type: Discrete/String ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:4:1","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Strategy of Analysis The most accurate way to test changes is to perform¬†A/B testing¬†by targeting a specific variable, in the case¬†retention¬†(for 1 and 7 days after installation). As we mentioned before, we have two groups in the¬†version¬†variable: Control group:¬†The time gate is located at level 30. We are going to consider this one as a no-treatment group. Treatment group:¬†The company plans to move the time gate to level 40. We are going to use this as a subject of study, due to the change involved. In an advanced stage, we are going to perform a¬†bootstrapping¬†technique, to be confident about the result comparison for the retention probabilities between groups. # Counting the number of players in each AB group. players_g30 = df[df['version'] == 'gate_30'] players_g40 = df[df['version'] == 'gate_40'] print('Number of players tested at Gate 30:', str(players_g30.shape[0]), '\\n', 'Number of players tested at Gate 40:', str(players_g40.shape[0])) Number of players tested at Gate 30: 44700 Number of players tested at Gate 40: 45489 ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:4:2","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Game rounds distribution As we see the proportion of players sampled for each group is balanced, so for now, only exploring the Game Rounds data is in the queue. Let‚Äôs see the distribution of Game Rounds (The plotly¬†layout¬†created is available in vizformatter library). import matplotlib.pyplot as plt %matplotlib inline import plotly.express as px # Own layout design library from vizformatter.standards import layout_plotly # Load layout base objects sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) # Distribution Boxplot with outliers box1 = px.box(df, x=\"sum_gamerounds\", title = \"Game Rounds Overall Distribution by player\", labels = {\"sum_gamerounds\":\"Game Rounds registered\"}) box1.update_layout(layout) box1.add_annotation(sign) box1.show() For now, we see that exist clear outliers in the dataset since one user has recorded¬†49,854¬†Game rounds played in less than 14 days, meanwhile, the max recorded, excluding the outlier, is around¬†2,900. The only response to this case situation is a ‚Äúbot‚Äù, a ‚Äúbug‚Äù or a ‚Äúglitch‚Äù. Nevertheless, it‚Äôs preferable to clean it, since only affected one record. Let‚Äôs prune it. df = df[df['sum_gamerounds'] != 49854] We can make an¬†Empirical Cumulative Distribution Function, to see the real distribution of our data. Note:¬†In this case, we won‚Äôt use histograms to avoid a binning bias. import plotly.graph_objects as go # Import numpy library import numpy as np # ECDF Generator function def ecdf(data): # Generate ECDF (Empirical Cumulative Distribution Function) # for on dimension arrays n = len(data) # X axis data x = np.sort(data) # Y axis data y = np.arange(1, n+1) / n return x, y # Generate ECDF data x_rounds, y_rounds = ecdf(df['sum_gamerounds']) # Generate percentile makers percentiles = np.array([5,25,50,75,95]) ptiles = np.percentile(df['sum_gamerounds'], percentiles) # ECDF plot ecdf = go.Figure() # Add traces ecdf.add_trace(go.Scatter(x=x_rounds, y=y_rounds, mode='markers', name='Game Rounds')) ecdf.add_trace(go.Scatter(x=ptiles, y=percentiles/100, mode='markers+text', name='Percentiles', marker_line_width=2, marker_size=10, text=percentiles, textposition=\"bottom right\")) ecdf.update_layout(layout) ecdf.update_layout(title='Game Rounds Cumulative Distribution Plot', yaxis_title=\"Cumulative Probability\") ecdf.add_annotation(sign) ecdf.show() As we see 95% of our data is below 500 Game Rounds. print(\"The 95 percentile of the data is at: \", ptiles[4], \"Game Rounds\",\"\\n\", \"This means \", df[df[\"sum_gamerounds\"] \u003c= ptiles[4]].shape[0], \" players\") The 95 percentile of the data is at: 221.0 Game Rounds This means 85706 players For us, this can be considered a valuable sample. In the plot above, we saw some players that installed the game but, then never return (0 game rounds). print(\"Players inactive since installation: \", df[df[\"sum_gamerounds\"] == 0].shape[0]) Players inactive since installation: 3994 And in most cases, players just play a couple of game rounds in their first two weeks. But, we are looking for players that like the game and to get hooked, that‚Äôs one of our interests. A common metric in the video gaming industry for how fun and engaging a game is 1-day retention as we mentioned before. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:4:3","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üìä Player retention analysis Retention is the percentage of players that come back and plays the game one day after they have installed it. The higher 1-day retention is, the easier it is to retain players and build a large player base. According to Anders Drachen et al. (2013), these customer kind metrics¬†‚Äúare notably interesting to professionals working with marketing and management of games and game development‚Äù, also this metric is described simply as¬†‚Äúhow sticky the game is‚Äù, in other words, it‚Äôs essential. As a first step, let‚Äôs look at what 1-day retention is overall. # The % of users that came back the day after they installed prop = len(df[df['retention_1'] == True]) / len(df['retention_1']) * 100 print(\"The overall retention for 1 day is: \", str(round(prop,2)),\"%\") The overall retention for 1 day is: 44.52 % Less than half of the players come back one day after installing the game. Now that we have a benchmark, let‚Äôs look at how 1-day retention differs between the two AB groups. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:5:0","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üîÉ 1-day retention by A/B Group Computing the retention individually, we have the next results. # Calculating 1-day retention for each AB-group # CONTROL GROUP prop_gate30 = len(players_g30[players_g30['retention_1'] == True])/len(players_g30['retention_1']) * 100 # TREATMENT GROUP prop_gate40 = len(players_g40[players_g40['retention_1'] == True])/len(players_g40['retention_1']) * 100 print('Group 30 at 1 day retention: ',str(round(prop_gate30,2)),\"%\",\"\\n\", 'Group 40 at 1 day retention: ',str(round(prop_gate40,2)),\"%\") Group 30 at 1 day retention: 44.82 % Group 40 at 1 day retention: 44.23 % It appears that there was a slight decrease in 1-day retention when the gate was moved to level 40 (44.23%) compared to the control when it was at level 30 (44.82%). It‚Äôs a smallish change, but even small changes in retention can have a huge impact. While we are sure of the difference in the data, how confident should we be that a gate at level 40 will be more threatening in the future? For this reason, it‚Äôs important to consider bootstrapping techniques, this means¬†‚Äúa sampling with replacement from observed data to estimate the variability in a statistic of interest‚Äù. In this case, retention, and we are going to do a function for that. # Bootstrapping Function def draw_bs_reps(data,func,iterations=1): boot_Xd = [] for i in range(iterations): boot_Xd.append(func(data = np.random.choice(data, len(data)))) return boot_Xd # Retention Function def retention(data): ret = len(data[data == True])/len(data) return ret ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:5:1","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Control Group Bootstrapping # Bootstrapping for gate 30 btg30_1d = draw_bs_reps(players_g30['retention_1'], retention, iterations = 1000) ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:5:2","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Treatment Group Bootstrapping # Bootstrapping for gate 40 btg40_1d = draw_bs_reps(players_g40['retention_1'], retention, iterations = 1000) Now, let‚Äôs check the results import plotly.figure_factory as ff mean_g40 = np.mean(btg40_1d) mean_g30 = np.mean(btg30_1d) # A Kernel Density Estimate plot of the bootstrap distributions boot_1d = pd.DataFrame(data = {'gate_30':btg30_1d, 'gate_40':btg40_1d}, index = range(1000)) # Plotting histogram hist_1d = [boot_1d.gate_30, boot_1d.gate_40] dist_1d = ff.create_distplot(hist_1d, group_labels=[\"Gate 30 (Control)\", \"Gate 40 (Treatment)\"], show_rug=False, colors = ['#3498DB','#28B463']) dist_1d.add_vline(x=mean_g40, line_width=3, line_dash=\"dash\", line_color=\"#28B463\") dist_1d.add_vline(x=mean_g30, line_width=3, line_dash=\"dash\", line_color=\"#3498DB\") dist_1d.add_vrect(x0=mean_g30, x1=mean_g40, line_width=0, fillcolor=\"#F1C40F\", opacity=0.2) dist_1d.update_layout(layout) dist_1d.update_layout(xaxis_range=[0.43,0.46]) dist_1d.update_layout(title='1-Day Retention Bootstrapping by A/B Group', xaxis_title=\"Retention\") dist_1d.add_annotation(sign) dist_1d.show() The difference still looking close, for this reason, is preferable to zoom it by plotting the difference as an individual measure. # Adding a column with the % difference between the two AB-groups boot_1d['diff'] = ( ((boot_1d['gate_30'] - boot_1d['gate_40']) / boot_1d['gate_40']) * 100 ) # Ploting the bootstrap % difference hist_1d_diff = [boot_1d['diff']] dist_1d_diff = ff.create_distplot(hist_1d_diff, show_rug=False, colors = ['#F1C40F'], group_labels = [\"Gate 30 - Gate 40\"], show_hist=False) dist_1d_diff.add_vline(x= np.mean(boot_1d['diff']), line_width=3, line_dash=\"dash\", line_color=\"black\") dist_1d_diff.update_layout(layout) dist_1d_diff.update_layout(xaxis_range=[-3,6]) dist_1d_diff.update_layout(title='Percentage of \"1 day retention\" difference between A/B Groups', xaxis_title=\"% Difference\") dist_1d_diff.add_annotation(sign) dist_1d_diff.show() From this chart, we can see that the percentual difference is around¬†1% - 2%, and that most of the distribution is above¬†0%, in favor of a gate at level 30. But, what is the probability that the difference is above¬†0%? Let‚Äôs calculate that as well. # Calculating the probability that 1-day retention is greater when the gate is at level 30 prob = (boot_1d['diff'] \u003e 0.0).sum() / len(boot_1d['diff']) # Pretty printing the probability print('The probabilty of Group 30 (Control) having a higher \\nretention than Group 40 (Treatment) is: ', prob*100, '%') The probabilty of Group 30 (Control) having a higher retention than Group 40 (Treatment) is: 96.39999999999999 % ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:5:3","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üîÉ 7-day retention by A/B Group The bootstrap analysis tells us that there is a high probability that 1-day retention is better when the time gate is at level 30. However, since players have only been playing the game for one day, likely, most players haven‚Äôt reached level 30 yet. That is, many players won‚Äôt have been affected by the gate, even if it‚Äôs as early as level 30. But after having played for a week, more players should have reached level 40, and therefore it makes sense to also look at 7-day retention. That is:¬†What percentage of the people that installed the game also showed up a week later to play the game again? Let‚Äôs start by calculating 7-day retention for the two AB groups. # Calculating 7-day retention for both AB-groups ret30_7d = len(players_g30[players_g30['retention_7'] == True])/len(players_g30['retention_7']) * 100 ret40_7d = len(players_g40[players_g40['retention_7'] == True])/len(players_g40['retention_7']) * 100 print('Group 30 at 7 day retention: ',str(round(ret30_7d,2)),\"%\",\"\\n\", 'Group 40 at 7 day retention: ',str(round(ret40_7d,2)),\"%\") Group 30 at 7 day retention: 19.02 % Group 40 at 7 day retention: 18.2 % Like with 1-day retention, we see that 7-day retention is barely lower (18.20%) when the gate is at level 40 than when the time gate is at level 30 (19.02%). This difference is also larger than for 1-day retention. We also see that the overall 7-day retention is lower than the overall 1-day retention; fewer people play a game a week than a day after installing. But as before, let‚Äôs use bootstrap analysis to figure out how sure we can be of the difference between the AB-groups. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:5:4","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Control \u0026 Treatment Group Bootstrapping # Creating a list with bootstrapped means for each AB-group # Bootstrapping for CONTROL group btg30_7d = draw_bs_reps(players_g30['retention_7'], retention, iterations = 500) # Bootstrapping for TREATMENT group btg40_7d = draw_bs_reps(players_g40['retention_7'], retention, iterations = 500) boot_7d = pd.DataFrame(data = {'gate_30':btg30_7d, 'gate_40':btg40_7d}, index = range(500)) # Adding a column with the % difference between the two AB-groups boot_7d['diff'] = (boot_7d['gate_30'] - boot_7d['gate_40']) / boot_7d['gate_30'] * 100 # Ploting the bootstrap % difference hist_7d_diff = [boot_7d['diff']] dist_7d_diff = ff.create_distplot(hist_7d_diff, show_rug=False, colors = ['#FF5733'], group_labels = [\"Gate 30 - Gate 40\"], show_hist=False) dist_7d_diff.add_vline(x= np.mean(boot_7d['diff']), line_width=3, line_dash=\"dash\", line_color=\"black\") dist_7d_diff.update_layout(layout) dist_7d_diff.update_layout(xaxis_range=[-4,12]) dist_7d_diff.update_layout(title='Percentage of \"7 day retention\" difference between A/B Groups', xaxis_title=\"% Difference\") dist_7d_diff.add_annotation(sign) dist_7d_diff.show() # Calculating the probability that 7-day retention is greater when the gate is at level 30 prob = (boot_7d['diff'] \u003e 0).sum() / len(boot_7d) # Pretty printing the probability print('The probabilty of Group 30 (Control) having a higher \\nretention than Group 40 (Treatment) is: ~', prob*100, '%') The probabilty of Group 30 (Control) having a higher retention than Group 40 (Treatment) is: ~ 100.0 % ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:5:5","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üóíÔ∏è Final thoughts \u0026 takeaways What can the stakeholders understand and take in consideration? As we underlined retention is crucial, because if we don‚Äôt retain our player base, it doesn‚Äôt matter how much money they spend in-game purchases. So, why is retention higher when the gate is positioned earlier? Normally, we could expect the opposite: The later the obstacle, the longer people get engaged with the game. But this is not what the data tells us, we explained this with the theory of hedonic adaptation. What could the stakeholders do to take action? Now we have enough statistical evidence to say that 7-day retention is higher when the gate is at level 30 than when it is at level 40, the same as we concluded for 1-day retention. If we want to keep consumer retention high, we should not move the gate from level 30 to level 40, it means we keep our Control method in the current gate system. What can stakeholders keep working on? For coming strategies the Game Designers can consider that, by pushing players to take a break when they reach a gate, the fun of the game is postponed. But, when the gate is moved to level 40, they are more likely to quit the game because they simply got bored of it. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:6:0","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"‚ÑπÔ∏è Additional Information About the article With acknowledgment to Rasmus Baraath for guiding this project. Which was developed for sharing knowledge while using cited sources of the material used. Thanks to you for reading as well. Related Content For more content related to the authors mentioned, I invite you to visit the next sources: ‚Äì Anders Drachen personal¬†website. ‚Äì Rasmus Baath personal¬†blog. ‚Äì Georg Zoeller personal¬†keybase. Also in case you want to share some ideas, please visit the¬†About¬†section and contact me. Datasets This project was developed with a dataset provided by Rasmus Baath, which also can be downloaded at my¬†Github repository. ","date":"2022-03-02","objectID":"/posts/ab_testing_cookiecats/:7:0","tags":["A/B Testing","Bootstrapping","Python"],"title":"A/B Testing Study on the Impact of Time Gate Positioning in Cookie Cats","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Making sense of Video Game Industry sales data through Python libraries, like Pandas \u0026 Plotly","date":"2021-12-01","objectID":"/posts/evga/","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"The industry of video games revenues is reaching the $173.7 billion in value, with around 2.5 billion users enjoying them worldwide, with a forecasted value of $314.40 billion by 2026 according to Mordor Intelligence. Impressive facts, right? Nowadays this market is no longer considered a simple hobby for kids, it has become a constantly growing giant which attracts more and more customers as it takes advantage of the growth of streaming platforms. But this industry, as we well know, is taking more fields, born from ambitious expectations such as the Nintendo World Championships in the 90‚Äôs to what today many have adopted as a lifestyle also known as Esports. In this exploratory analysis, we will also see one of the events that marked the industry in the 80s. Here is a preview. Looking for an interactive experience?\r\rüöÄ Download the Jupyter Notebook, available here\r\r We‚Äôll take a tour through the history of videogames, starting from the late 70s and the early 80s. However, as a way of clarification, if you are a member of the culture, it‚Äôs important to mention that due to limitations of the scope of data available for analysis, Tomohiro Nishikado‚Äôs masterpiece, released as Space Invaders, will not be part of the analysis; and in case you‚Äôre not a member don‚Äôt worry this is for you as well. From an optimistic point of view, we will analyze quite important historical data, because is difficult to even think about getting the 70s data like Pong; and another advantage is that we can start our journey from the market revolution in the early 80s. Before starting our journey, like any exploratory data analysis we must import our libraries. # To manage Dataframes import pandas as pd # To manage number operators import numpy as np # To do interactive visualizations import plotly.express as px import plotly.graph_objects as go import plotly.figure_factory as ff # Format from vizformatter.standards import layout_plotly Now, let‚Äôs import our data. We must consider that we already prepared it, as shown in this articles‚Äôs footnote.1 # Data frame of videogames df = pd.read_csv(\"data/videogames.csv\", na_values=[\"N/A\",\"\", \" \", \"nan\"], index_col=0) In addition to facilitate the management of dates in the visualizations, two extra columns will be generated, one as a Timestamp and another as a String, which will be used only if required. # Transform Year column to a timestamp df[\"Year_ts\"] = pd.to_datetime(df[\"Year_of_Release\"], format='%Y') # Transform Year column to a string df[\"Year_str\"] = df[\"Year_of_Release\"].apply(str) \\ .str.slice(stop=-2) Also we can import the layout format as a variable from my repository. sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) ","date":"2021-12-01","objectID":"/posts/evga/:0:0","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Data integrity validation First, we check our current dataset using the method .info() df.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e Int64Index: 16716 entries, 0 to 16718 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 16716 non-null object 1 Year_of_Release 16447 non-null float64 2 Publisher 16662 non-null object 3 Country 9280 non-null object 4 City 9279 non-null object 5 Developer 10096 non-null object 6 Platform 16716 non-null object 7 Genre 16716 non-null object 8 NA_Sales 16716 non-null float64 9 EU_Sales 16716 non-null float64 10 JP_Sales 16716 non-null float64 11 Other_Sales 16716 non-null float64 12 Global_Sales 16716 non-null float64 13 Critic_Score 8137 non-null float64 14 Critic_Count 8137 non-null float64 15 User_Score 7590 non-null float64 16 User_Count 7590 non-null float64 17 Rating 9950 non-null object 18 Year_ts 16447 non-null datetime64[ns] 19 Year_str 16716 non-null object dtypes: datetime64[ns](1), float64(10), object(9) memory usage: 2.7+ MB To one side we find a great variety of data and attributes, to the other one we see that of the total of 16,716 records there are several attributes with a significant number of null values, which we are going to see next, in percentage terms. # Function the plot the percentage of missing values def na_counter(df): print(\"NaN Values per column:\") print(\"\") for i in df.columns: percentage = 100 - ((len(df[i]) - df[i].isna().sum())/len(df[i]))*100 # Only return columns with more than 5% of NA values if percentage \u003e 5: print(i+\" has \"+ str(round(percentage)) +\"% of Null Values\") else: continue # Execute function na_counter(df) NaN Values per column: Country has 44% of Null Values City has 44% of Null Values Developer has 40% of Null Values Critic_Score has 51% of Null Values Critic_Count has 51% of Null Values User_Score has 55% of Null Values User_Count has 55% of Null Values Rating has 40% of Null Values These correspond to the attributes that hold more than 5% of the null values considering a confidence standard, which consists of having at least 95% of the data. In a visual way, we can look at it in the following graphic. # Make a dataframe of the number of Missing Values per attribute df_na = df.isna().sum().reset_index() # Rename our dataframe columns df_na.columns = [\"Column\",\"Missing_Values\"] # Plot barchart of Missing Values barna = px.bar(df_na[df_na[\"Missing_Values\"] \u003e 0].sort_values (\"Missing_Values\", ascending = False), y=\"Missing_Values\", x=\"Column\", color=\"Missing_Values\", opacity=0.7, title = \"Total of Missing Values per attribute\", color_continuous_scale= \"teal\", labels = {\"Missing_Values\":\"Missing Values\"}) # Update layout barna.update_layout(layout) barna.update_annotations(sign) barna.show() \rWe see that there is a significant quantity of null values, predominantly in columns related to critics and their respective value (Metacritic); as well as its content Rating made by ESRB (Entertainment Software Rating Board). Still, since these are not categorical variables, they won‚Äôt have an identifier role, in which case our main interest will be ‚ÄúName‚Äù and ‚ÄúYear_of_Release‚Äù, and subsequently their elimination or omission will be evaluated if necessary. ","date":"2021-12-01","objectID":"/posts/evga/:1:0","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Exploratory Video Game Analysis (EVGA) Before starting with our expedition we should begin by understanding the behavior of the data with which our analysis will be built, for this we‚Äôll use the .describe() method. # Modify decimal number attribute pd.options.display.float_format = \"{:.2f}\".format # Print description df.describe() The numerical attributes show us that we have a total of 40 years of records (from 1980 to 2020) of sales in North America, Europe, Japan, and other parts of the world. Where the median indicates that 50% of the years recorded are less than or equal to 2007, and we did not find outliers. Also, the average sales value is higher in North America despite the fact that the average sale of the titles is around 263,000 units, but its variation is quite high, so it should be compared more exhaustively. From a historical point of view, it makes sense, that knowing that the focus of sales is North America, cause even during the 60s the head of Nintendo of America, Minoru Arakawa, decided to expand their operations in the United States starting from the world of the arcade, so we can have the hypothesis to see this as a place of opportunities for this market. ","date":"2021-12-01","objectID":"/posts/evga/:2:0","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Golden Age of Videogames 1977 ‚Äì Launch of Atari 2600\rWe will begin with the global view, I mean, the superficial perspective of the sales during this period. # Mask to subset games from 1980 to 1990 games8090 = df[\"Year_of_Release\"].isin(np.arange(1980,1991)) # Top publishers between 1980 and 1990 top_pub = df[df[\"Year_of_Release\"]\u003c=1990].groupby(\"Publisher\") \\ .sum(\"Global_Sales\") \\ .sort_values(\"Global_Sales\", ascending = False)[\"Global_Sales\"] \\ .head(10) # Dataframe for Line Plot of most frequent companies df_sales_ts = df[games8090][df[\"Publisher\"].isin(top_pub.index)] \\ .pivot_table(values = \"Global_Sales\", index = [\"Year_of_Release\", \"Year_str\", \"Year_ts\", \"Publisher\",\"Platform\"], aggfunc= np.sum) \\ .reset_index() \\ .sort_values(\"Year_of_Release\", ascending = True) \\ .groupby([\"Publisher\",\"Year_ts\"]) \\ .sum() \\ .reset_index() # Plot a lineplot gline = px.line(df_sales_ts, x=\"Year_ts\", y=\"Global_Sales\", color='Publisher', labels={\"Year_ts\": \"Years\", \"Global_Sales\": \"Millions of Units Sold\", \"total_bill\": \"Receipts\"}, title = \"Millions of units during Golden Age sold by Publisher\") # To plot markers for i in np.arange(0,10): gline.data[i].update(mode='markers+lines') # Update Layout gline.update_layout(layout) gline.update_annotations(sign) gline.show() \rAs we can see at the beginning of the decade and probably after 1977, the market was dominated by Atari Studios while Activision was its main competitor in terms of IPs, because these competitors eventually published their titles on the Atari 2600, example of this was Activision with Kaboom! or Parker Bros with Frogger. Another important fact is that in 1982 we can remember that it was one of the best times for Atari where they published titles that had a great impact such as Tod Frye‚Äôs Pac-Man. # Mask of 1982 games games82 = df[df.Year_of_Release == 1982] # Distribution column games82['Distribution'] = (games82.Global_Sales/sum(games82.Global_Sales))*100 # Extracting top titles of 1982 games82 = games82.sort_values('Distribution', ascending=False).head(10) # Fix Publisher Issue of Mario Bros., this game was originally published by # Nintendo for arcades games82.loc[games82.Name == 'Mario Bros.','Publisher'] = 'Nintendo' # Distribution bar82 = px.bar(games82, y='Distribution', text='Distribution', x='Name', color = 'Publisher', title='Distribution of total sales in 1982 by Publisher', labels={\"Distribution\":\"Market Participation distribution\", \"Name\":\"Videogame title\"}) # Adding text of percentages bar82.update_traces(texttemplate='%{text:.3s}%', textposition='outside') # Update layout bar82.update_layout(layout) bar82.update_annotations(sign) bar82.show() \rIt is evident that the adaptation of this arcade game released in 1980, had outstanding sales, once it was introduced to the world of the Atari 2600. According to the documentary ‚ÄúOunce Upon Atari‚Äù, episode 4 to be exactly, this title managed to sell more than 7 million copies, due to optimizations in the display and in the intelligence of the NPCs, compared to the original version. FYI: The version of Mario Bros in the dataset corresponds to the Atari 2600 and Arcade version are different from the success that was later introduced to the NES. 1983 ‚Äì Crisis of the Video Game industry\rUndoubtedly, the timeline above shows a clear drop in sales from 1983. And yes, I‚Äôm sure they want to know what happened here. For sure, if we had Howard Scott Warshaw talking with us, we would surely understand one of the crudest realities in this industry‚Äôs history, since he lived this in his own flesh. But in this case, I will explain. In summary, he was one of the greatest designers of that moment, who was hired to design a video game based on one of the biggest hits in cinema, E.T. the Extra-Terrestrial. At the time Steven Spielberg shares the vision of a game very similar to Pac-Man, something extremely strange, and by the way a release date is designated just a few months after this. As you may have thought, it was a complete disast","date":"2021-12-01","objectID":"/posts/evga/:2:1","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"I World Console War (WCWI) 1989 - Sega Enterprises Inc. launches worldwide Sega Megadrive Genesis\r1991 - Nintendo launches worldwide Super Nintendo Entertainment system\rAt the beginning of the 90s, after the launch of the SEGA and Nintendo consoles, the First World War of Videogames began. Mainly in two of their biggest titles, Sonic The Hedgedog and Super Mario Bros. During 1990, approximately 90% of the US market was controlled by Nintendo, until in 1992 SEGA began to push with strong marketing campaigns aimed at an older audience. One of the most remarkable fact of this period was the launch of Mortal Kombat in 1992, where Nintendo censored part of its content (blood-content) since they had an initiative to be a family friendly company, and of course this became very annoying the followers of this series. # Transform current dataframe as long format df_long = df.melt(id_vars = [\"Name\",\"Platform\",\"Year_of_Release\",\"Genre\", \"Publisher\", \"Developer\", \"Rating\", \"Year_str\", \"Year_ts\", \"Country\", \"City\",\"Critic_Score\",\"User_Score\"], value_vars = [\"NA_Sales\", \"EU_Sales\",\"JP_Sales\",\"Other_Sales\"], var_name = [\"Location\"], value_name = \"Sales\") # Giving a better format to the location's Name df_long = df_long.replace({\"Location\": {\"NA_Sales\": \"North America\", \"EU_Sales\": \"Europe\", \"JP_Sales\": \"Japan\", \"Other_Sales\": \"Rest of the World\"} }) # To delete columns without sales registry df_long = df_long[df_long[\"Sales\"] \u003e 0].dropna(subset = [\"Sales\"]) # Dataframe df_gen90 = df_long[(df_long[\"Year_of_Release\"] \u003e 1989) \u0026 (df_long[\"Year_of_Release\"] \u003c 2000)] \\ .pivot_table(values = \"Sales\", index = \"Genre\", columns = \"Location\", aggfunc = np.sum) # Image plot ima = px.imshow(df_gen90.reset_index(drop=True).T, y= [\"Europe\",\"Japan\",\"North America\",\"Rest of the Worlds\"], x= ['Action', 'Adventure', 'Fighting', 'Misc', 'Platform', 'Puzzle', 'Racing', 'Role-Playing', 'Shooter', 'Simulation', 'Sports','Strategy'], labels=dict(color=\"Total Sales in Millions\"), color_continuous_scale='RdBu_r', title = \"Heatmap of Location vs Genre during WCWI\") # Update layout ima.update_layout(layout) ima.update_annotations(sign) ima.show() \rFollowing the Mortal Kombat censorship, Nintendo was hit, noticing that fighting genres were among the most purchased during the 90s. However, the success of Nintendo IPs such as The Legend of Zelda and Super Mario, ended up destroying the SEGA console in 1998, in addition because of Nintendo grew stronger thanks to role-playing games during these years. # Dataframe with just SNES and GEN, Super Mario was removed to avoid outliers df_sn = df_long[(df_long[\"Year_of_Release\"] \u003e 1989) \u0026 (df_long[\"Year_of_Release\"] \u003c 2000) \u0026 ((df_long[\"Platform\"].isin([\"GB\",\"NES\",\"SNES\",\"GEN\",\"PC\",\"PS\",\"N64\",\"DC\"])) )].sort_values(\"Year_of_Release\", ascending=True).drop(18) # Plot of sales during 90s strip90 = px.strip(df_sn, x = \"Year_of_Release\", y = \"Sales\", color = \"Platform\", hover_name=\"Name\", labels={\"Name\":\"Title\", \"Year_of_Release\":\"Year\"}, hover_data=[\"Country\"]) # Update layout strip90.update_layout(layout) strip90.update_traces(jitter = 1) strip90.update_annotations(sign) strip90.show() \rThe first impression, when looking at this graph is that we notice the great dominance of Nintendo since the sales of the Sega Genesis collapsed in 1995, until during the Sega Dreamcast campaign, where the leadership was taken by the GameBoy and the Nintendo 64, followed by the new competitor Play Station, a topic that we will cover later. ","date":"2021-12-01","objectID":"/posts/evga/:2:2","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Role-playing game revolution One of the most characteristic events of this time was the implementation of 16-bit graphic technologies, which at the time was double what was available. Along with this, the Japanese once again made another master move, which gave a decisive turn to a genre, after the expected fall of RPGs on the PC platform. Before highlighting the Japanese originality, it is necessary to know after successes of role-playing games such as Ultima VIII: Pagan (PC), this genre had a slow development, since the CD-ROMs generated great graphic expectations for the developers, by the way prolonging the releases, and for sure this caused a lack of interest from the community, and began to move towards action games or first person shooter such as Golden Eye (1997). However, success stories continued to appear in this genre such as Diablo (1996), developed by Blizzard Entertainment. # Dataframe for Genre lineplot df90G = df_long[(df_long[\"Year_of_Release\"] \u003e 1989) \u0026 (df_long[\"Year_of_Release\"] \u003c 2000) \u0026 ((df_long[\"Genre\"] == \"Role-Playing\") | (df_long[\"Genre\"] == \"Action\") | (df_long[\"Genre\"] == \"Platform\") | (df_long[\"Genre\"] == \"Fighting\") )] \\ .groupby([\"Genre\", \"Year_ts\"]).sum(\"Sales\").reset_index() # Plot an animated lineplot linegen = px.line(df90G, x=\"Year_ts\", y=\"Sales\", color=\"Genre\", title = \"Millions of units sold during 90s by Genre \", labels={\"Sales\": \"Millions of Units Sold\", \"Year_ts\":\"Years\"}) # To plot markers for i in np.arange(0,4): linegen.data[i].update(mode='markers+lines') # Update layout linegen.update_layout(layout) linegen.update_annotations(sign) linegen.show() \rAmong all genres, the growth of RPGs over time must be underlined. The release of Pok√©mon in 1996 for GameBoy by the developer Game Freak was a success for Nintendo, which swept everything in its path, with its first generation of Pokemon Blue, Red and Yellow that was released in 1999, the latter is the fourth Japanese version. ","date":"2021-12-01","objectID":"/posts/evga/:2:3","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"A new Japanese Ruler takes the throne 1994 - Sony Computer Entertainment's PlayStation is born\rRPGs not only boosted Nintendo, but multiplatform IPs like Final Fantasy VII gave companies such as Sony Computer Entertainment a boost during the introduction of their new 32-bit console and at the same time to publicize the JRPGs within the western market. In 1995, when Sony planned their introduction of the PlayStation to America, they chose not to focus their Video Game market on a single type of genre or audience, but instead diversified their video game portfolio and memorable titles such as Crash Bandicoot, Metal Gear Solid and Tekken emerged. # Subset only PS games df_sony = df[(df[\"Year_of_Release\"].isin([1995,1996])) \u0026 (df[\"Platform\"] == \"PS\")] # Subset the columns needed df_sony = df_sony[[\"Name\",\"Year_of_Release\",\"Publisher\",\"Platform\",\"Genre\", \"Global_Sales\"]] # Pie plot piesony = px.pie(df_sony, values= \"Global_Sales\", names='Genre', color_discrete_sequence = px.colors.sequential.Blues_r, labels={\"Global_Sales\":\"Sales\"}) # Update layout piesony.update_layout(layout) piesony.update_traces({\"textinfo\":\"label+text+percent\", \"hole\":0.15}) piesony.update_annotations(sign) piesony.show() \rAs we can see in the graph, Sony‚Äôs video game distribution during its first two years in the North American market. Even if we pay attention, titles like Tekken and Mortal Kombat had a significant presence by showing the highest levels of sales by genre (referring to ‚ÄúFighting‚Äù genre). ","date":"2021-12-01","objectID":"/posts/evga/:2:4","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Content control warnings 1994 - Foundation of Entertainment Software Rating Board\rAfter titles like Doom, Wolfenstein and Mortal Kombat, an American system arises to classify the content of video games, and assign it a category depending on its potential audience maturity. It was established in 1994 by the Entertainment Software Association, the formerly called the Interactive Digital Software Association. # ESRB Rating Dataframe df_r = df[df[\"Rating\"].isna() == False] df_r = df_r.groupby([\"Rating\",\"Platform\"]).count()[\"Name\"] \\ .reset_index() \\ .pivot_table(values = \"Name\", index = \"Rating\", columns = \"Platform\", aggfunc = [np.sum]) \\ .fillna(0) # Drop empty classifications df_r = df_r.drop([\"AO\",\"EC\",\"K-A\",\"RP\"]) # Heatmap of ESRB Rating vs Consoles gesrb = px.imshow(df_r.reset_index(drop=True), x= [\"3DS\",\"DC\",\"DS\",\"GBA\",\"GC\",\"PC\",\"PS\",\"PS2\",\"PS3\",\"PS4\",\"PSP\",\"PSV\", \"Wii\",\"WiiU\",\"Xbox 360\",\"Xbox\",\"Xbox One\"], y= ['E', 'E10+', 'M', 'T'], labels=dict(x=\"Console\", y=\"ESRB Rating\", color=\"Number of Titles\"), color_continuous_scale='BuGn', title = \"Heatmap of ESRB Rating vs Consoles updated to 2016\") # Update layout gesrb.update_layout(layout) gesrb.update_annotations(sign) gesrb.show() \rBased on the classification established by the ESRB, from the data available it can be concluded that the video game console with more titles for universal use is the Nintendo DS, followed by the PS2 and then is the Wii, thus highlighting the impact they had on sales, which it will be shown later. Meanwhile, the Xbox 360 and PS3 were geared towards a more mature audience with a significant presence of M-rated titles. ","date":"2021-12-01","objectID":"/posts/evga/:2:5","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Last years of 32 bit era In the early 2000s, after the launch of the PS1, Sony continued leading the console market and diversifying its portfolio of games. On the other side of the coin SEGA, despite having launched the first console with an online system, in 2002 they retired their console from the market and dedicated itself exclusively to third-party development and Arcade, a situation that is outlined in the following graph. # Lineplot sales by platform before 2005 # Extract columns games20 = df[[\"Year_of_Release\",\"Platform\",\"Global_Sales\"]] # Subset dates games20 = games20[(games20.Year_of_Release \u003e 1998) \u0026 (games20.Year_of_Release \u003c 2005)] # Omit WonderSwan by Bandai, SEGA Saturn due low sales profiles and NDS that is not # relevant yet games20 = games20[~games20.Platform.isin([\"WS\",\"DS\",\"SAT\",\"SNES\",\"PSP\"])] # Group and summarize games20 = games20.groupby([\"Year_of_Release\",\"Platform\"]).agg(sum).reset_index()\\ .sort_values([\"Year_of_Release\",\"Platform\"], ascending=True) # Save an Array of Platforms Platforms = games20.Platform.unique() # Pivot to put in long format games20 = games20.pivot_table(values=\"Global_Sales\", index=\"Year_of_Release\", columns=\"Platform\").reset_index() # Assemble lineplot line20 = go.Figure() for platform in Platforms: line20.add_trace(go.Scatter(x = games20[\"Year_of_Release\"], y = games20[platform], name=platform, line_shape='linear')) # Update layout line20.update_layout(layout) line20.update_annotations(sign) line20.show() \rThe Japanese domain was becoming more and more determined, a situation that the software technology giant, Microsoft, takes as a challenge to enter a new market and start a contest with the PS2. 2000 - The beginning of the longest rivalry in the console market\rThis famous image of Bill Gates with Dwayne Johnson was part of a great marketing strategy carried out by Microsoft, they were willing to do whatever it took to strengthen the presence of Xbox in the market. Microsoft‚Äôs vision was to standardize the game hardware so that it was as similar as possible to a PC, so they implemented Direct X, an Intel Pentium III, a 7.3GFLOPS Nvidia GPU and an 8GB hard drive, trying to secure a significant advantage over the competitors. At this time, Nintendo announced the GameCube as a console contender, but it was not very successful, a situation that was neutralized with the sales of the Game Boy Advance within the portable market. Nevertheless, the PS2 led the first part of the decade in terms of sales, while Xbox got the second place as we saw in the last graph. And of course, that was a very expensive silver medal, according to Vladimir Cole from Joystiq, Forbes estimated around $4 billion in total lost after that trip, but at the same time they proved that they could compete with the Japanese Ruler of that time. # Mask of 2000-2004 games games2004 = (df.Year_of_Release \u003e 1999) \u0026 (df.Year_of_Release \u003c 2005) # Array to Subset publishers with hightest sales from 2000 to 2004 toppub2004 = df[games2004].groupby\\ ([\"Publisher\"])[\"Global_Sales\"].agg(sum).reset_index()\\ .sort_values(\"Global_Sales\",ascending=False).head(15) # New DF with top Titles per Publisher toppub = df[games2004 \u0026 df.Publisher.isin(toppub2004.Publisher)]\\ .sort_values([\"Publisher\",\"Name\"]) # Substitute empty scores with the mean toppub.Critic_Score = toppub.Critic_Score.fillna(toppub.Critic_Score.mean()) # Top 3 toppub3 = toppub.sort_values([\"Publisher\",\"Global_Sales\"], ascending = False)\\ .groupby(\"Publisher\")[\"Name\",\"Year_of_Release\",\"Publisher\",\"Global_Sales\", \"Critic_Score\", \"Country\",\"City\"].head(3)\\ .sort_values(\"Global_Sales\", ascending=True) # Bubble plot bubpub3 = px.scatter(toppub3, y=\"Publisher\", x=\"Global_Sales\", size=\"Critic_Score\", color=\"Critic_Score\", hover_name=\"Name\", color_continuous_scale=px.colors.sequential.Greens, labels={\"Global_Sales\":\"Millions of units sold\", \"Critic_Score\":\"Metacritic value\"}) # Add reference line bubpub3.add_vrect(x0 = 8.0, x1 = 8.98, y0= 0.32, y1=0.44, line_width=0, fillco","date":"2021-12-01","objectID":"/posts/evga/:2:6","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"The ‚ÄúNon-competitor‚Äù takes the lead 2005 - Microsoft launch the Xbox 360\r2006 - PS3 and Nintendo Wii are released\rAs Microsoft and Sony continued competing for a market with high-definition titles, online connection services like Xbox Live and PSN, and high-capacity hard drives, Nintendo chose to follow a Blue Ocean Strategy after the failure of the GameCube, who tried to compete in the big leagues. Their strategy consisted of offering something new and innovative, instead of competing to be better in the characteristics offered by the competition, becoming the fastest selling console, reaching to sell 50 million units around the world according to D. Melanson from Verizon Media, so this is the best way to describe the Wii console. # Mask of 2005-2010 games games2010 = (df.Year_of_Release \u003e 2004) \u0026 (df.Year_of_Release \u003c 2011) # Dataframe of games df2010 = df[games2010] # Excluding data to focus on new consoles df2010 = df2010[df2010.Platform.isin(['Wii','DS','X360','PS3'])]\\ .groupby([\"Platform\",\"Year_str\"])[\"Global_Sales\"].agg(sum).reset_index()\\ .sort_values([\"Year_str\",\"Global_Sales\"]) # Plot of Sales by Platform bar2010 = px.bar(df2010, color=\"Platform\", y=\"Global_Sales\", x=\"Year_str\", barmode=\"group\", labels={\"Year_str\":\"Year\", \"Global_Sales\":\"Millions of Units\"}, pattern_shape=\"Platform\", pattern_shape_sequence=[\"\", \"\", \"\", \"\", \".\"], color_discrete_sequence=[\"#00DEB7\",\"#0082C2\",\"#1333A7\",\"#5849B6\"]) # Update layout bar2010.update_layout(layout) bar2010.update_annotations(sign) bar2010.show() \rAs you can see, from the start of the Wii sales, the strategy of Nintendo began to flourish, surpassing the sales of its rivals by 4 years in a row. An interesting aspect of Nintendo among the others, is that the success of their sales was due to exclusive titles involving their unique accessories with motion sensors. Referring to sales, among the most successful titles are the following. # Dataframe for table with best-selling games table_data = df[games2010] table_data = table_data[table_data.Platform.isin(['Wii','DS','X360','PS3'])]\\ .sort_values([\"Year_str\",\"Platform\",\"Global_Sales\"], ascending=False)\\ .reset_index()\\ .groupby([\"Platform\",\"Year_str\"]).head(1)\\ .sort_values(\"Platform\", ascending = False) table_data = table_data[[\"Year_str\",\"Name\",\"Publisher\",\"Platform\",\"Global_Sales\"]] # Plot of Table table10 = go.Figure(data=[go.Table( header=dict(values=list([\"Year\",\"Game title\", \"Platform\", \"Publisher\",\"Units Sold\"]), fill_color='#5849B6', align=\"center\"), cells=dict(values=[table_data.Year_str, table_data.Name, table_data.Platform, table_data.Publisher, np.round(table_data.Global_Sales * 1000000,0)], fill_color='lavender', align=list(['center', 'left', 'center', 'left', 'right'])))]) # Update layout table10.update_layout(layout) table10.update_traces({\"header\":{\"font.color\":\"#fcfcfc\", \"font.size\":fontimg+3}}) table10.update_annotations(sign) table10.show() \rFour of Wii‚Äôs five most successful titles involve Nintendo Publishers, among its most famous IPs were Mario Kart and Wii Sports. This innovation marked an era of hardware extensions and motion sensors, a situation that Activision was able to take advantage of, when acquiring Red Octane, reaching around 13 titles of the IP known as Guitar Hero until 2009, being sold with its flagship item that imitated a Gibson SG. ","date":"2021-12-01","objectID":"/posts/evga/:2:7","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Prevalence of Social Gaming After the success of some local-gaming titles, the decade from 2010 to 2020 took a more competitive or cooperative way in certain cases, guided by a new era of interconnectivity and mobility. This reason motivated developers with extraordinary visions to create not only multiplayer, but also online content that maintains high audience rates. # Subset games from 2010 to 2015 games2010 = df[(df.Year_of_Release \u003e 2009) \u0026 (df.Year_of_Release \u003c 2016)]\\ .sort_values([\"Year_str\",\"Platform\",\"Global_Sales\"]) # Subset games with more sales from 2010 to 2015 topgames2010 = games2010.sort_values([\"Genre\",\"Global_Sales\"], ascending = False)\\ .groupby(\"Genre\").head(1).sort_values(\"Year_of_Release\", ascending = True)\\ .sort_values(\"Global_Sales\", ascending=False) topgames2010 = topgames2010[[\"Year_of_Release\",\"Name\",\"Platform\",\"Publisher\",\"Genre\", \"Global_Sales\"]] # Barplot Base bargen10 = px.bar(topgames2010, y=\"Genre\", x=\"Global_Sales\", orientation=\"h\", text=\"Name\", labels={\"Name\":\"Title\", \"Global_Sales\":\"Millions of units sold\"}, color=\"Genre\", color_discrete_sequence=[\"#8B58B0\",\"#58B081\",\"#B0B058\",\"#535353\", \"#B05858\",\"#58B09E\",\"#B05890\",\"#587FB0\", \"#B05858\",\"#58B0AA\",\"#686868\",\"#C3A149\"]) bargen10.update_traces(textposition='inside', marker_line_color='#404040', textfont = {\"color\":\"#FFFFFF\",\"family\": \"segoe ui\"}, marker_line_width=1, opacity=0.7) # Update layout bargen10.update_layout(layout) bargen10.update_annotations(sign) bargen10.show() \rBetween 2010 and 2015, the best-selling title was Kinect Adventures for Xbox 360, which had a focus on enhancing multiplayer gameplay and taking advantage of the latest technological innovation of the moment, the Microsoft‚Äôs Kinect. The second best-selling title at that time was Grand Theft Auto V for PS3, which to this day continues to prevail as one of the online systems with the largest number of users in the industry. Their vision went beyond creating an Open-World game, they had the intention of creating a dynamic online content structure, which provides seasonal content. This type of model also motivated Publishers such as Epic Games and Activision, to innovate but in this case not selling games but focusing on aesthetics, where game content is offered as an extra to the online service without having to be paid as a DLC. #pubgen # Publishers with more sales in history toppubarray = df.groupby(\"Publisher\")[\"Global_Sales\"].agg(sum).reset_index()\\ .sort_values(\"Global_Sales\", ascending= False)\\ .head(len(df.Genre.unique()))[\"Publisher\"] # Extract publisher from raw df puball = df[df.Publisher.isin(toppubarray)].groupby([\"Publisher\",\"Name\",\"Genre\"]).agg(sum) # Add a column of 1s puball[\"counter\"] = np.ones(puball.shape[0]) # Create the pivot table puball = puball.pivot_table(\"counter\", index = \"Publisher\", columns=\"Genre\", aggfunc=\"sum\") # Display rounded values pd.options.display.float_format = '{:,.0f}'.format pubmatrix = ff.create_annotated_heatmap(puball.values, x=puball.columns.tolist(), y=puball.index.tolist(), annotation_text= np.around(puball.values, decimals=0), colorscale='sunset') # Update layout pubmatrix.update_layout(layout) # Extra annotation to avoid overlapping of layers pubmatrix.add_annotation(text=author, align= \"right\", visible = True, xref=\"paper\", yref=\"paper\", x= 1, y= -0.11, showarrow=False, font={\"size\":fontimg-1}) pubmatrix.update_annotations(sign) pubmatrix.show() \rThe fact that every day more Free to Play games are announced, does not indicate that this is the exclusive focus companies will have on the industry now on. Beyond this, as we see in the previous graph, each of the most recognized Publishers in history has its own style in exclusive series, despite having titles in many genres. Even today, large companies like Microsoft offer services such as Xbox GamePass, with subscriptions that offer big catalogs of games, which even include titles from Independent Developers, supporting their growth through advertising systems, helping to in","date":"2021-12-01","objectID":"/posts/evga/:2:8","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Summary The video game industry, beyond having started as a simple experiment at MIT, is a lifestyle for many. Like any market, it has had its moments of glory and its difficulties, but if we can rescue something, it is that the secret of its success lies in the emotional bond it generates with its customers. Through this article, my goal is to use data tools to inform the reader about curious events, which perhaps they did not know. I want to thank you for taking the time to read carefully. As a curious detail, there are no easter eggs üòâ Good luck! Additional Information About the article This infographic article was adapted to a general public, I hope you enjoyed it. In case you are interested in learning more about the development of the script, I invite you to the contact section in my About Page, and with pleasure we can connect. Related Content As a recommendation I suggest a look at this great article, published by Omri Wallach on the Visual Capitalist infographic page, where many interesting details about the industry‚Äôs history are covered. Datasets Videogames Dataset Videogames Developers Regions Indie Developers Regions Footnote: Specific datasets contain information from Publishers, which they were named in the source attribute as Developers, but not in all cases. For more details on the data transformation, please visit my Github repository.¬†‚Ü©Ô∏é ","date":"2021-12-01","objectID":"/posts/evga/:3:0","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Others"],"content":"Guide diagram for Game Development roles and more sources of information","date":"2021-11-29","objectID":"/posts/gamedev_structure/","tags":["Game Development","Figma"],"title":"Game Development Team Structure","uri":"/posts/gamedev_structure/"},{"categories":["Others"],"content":"In the next portal, you will find a frame with a diagram. This one displays the distinct roles and sub-roles in a video game development team. This section intends that as you read the Stakeholders section in each of my projects, you can learn about the operations, roles, and skills that each team member must possess. For this, you can read the details at the bottom of the card made for each. Important: not all of them are a necessity for development, depending on the scale of the project some are required and others are not. Also, within different branches, you will find positions that may only be necessary for mobile game development, in addition to PR and marketing roles. ","date":"2021-11-29","objectID":"/posts/gamedev_structure/:0:0","tags":["Game Development","Figma"],"title":"Game Development Team Structure","uri":"/posts/gamedev_structure/"},{"categories":["Others"],"content":"Navigation instructions ¬†Keep pressed left click to navigate inside the frame. ¬†Keep pressed Ctrl + Scroll to zoom in/out. ","date":"2021-11-29","objectID":"/posts/gamedev_structure/:0:1","tags":["Game Development","Figma"],"title":"Game Development Team Structure","uri":"/posts/gamedev_structure/"},{"categories":["Others"],"content":"More content \u0026 resources The Game Designer‚Äôs Playbook by Samantha Stahlke and Pejman Mirza-Babaei at Oxford University Press. Introductory and visual explained resources at Ask Gamedev. Note\r\rIt is recommended to use the desktop mode for a better experience, but if the viewer fails to load, access my Figma content here. MacOS users may experience WebGL issues while loading Figma preview, so ensure that Hardware Acceleration mode is enabled in your browser. \r\r ","date":"2021-11-29","objectID":"/posts/gamedev_structure/:0:2","tags":["Game Development","Figma"],"title":"Game Development Team Structure","uri":"/posts/gamedev_structure/"},{"categories":["Articles"],"content":"Cloud Service Architecture for NLP in Google Cloud","date":"2021-10-04","objectID":"/posts/gcp_academicsolution/","tags":["Google Cloud"],"title":"Cloud Service Architecture for NLP in Google Cloud","uri":"/posts/gcp_academicsolution/"},{"categories":["Articles"],"content":"Summary: This academic article was developed through my postgraduate course, DAT-03 Data Analysis in Big Data Environments, which presents a business case in which a company is interested in combining and migrating relational databases in Microsoft SQL Server and Oracle DB. Given the need to feed them with an NLP system output of social networks such as Instagram, Telegram, and Facebook Messenger. Initially, the vulnerabilities that the company has when managing its data in a Local Storage are shown, where it is compared with the advantages of implementing a Cloud Service. This service is compared with others such as Microsoft Azure and Amazon Cloud Services, showing why this solution is the one that best suits the business case. An architecture diagram was designed, covering the required APIs to the ingestion, storage, processing, and loading of the data in BI tools to ease access to the end-user. Finally, financial aspects are covered in terms of operating costs for the proposed solution. In case you want to access it, next you will find a link where it was uploaded to my LinkedIn. Click HERE to access or download. \r","date":"2021-10-04","objectID":"/posts/gcp_academicsolution/:0:1","tags":["Google Cloud"],"title":"Cloud Service Architecture for NLP in Google Cloud","uri":"/posts/gcp_academicsolution/"},{"categories":null,"content":"Contact platforms and information about the author from his background to his personal interests you can access them here","date":"2021-08-02","objectID":"/about/","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"¬†Important: Any views, material or statements expressed are mines and not those of my employer ¬†About me As a Data Scientist and Engineer, I specialize in transforming complex data into tangible business outcomes, particularly for the retail and consumer packaged goods sectors. My expertise lies in deploying advanced analytics algorithms, deep learning solutions, and machine learning approaches. ¬†Interests The passion for exploring the convergence of graphics technology platforms and analytics solutions, particularly in areas such as gaming and the metaverse, has been a driving force in my career, as in the coming years this unique intersection will offer numerous innovative solutions that can contribute effectively to a tangible impact. My journey in this field was initiated some years ago with a self-taught venture into Unity programming, which not only laid the foundation for my technical background but also sparked this deep interest. ¬†Experience McKinsey \u0026 Company\r\r Analyst, Software Delivery, Lilli GenAI, Feb 2024 - Present Proven impact as Fullstack Data Scientist: Contributed in the redesign of an advanced Named Entity Recognition System for McKinsey‚Äôs core GenAI product, significantly enhancing the precision of financial information searches using a LLM model (Internal Asset). Enhanced various semantic techniques, leading to the development of a prompt engineering strategy that significantly boosted named entity detection coverage by over 45% (Internal Asset). Data Scientist, QuantumBlack, Sep 2023 - Jan 2024 Proven impact as Data Scientist (seasonal staff): Assetized Competitor Price Index scenario generator capable of increasing 5.2% Gross Margin uplift of middle demand class products within a 6pp CPI variation, in addition to the Python-based pricing recommendation engine (Electronics Retailer). Integrated a QuantumBlack deep learning‚Äëbased demand transfer engine into Periscope‚Äôs Assortment Advisor platform by creating a highly efficient assembly module using PySpark, which resulted in a $3M signed‚Äëup proof‚Äëof‚Äëconcept (Food Retailer). Enhanced a regressor model reproducibility and performance consistency by integrating a model registry with MLFlow and Databricks. Data Analyst, Periscope, April 2023 - Aug 2023 Proven impact as Data Engineer: Contributed in the migration of a +25TB customer database by seamlessly migrating it from ExasolDB to SparkSQL transcoding, optimizing critical inbound pipelines (Department Store \u0026 Luxury Goods). Implemented a robust infrastructure within Periscope, orchestrated seamlessly with Jenkins, and enhanced data flow efficiency by integrating with secure SFTP protocols (Department Store \u0026 Luxury Goods). Junior Data Analyst, Periscope, March 2022 - Mar 2023 Proven impact as Data Scientist: Achieved a 0.5% profit uplift by implementing an AA module that optimized the pricing process by recommending package size relationships within product families of items formed by clusters based on NLP similarity scores (Food Retailer). Learn more about the responsibilities of this role at LinkedIn. \r\r Belcorp\r\r Junior Business Intelligence Analyst, Belcorp, February 2021 - Jul 2021 Proven impact as Business Analyst: Developed data analysis tools enabling LatAM teams and managers to derive actionable commercial insights, facilitating informed decision-making into a B2C functionality. Integrated BI assets into CRM tasks, enhancing intelligence for service personnel. Designed tailored data collection models for efficient regional visualization. Addressed BI inquiries from Central American personnel by creating user-friendly Tableau dashboards, presenting information clearly and comprehensibly. Learn more about the responsibilities of this role at LinkedIn. \r\r VMware\r\r Enterprise Data \u0026 Analytics Intern, VMware, June 2020 - Sep 2020 Proven impact as Data Analyst: Led comprehensive data analyses throughout the SaaS cycle, uncovering trends and patterns. Utilized data visualization techniques and charts to","date":"2021-08-02","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]