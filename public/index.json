[{"categories":["Projects"],"content":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"The plot of the sequel of Fallout 3 takes context in the year 2281, 4 years after the first escape from Vault 101, within a post-apocalyptic world after the Great War. This RPG has an open world inspired by cities such as Nevada, California, Arizona, and Utah, allowing users to have a large number of places to explore and a world with NPCs to interact with, ideal for developing behavioral and exploratory analysis. After its launch in 2010, like many titles of this IP, they have been exposed to mods developed by their community, some of them for Serious-Game purposes and others as non-Serious games. According to Anders Drachen et al. (2021, p. 5), it‚Äôs ‚Äúa term used to describe games developed for purposes other than entertainment, such as training, promoting health, citizen science, or psychological experiments‚Äù. In this case, the data was obtained from a mod developed at the PLAIT (Playable interactive technologies) at Northeastern University. The game is called VPAL: Virtual Personality Assessment Lab, and also can be accessed to the raw telemetry data in the Game Data Science book. It‚Äôs important to keep in mind that the game has been already released, so we‚Äôll require to develop a post-production analysis with a help of a mod, so the main conclusions will be focused on a fictional patch improvement made by the Programming team. Looking for an interactive experience? üöÄ Download the Jupyter Notebook, available here ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:0:0","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"‚ö†Ô∏è Introduction to problem ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:1:0","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Hypothesis The Producers are concerned for some comments about a change made in the¬†control mechanics¬†since the last patch was released. The UX research team has communicated to them that the QA testers think that some of them found the quest‚Äôs difficulty too easy and others very leveraged, especially for veteran players of the saga, reason why we can start inferring about changes made in the gameplay experience, which the major changes made were on the mechanics. The problem is that the QA team is very diversified between experienced and non-experienced testers, so it won‚Äôt be easy to make conclusions. For this reason, we need to look for statistical evidence to validate a hypothesis based on the profile of those testers. Now let‚Äôs establish the main hypothesis for the analysis: Experienced players/testers will have completed¬†more¬†quests and more kills (the usual), the mechanical changes were not significant. Inexperienced players/testers will have completed¬†more or an equal¬†quantity of quests and kills (not usual), the mechanical changes were significant. From all the KPIs that you will see next, it‚Äôs important to mention that we‚Äôll bring kills as a secondary validation measure since Fallout it‚Äôs a survival-RPG-like where you have to kill radioactive NPCs in almost every quest. We won‚Äôt count with a difficulty category for each player, so the kills attribute will be one of the few not affected by the difficulty categorization, also because it‚Äôs an end measure, not a progressive one, like shots for example. ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:1:1","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Potential Stakeholders There is an unconformity in the changes made in the control mechanics, the team requires the assistance of a Data Analyst to perform a test and validate if the QA tester‚Äôs statements make sense. On the other side of the table we have some stakeholders on the development and testing side: Gameplay Engineers: They need to know if the control mechanics are affecting the interactions of the NPCs with the progress of the players since the release of the last patch. Level Designer: As the whole RPG is designed as quests on an isolated map, they need to know if these are balanced in the sequence or chapter located, or if there is a significant change in the patch that can explain the unconformity. QA Testers: They are a key source of information while communicating the QA statement to the UX Designer. UX Designer: They are working side by side with the consumer insights team and the testers to find a clear current situation and take immediate actions if required, by giving the final report to the level designers and the programmers. Note: To facilitate the understanding of the roles of the development team, I invite you to take a look at this diagram that I designed. ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:1:2","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üì• About the data \u0026 preprocessing reference In a summary, the data is a complete chaos allocated in a repository of text files, so this will require a pipeline to be transformed, cleaned, and get ready to use. The purpose of data cleaning is to make sure the data is correct. It is rarely the case that once data is collected through the game and transferred to the server, it is automatically ready for analysis. ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:2:0","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Collection process and structure Before start let‚Äôs import the libraries we‚Äôre going to need for the preprocess. import numpy as np import pandas as pd # Visuals import plotly.express as px import plotly.figure_factory as ff # Own package from vizformatter.standards import layout_plotly # Load layout base objects sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) The instrumentation of the data and storage of all game actions within the game were done through TXT files that are stored on the client side per player, with an unstructured format. For each one, we created a file named [participantNumber].txt, which includes all session data for that participant. If the participant has more than one session, a folder is created, and multiple files are created for that participant. The transfer process can be made by using a product like Databricks, to run an entire pipeline over a PySpark Notebook. As a demonstration we made an example in a Jupyter notebook, where we got several methods to target, extract, process singles labeled activities, and merge into activity-designated CSVs, which were originally parsed from TXT files by player record. In reality the best practices show that the data can be in a JSON file, stored in a MongoDB, or an SQL database. The collection was an extensive process so you can access the simulation right¬†here¬†with a complete explanation of the bad practices made and how we tackled them. Also, it‚Äôs important to mention that the data was parsed into a single data frame with counted activities per player. After this, we applied a Likert scale to an experience metric according to Anders Drachen et al. (2021), which helped us to separate the data into Experienced and Inexperienced players. First let‚Äôs check our data. # Inexperienced group of players dataframe InExperienced = pd.read_csv(\"data/ExpNeg_GameData.csv\") # Experienced group of players dataframe Experienced = pd.read_csv(\"data/ExpPos_GameData.csv\") Experienced.head() Both data frames have 17 attributes and, the Experienced data have 28 rows, while the Inexperienced data has 42 rows. In our case the attributes in use will be ‚ÄúQuest Completed‚Äù and ‚ÄúKills‚Äù will be the variables to test. Quest Completed: Integer number counting the number of quests the tester completed during the session Kills: Number of kills registered by the player during the session ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:2:1","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üîß Data Preprocessing Before moving to our analysis it‚Äôs important to validate that our data is usable in order to make correct conclusions for the analysis. ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:3:0","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Data Cleaning \u0026 Consistency First let‚Äôs validate the data types and the ranges for both groups. InExperienced.loc[:,[\"User ID\",\"Kills\", \"Quest Completed\"]].describe() Experienced.loc[:,[\"User ID\",\"Kills\", \"Quest Completed\"]].describe() As we saw there is no problems linked to the data types, because in the data parsing noted above we took care of it. And from the ranges we infer that the maximum number of Kills registered is from an Inexperienced player with 44, but at the same time 75% of the Experienced players register a higher number in comparison to the Inexperienced ones, the same case apply to the number of Quests Completed. ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:3:1","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üîç Inferential Analysis \u0026 In-game interpretations ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:4:0","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Descriptive statistics Now, we got the next conclusions about their distribution and measurement: User ID Interpretation: Unique and counts with 70 distinct values which make sense since there is a player associated to each one of the data-row parsed Data type: Nominal transformed to numerical Measurement type: Discrete/String transformed to integer Quest Completed Interpretation: Not unique and counts the quests registries per player, and at first sight make sense that the Experienced players show higher numbers than the Inexperienced ones Data type: Numerical Measurement type: Integer Kills Interpretation: Not unique and counts the quests registries per player, and at first sight make sense that the Experienced players show higher numbers than the Inexperienced ones Data type: Numerical Measurement type: Integer For the next test plots, we will plot a simulated graph showing how fitted is the actual distribution to an ideal normal curve from the same data, that‚Äôs why we are going to create two functions, one for the new distribution calculation from the median and another to plot it over the original data. This will be our general approach because this plot will also let us see more clearly the existence of skewing elements. import math # Generate simulated-std based on the median def median_std(data): # Number of observations n = len(data) # Median of the data median = data.median() # Square deviations deviations = [(x - median) ** 2 for x in data] # Variance variance = sum(deviations) / n return math.sqrt(variance) # Function to generate a plot of a vector to compare it vs a fitted normal distribution def normalplot(data, var_name): # Generate subset of dataframe variable = data[var_name] # Max and min from variable max_var = max(variable) min_var = min(variable) # Create a random array of normally distributed number with the same parameter as the original data y_fit = np.random.normal(loc= variable.median(), scale= median_std(variable), size= len(variable)) # Max and min from simulated curve max_fit = max(y_fit) min_fit = min(y_fit) # Group data together hist_data = [variable, y_fit] group_labels = [var_name, 'Median-Simulated \u003cbr\u003e Normal Curve'] colors = [\"#007FFF\",\"#FF5A5F\"] # Create distplot with custom bin_size plot = ff.create_distplot(hist_data, group_labels, bin_size=[0.75,0.75], show_rug=False, show_hist=True, colors= colors) # Update layout plot.update_layout(layout) plot.update_layout(title = {'text':'Density Plot of '+var_name+' vs a Normal Distribution'}, xaxis = {\"title\":var_name}, yaxis = {\"title\":\"Density\"}) plot.update_xaxes(range=[min_var, max(max_var,max_fit)]) plot.add_annotation(sign) return plot ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:4:1","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"‚öîÔ∏è Two Sample T-Test for Quests Completed by Experience Group A two sample t-test is used to test whether there is a significant difference between two groups means, where the hypothesis will be: $H_0: \\mu_{ex} = \\mu_{inex}$ (population mean of ‚ÄúExperienced Players Completed Quests‚Äù is equal to ‚ÄúInexperienced Players Completed Quests‚Äù) $H_1: \\mu_{ex} \\ne \\mu_{inex}$ (population mean of ‚ÄúExperienced Players Completed Quests‚Äù is different from ‚ÄúInexperienced Players Completed Quests‚Äù) This test makes the following assumptions: The two samples data groups are independent The data elements in respective groups follow any normal distribution The given two samples have similar variances (homogeneity assumption) In this case, both groups are independent since none of them provide information about the other and vice versa. Normality Check First let‚Äôs visualize the distribution for the Experienced players. expQuestnorm = normalplot(data= Experienced, var_name= 'Quest Completed') expQuestnorm.update_layout(title = {'text':'Density Plot of Quest Completed by Experienced Players'}) expQuestnorm.show() And the distribution for Inexperienced players. inQuestnorm = normalplot(data= InExperienced, var_name= 'Quest Completed') inQuestnorm.update_layout(title = {'text':'Density Plot of Quest Completed by Inexperienced Players'}) inQuestnorm.show() Notice that the data is normal for both cases and in terms of the quantity of ‚Äúquests completed‚Äù by the experienced player‚Äôs data is slightly higher than the inexperienced ones. Variance Homogeneity First let‚Äôs see the variance for each group of testers. # Print the variance of both data groups print(\"Quest Completed variance for Experienced players: \" + str(np.var(Experienced[\"Quest Completed\"])) + \"\\n\", \"Quest Completed variance for Inexperienced players: \" + str(np.var(InExperienced[\"Quest Completed\"])) + \"\\n\") Quest Completed variance for Experienced players: 3.25 Quest Completed variance for Inexperienced players: 2.283446712018141 In the good practice the correct way to do this validation is by a Homogeneity Test, so let‚Äôs make a Barlett test since our data is normal (for non-normal is preferred Levene‚Äôs). So let‚Äôs start from the next hypothesis. $H_0: \\sigma^2_{ex} = \\sigma^2_{inex}$ (The variances are equal across in both groups) $H_1: \\sigma^2_{ex} \\ne \\sigma^2_{inex}$ (The variances are not equal across in both groups) # Import the bartlett method from scipy.stats import bartlett # Bartlett's test in Python with SciPy: bartlett(Experienced[\"Quest Completed\"], InExperienced[\"Quest Completed\"]) BartlettResult(statistic=1.0905002879637673, pvalue=0.29636038782360125) Our P-value is above 0.05, so we have enough statistical evidence to accept the hypothesis that the variances are equal between the Experienced testers and the Inexperienced players. Now that we checked that we have normal data and homogeneity in our variances, let‚Äôs continue with our two sample t-test. # Import the stats module import scipy.stats as stats # Perform the two sample t-test with equal variances stats.ttest_ind(a=Experienced[\"Quest Completed\"], b=InExperienced[\"Quest Completed\"], equal_var=True) Ttest_indResult(statistic=3.8261587063084392, pvalue=0.0002855175095244425) We have enough statistical evidence to reject the null hypothesis, the population mean of Quests Completed by ‚ÄúExperienced Players‚Äù is significantly different from the ‚ÄúInexperienced Players‚Äù. As a supposition, we can say that the difficulty selected by the player can be a factor affecting the Completion of the Quest. Considering that the difficulty levels in Fallout New Vegas can vary by two variables which are ‚ÄúDamage taken from Enemy‚Äù and ‚ÄúDamage dealt with Enemy‚Äù, without mentioning the Hardcore mode which was excluded from the samples. The difficulty is an important factor to consider since an experienced player can know what is the level of preference, while for an inexperienced player is a ‚Äútry and fail situation‚Äù, so for a future study it‚Äôs imp","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:4:2","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üíÄ Two Sample Mann-Whitney Test for Kills by Experience Group A two sample Mann-Whitney U test is used to test whether there is a significant difference between two groups distributions by comparing medians, where the hypothesis will be: $H_0: P(x_{ex} \u003e x_{inex}) = 0.5$ (population median of ‚ÄúExperienced Players Kills‚Äù is equal to ‚ÄúInexperienced Players Kills‚Äù, same distribution) $H_1: P(x_{ex} \u003e x_{inex}) \\ne 0.5$ (population median of ‚ÄúExperienced Players Kills‚Äù is different from ‚ÄúInexperienced Players Kills‚Äù, different distribution) In other words, the null hypothesis is that, ‚Äúthe probability that a randomly drawn member of the first population will exceed a member of the second population, is 50%‚Äù. This test makes the following assumptions: The two samples data groups are independent The data elements in respective groups are continuous and not-normal It‚Äôs the equivalent of a two sample t-test without the normality assumption. Also, both groups are independent since none of them provide information about the other and vice-versa. Normality Check First let‚Äôs visualize the distribution for the Experienced players. expKillsnorm = normalplot(data= Experienced, var_name= 'Kills') expKillsnorm.update_layout(title = {'text':'Density Plot of Kills by Experienced Players'}) expKillsnorm.show() And for Inexperienced players. inKillsnorm = normalplot(data= InExperienced, var_name= 'Kills') inKillsnorm.update_layout(title = {'text':'Density Plot of Kills by Inexperienced Players'}) inKillsnorm.show() Both groups show a clear violation of the normality principle, where the data is highly skewed to the right. For the Inexperienced players, despite considering excluding the players with 35 or more kills registered, it wouldn‚Äôt be significant, it will still remain skewed to the right. In cases where our data don‚Äôt have a normal distribution, we need to consider¬†non-parametric¬†alternatives to make assumptions. That is the main reason why we are going to use a¬†Mann Whitney U Test. from scipy.stats import mannwhitneyu def mannwhitneyu_est(data1, data2): statistic, p = mannwhitneyu(data1, data2, alternative = 'two-sided') return p print(mannwhitneyu_est(Experienced.Kills, InExperienced.Kills)) 0.0020247429887724775 Finally we reject the null hypothesis, since our p-value is less than 0.05 with a 95% of confidence. This means that population median of ‚ÄúExperienced Player Kills‚Äù is different from ‚ÄúInexperienced Player Kills‚Äù, the same we can conclude from the distribution. ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:4:3","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üîÉ Two-sample bootstrap hypothesis test for Kills metric After some meetings with the Level Designers, they aren‚Äôt convinced to take action upon the recommendations given from a sample size of 28 and 42 for the Experienced and Inexperienced groups respectively. So they require us to take another type of validation to ensure the confidence of our conclusion. Unfortunately, the Programmers require to get that insight as soon as possible to start fixing the patch. So, the only alternative to satisfy both parts is to perform a bootstrapping with the means and the p-value of the past test. Bootstrapping is one of the best choices to perform better results in sample statistics when we count with few logs. This Statistical Inferential Procedure works by resampling the data. We will take the same values sampled by group, from a resampling with replacement, to an extensive dataset to compare the means between the Kills metric of Experienced players and Inexperienced players. On the other side, we¬†can‚Äôt¬†use a permutation of the samples, since we can‚Äôt conclude that both groups' distributions are the same. Before this let‚Äôs create our bootstrapping replicates generator function. # Bootstrapping Means Function def draw_bs_reps(data,func,iterations): boot_arr = [] for i in range(iterations): boot_arr.append( func(data = np.random.choice(data, len(data))) ) return boot_arr # Bootstrapping Mann-Whitney Test def draw_bs_test(data1, data2, iterations): boot_arr = [] for i in range(iterations): estimate = mannwhitneyu_est(np.random.choice(data1, len(data1)),np.random.choice(data2, len(data2))) boot_arr.append(estimate) return boot_arr And with the next function we will create our new estimates for the empirical means and the values of the Mann-Whitney U Test. This function will return two p-values and two arrays of bootstrapped estimates, which we will describe next: p_emp: P-value measured from the estimated difference in means between both groups, showing the probabilities of randomly getting a ‚ÄúKills‚Äù value from the Experienced that will be greater than one of the Inexperienced group bs_emp_replicates: Array of bootstrapped difference in means p_test: P-value measured from the estimated probability of getting a statically significant p-value from the Mann-Whitney Test (p-value \u003c 0.05), in other words how many tests present different distribution for both groups bs_test_replicates: Array of bootstrapped p-values of Mann-Whitney Test from statistics import mean # Bootrapping probability estimates (p-value) def twosampleboot(data1, data2, operation, iterations): # Compute n bootstrap replicates from the groups arrays bs_replicates_1 = draw_bs_reps(data1, operation, iterations= iterations) bs_replicates_2 = draw_bs_reps(data2, operation, iterations= iterations) # Get replicates of empirical difference of means bs_emp = pd.Series(bs_replicates_1) - pd.Series(bs_replicates_2) # Compute empircal difference in means for p-value, where of Experienced-Kills \u003e InExperienced-Kills p_emp = np.sum(bs_emp \u003e= 0) / len(bs_emp) # Get replicates of Mann-Whitney U Test bs_test = draw_bs_test(data1= Experienced.Kills, data2= InExperienced.Kills, iterations= iterations) # Compute and print p-value of estimate with different distributions/medians p_test = np.sum([1 if i \u003c= 0.05 else 0 for i in bs_test]) / len(bs_test) return p_emp, bs_emp, p_test, bs_test # Return p-values and bs_replicates p_emp, bs_emp_replicates, p_test, bs_test_replicates = twosampleboot(Experienced.Kills, InExperienced.Kills, operation= mean, iterations= 10000) print('Diff. Means p-value =', p_emp, '\\n', 'Mann-Whitney Test p-value =',p_test) Diff. Means p-value = 0.9852 Mann-Whitney Test p-value = 0.8991 After making a sample of 10,000 replicates, we can conclude that 98% of the time, the disparity in the Kills average will be higher in Experienced Players than in Inexperienced players, and from that replicates 90% of them will present a significant difference between distributions. Now let‚Äôs ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:4:4","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"üóíÔ∏è Final thoughts \u0026 takeaways What can the stakeholders understand and take in consideration? There is a significant advantage of veteran players over the new ones, giving us the conclusion that the changes in mechanics made from the last Fallout New Vegas patch still very similar and don‚Äôt present meaningful changes, reason why veteran players are finding it easy and have a clear advantage like it‚Äôs usual, so for now the programmers should not be concerned about fixing the patch. What could the stakeholders do to take action? The Level Designers can consider working side by side with the UX team, to make recurrent revisions before launching a new patch, because always the community will expect an experience-centric work of the whole development team in terms of the quality delivered. Also for a future study we could consider to gather the difficulty category and merge it into the dataset and see if this variable is producing a significant variability in our final output. What can stakeholders keep working on? For significant changes in game mechanics, is recommended to do it just under a critical situation, like when is a significant bug affecting the player experience, otherwise it‚Äôs preferable to leave it until the launch of the next title reveal and test the audience reaction before the final implementation, if it‚Äôs possible. ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:5:0","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"‚ÑπÔ∏è Additional Information About the article This article was developed from the content explained in the Inferential statistics section of Chapter 3 of the Game Data Science book. All the conclusions made were inspired by a player-profiling from in-game metrics by using deductive reasoning where we assumed, and then we proved it using Significance Confidence and Variance, using Inferential analysis. All the assumptions and the whole case scenario were developed by the author of this article, for any suggestion I want to invite you to go to my about section and contact me. Thanks to you for reading as well. Related Content ‚Äî Game Data Science book and additional info at the Oxford University Press ‚Äî Anders Drachen personal¬†website Datasets This project was developed with a dataset provided by Anders Drachen et. al (2021), respecting the author rights of this book the entire raw data won‚Äôt be published, however, you can access the transformed data in my¬†Github repository. ","date":"2022-05-22","objectID":"/posts/control_mechanics_fallout/:6:0","tags":["t-test","Mann-Whitney","Python","Bootstrapping"],"title":"Inspecting Control Mechanics using T-Tests on Fallout New Vegas","uri":"/posts/control_mechanics_fallout/"},{"categories":["Projects"],"content":"Leveling Difficulty in Candy Crush Saga","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Today‚Äôs analysis will lead us to a world fulfilled with divine puzzle adventures at the side of Tiffi and Mr. Toffee, in which we‚Äôll glimpse the success rate of more than 6800 peers with eagerness for treats. The game is Candy Crush Saga a record-breaking mobile game developed by King, a subsidiary of Activision Blizzard since 2016. In terms of its gameplay, this game used to have a total of 816 episodes, until one of the major add-ons was removed (Dreamworld). The point is that now the game has 771 episodes, in which the player has 5 episodes per world, and, each episode contains contain 15 levels approximately. If you are one of the few that haven‚Äôt played Candy Crush, here‚Äôs a short intro: Looking for an interactive experience? üöÄ Download the Jupyter Notebook, available here ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:0:0","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"‚ö†Ô∏è Introduction to problem ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:1:0","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Hypothesis We‚Äôll review a game that potentially can lead any developer to many unseen problems, considering the abundance of levels. From the perspective of a customer, there can be several points of view that can emerge and, at the same time, can become unnoticed. That‚Äôs why our diagnosis will start from 2 potential hypothesis: The game is too easy so it became boring over time The game is too hard so the players leave it and become frustrated ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:1:1","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Potential Stakeholders None of the past hypotheses are the main intentions of the developers. So they require a Data Analyst to help with this task since the developers are seeing only the backend factors affecting the game, but it‚Äôs also critical to consider those external ones that affect the experience for the player and the sustainability of this game for the company. Among the main stakeholders could be: Level Designers: They work aligned with the rest of the Engineering Team because they still have a backend perspective and their next patch release needs to be aligned with the insights given by the analyst. Mobile Designer \u0026 User Retention Expert: This is a game whose main income input is in-game purchases because it‚Äôs a F2P, the main source of income is centered in retain the engagement in the game and keeping the consumers on the platform. Gameplay Engineer: They require to start working on the difficulty adjustment patch as soon as they receive the final statement. Executive Producer: Besides Candy Crush being an IP with internal producers since it‚Äôs developed and published by King, the parent company will expect to have an ROI aligned with their expectations. Players' community: They expect to have an endurable and great experience with a brief response in case of disconformities. Note: To facilitate the understanding of the roles of the development team, I invite you to take a look at this diagram that I designed. ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:1:2","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"üì• About the data ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:2:0","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Collection process and structure Before start let‚Äôs import the libraries we‚Äôre going to need import pandas as pd import numpy as np # For visualization import matplotlib.pyplot as plt %matplotlib inline import plotly.express as px import plotly.graph_objects as go # Own layout design library from vizformatter.standards import layout_plotly # Load layout base objects sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) Due to the extensive size of possible episodes to analyze, we‚Äôll limit the analysis to just one of them, which exactly will have data available for 15 levels. To do this, the analysts need to request a sample from the telemetry systems to get data related to the number of attempts per player in each episode level. Also, it‚Äôs important to mention that in terms of privacy, this analysis requires importing the data with the player_id codified for privacy reasons. Fortunately, in this case, Rasmus Baath, Data Science Lead at¬†castle.io, provided us with a Dataset with a sample gathered in 2014. df = pd.read_csv(\"datasets/candy_crush.csv\") df.head() We can see that our data is structured and consists of 5 attributes: player_id: a unique player id dt: the date level: the level number within the episode, from 1 to 15 num_attempts: number of level attempts for the player on that level and date num_success: number of level attempts that resulted in a success/win for the player on that level and date ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:2:1","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"üîß Data Preprocessing Before starting the analysis we need to do some validations on the dataset. # Count and display the number of unique players print(\"Number of players: \\n\", df.player_id.nunique(), '\\n', \"Number of records: \\n\", len(df.player_id),'\\n') # Display the date range of the data print(\"Period for which we have data: \\nFrom: \", min(df.dt), ' To:', max(df.dt)) Number of players: 6814 Number of records: 16865 Period for which we have data: From: 2014-01-01 To: 2014-01-07 ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:3:0","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Data Cleaning The data doesn‚Äôt require any kind of transformation and the data types are aligned with their purpose. print(df.dtypes) player_id object dt object level int64 num_attempts int64 num_success int64 dtype: object ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:3:1","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Data Consistency The usability of the data it‚Äôs rather good, since we don‚Äôt count with ‚ÄúNAN‚Äù (Not A Number), ‚ÄúNA‚Äù (Not Available), or ‚ÄúNULL‚Äù (an empty set) values. # Function the plot the percentage of missing values def na_counter(df): print(\"NaN Values per column:\") print(\"\") for i in df.columns: percentage = 100 - ((len(df[i]) - df[i].isna().sum())/len(df[i]))*100 # Only return columns with more than 5% of NA values if percentage \u003e 5: print(i+\" has \"+ str(round(percentage)) +\"% of Null Values\") else: continue # Execute function na_counter(df) NaN Values per column: None By this way, we can conclude that there were not errors in our telemetry logs during the data collection. ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:3:2","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Normalization Next, we can conclude there were no impossible numbers, except for a player that tried to complete the level 11 in 258 attempts with just 1 success. This is the only registry we exclude since it can be an influential outlier and we don‚Äôt rely on more attributes about him to create conclusions. Noticing the distribution of the quartiles and comprehending the purpose of our analysis, we can validate that the data is comparable and doesn‚Äôt need transformations. df = df[df.num_attempts != 258] df.describe() ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:3:3","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"üîç Exploratory Analysis \u0026 In-game interpretations ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:4:0","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Summary statistics Excluding the outliers we mentioned before, we got the next conclusions about their distribution and measurement: player_id Interpretation: Not unique and counts with 6814 distinct values which make sense since there is a player with records of multiple levels Data type: Nominal Measurement type: Discrete/String dt Interpretation: Only includes data from January 1st to January 7th of 2014. Also, the analysis won‚Äôt consider this as a lapse per player since the records per player are not continuous, so they will be limited as a timestamp Data type: Ordinal Measurement type: Temporal level Interpretation: They‚Äôre registered as numbers, but for further analysis will be transformed as factors. 50% of the records are equal to or less than level 9 Data type: Ordinal Measurement type: Discrete/String num_attempts Interpretation: The registries are consistent, the interquartile range mention that half of the players try between 1 and 7 time to complete each level. Furthermore, there are players with 0 attempts, so we need to evaluate if this is present at level 1, which can explain a problem in retention rate for that episode Data type: Numerical Measurement type: Integer num_success Interpretation: Most of the players are casual gamers because 75% of them complete the level and don‚Äôt repeat it Data type: Numerical Measurement type: Integer ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:4:1","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Levels played in Episode First, let‚Äôs examine the number of registries per player. This will tell us, from the episode how many levels have each player recorded in the lapse of 7 days. from plotly.subplots import make_subplots # Group data of amount of levels recorded by player id countdf = df.groupby('player_id')['level'].nunique().reset_index() # Count the number amount of players according to amount of levels recorded by player countdf = countdf.groupby('level')['player_id'].nunique().reset_index() # Arrange data according to amount of levels countdf.level = [str(i)+'s' for i in countdf.level] countdf = countdf.sort_values('player_id', ascending= False) # Generate CumSum cumulative_per = countdf.player_id / countdf.player_id.sum() * 100 cumulative_per = cumulative_per.cumsum() # Format new DF countdf = pd.concat([countdf, cumulative_per], axis = 1) countdf.columns = [\"levels\",\"players\",\"Cum_per\"] # Pareto Chart linec = make_subplots(specs=[[{\"secondary_y\": True}]]) # Bar plot graphic object linec.add_trace(go.Bar(x = countdf.levels, y = countdf.players, name = \"Players\", marker_color= \"#007FFF\"), secondary_y = False) # Scatter plot graphic object linec.add_trace(go.Scatter(x = countdf.levels, y = countdf.Cum_per/100, mode='lines+markers', name = \"Percentage\", marker_color= \"#FF5A5F\"), secondary_y = True) # Layout linec.update_layout(title = {'text':'Pareto Chart of Number of Levels recorded by player'}, xaxis = {\"title\":\"Number of Levels recorded\"}, yaxis = {\"title\":\"Unique players\"}) linec.update_layout(layout) linec.update_yaxes(tickformat = \"0%\", title = \"Cumulative Percentage\", secondary_y = True) linec.update_layout(showlegend=False) linec.add_hline(y=0.8, line_dash = \"dash\", line_color=\"red\", opacity=0.5, secondary_y = True) linec.add_annotation(sign) linec.show() From the last Pareto chart, we can deduce that 80% of the 6814 players just count with 3 levels recorded of 15. But, since this was extracted from a random sample, this won‚Äôt affect our study. ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:4:2","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Difficulty of completing a level in a single try There is a combination of easier and challenging levels. Chance and skills make the number of attempts required to pass a level different from one player to another. The presumption is that difficult levels demand more tries on average than easier ones. That is, the harder a level is the lower the likelihood to pass that level in a single try. In these circumstances, the¬†Bernoulli process might be useful. As a Boolean result, there are only two possibilities, win or lose. This can be measured by a single parameter: $p_{win} = \\frac{\\Sigma wins }{\\Sigma attempts }$: the probability of completing the level in a single attempt Let‚Äôs calculate the difficulty $p_{win}$ individually for each of the 15 levels. difficulty = df.groupby('level').agg(attempts = ('num_attempts', 'sum'), wins =('num_success', 'sum')).reset_index() difficulty['p_win'] = difficulty.wins / difficulty.attempts difficulty We have levels where 50% of players finished on the first attempt and others that are the opposite. But let‚Äôs visualize it through the episode, to make it clear. # Lineplot of Success Probability per Level line1 = px.line(difficulty, x='level', y=\"p_win\", title = \"Probability of Level Success at first attempt\", labels = {\"p_win\":\"Probability\", \"level\":\"Episode Level\"}) line1.update_layout(layout) line1.update_xaxes(range=[1,15], tick0 = 1, dtick = 1) line1.update_yaxes(range=[0,0.7], tick0 = 0, dtick = 0.1) line1.update_layout(yaxis_tickformat = \"0%\") line1.add_annotation(sign) line1.show() ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:4:3","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Defining hard levels It‚Äôs subjective what we can consider a hard level because not consistently depends on a single factor and for all player profile groups this can be different. So, for the outcomes of this study, we will arbitrarily assume that a difficult level is the one that has a probability to be completed in the first attempt of a 10% ($p_{win} \u003c 10%$). # Lineplot of Success Probability per Level line2 = go.Figure(go.Scatter( x = difficulty.level, y = difficulty.p_win)) line2.update_layout(title = {'text':'Probability of Level Success at first attempt with Hard Levels'}, xaxis = {\"title\":\"Episode Level\"}, yaxis = {\"title\":\"Probability\"}) line2.update_layout(layout) line2.update_xaxes(range=[1,15], tick0 = 1, dtick = 1) line2.update_layout(yaxis_tickformat = \"0%\") line2.update_layout(showlegend=False) line2.add_hrect(y0=0.02, y1=0.1, line_width=0, fillcolor=\"red\", opacity=0.2) line2.add_annotation(sign) line2.show() From our predefined threshold, we see that the level digit is not aligned with its difficulty. While we have hard levels as 5, 8, 9, and 15; others like 13 and 15 are unleveraged and need to be rebalanced by the level designers. ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:4:4","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"Measuring the uncertainty of success We should always report some calculation of the uncertainty of any provided numbers. Simply, because another sample will give us little different values for the difficulties measured by level. Here we will simply use the¬†Standard error¬†as a measure of uncertainty: $\\sigma_{error} \\approx \\frac{\\sigma_{sample}}{\\sqrt{n}}$ Here n is the number of datapoints and $\\sigma_{sample}$ is the sample standard deviation. For a Bernoulli process, the sample standard deviation is: $\\sigma_{sample} = \\sqrt{p_{win}(1-p_{win})}$ Therefore, we can calculate the standard error like this: $\\sigma_{error} \\approx \\sqrt{\\frac{p_{win}(1-p_{win})}{n} }$ Consider that every level has been played n number of times and we have their difficulty $p_{win}$. Now, let‚Äôs calculate the standard error for each level of this episode. # Computing the standard error of p_win for each level difficulty['error'] = np.sqrt(difficulty.p_win * (1 - difficulty.p_win) / difficulty.attempts) difficulty We have a measure of uncertainty for each levels' difficulty. As always, this would be more appealing if we plot it. Let‚Äôs use¬†error bars¬†to show this uncertainty in the plot. We will set the height of the error bars to one standard error. The upper limit and the lower limit of each error bar should be defined by: $p_{win} \\pm \\sigma_{error}$ # Lineplot of Success Probability per Level line3 = px.line(difficulty, x='level', y=\"p_win\", title = \"Probability of Level Success at first attempt with Error Bars\", labels = {\"p_win\":\"Probability\", \"level\":\"Episode Level\"}, error_y=\"error\") line3.update_layout(layout) line3.update_xaxes(range=[0,16], tick0 = 1, dtick = 1) line3.update_yaxes(range=[0,0.65], tick0 = 0, dtick = 0.1) line3.update_layout(yaxis_tickformat = \"0%\") line3.add_hrect(y0=0.02, y1=0.1, line_width=0, fillcolor=\"red\", opacity=0.2) line3.add_annotation(sign) line3.show() Looks like the difficulty estimates a very exact. Furthermore, for the hardest levels, the measure is even more precise, and that‚Äôs a good point because from this we can make valid conclusions based on that levels. As a curious fact, also we can measure the probability of completing all the levels of that episode in a single attempt, just for fun. # The probability of completing the episode without losing a single time prob = 1 for i in difficulty.p_win: prob = prob*i # Printing it out print(\"Probability of Success in one single attempt \\nfor whole episode: \", prob*100, \"%\") Probability of Success in one single attempt for whole episode: 9.889123140886191e-10 % ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:4:5","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"üóíÔ∏è Final thoughts \u0026 takeaways What can the stakeholders understand and take into consideration? From the sample extracted we conclude that just 33% of the levels are considered of high difficulty, which it‚Äôs acceptable since each episode counts with 15 levels, so by now the level designer should not worry about leveling the difficulty. What could the stakeholders do to take action ? As a suggestion, in the case that the Publisher decides to invest more in in-game mechanics, a solution for a long-time and reactive engagement could be the use of Machine Learning to generate a DGDB as some competitors have adapted in IPs like EA Sports FIFA, Madden NFL or the ‚ÄúAI Director‚Äù of Left 4 Dead. What can stakeholders keep working on? The way their level difficulty design work today is precise since our first hypothesis was that the game wasn‚Äôt too linear to unengaged the player and churn as consequence. Because as we saw, the game has drastic variations in the levels 5-6 and 8-10, which can help to avoid frustrations in players. ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:5:0","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"‚ÑπÔ∏è Additional Information About the article Based on the dataset provided, we will not proceed with a retention analysis as mentioned above. Because the data is from a random episode, if this were episode one, this type of analysis could be useful as it can explain the pool of players who log in, created an account, or install the game but never start playing, causing traction problems. Therefore, this will be left as a limitation to the scope of the analysis. With acknowledgment to Rasmus Baraath for guiding this project. Which was developed for sharing knowledge while using cited sources of the material used. Thanks to you for reading as well. Related Content ‚Äî Rasmus Baath personal¬†blog ‚Äî Anders Drachen personal¬†website Datasets This project was developed with a dataset provided by Rasmus Baraath, which also can be downloaded at my Github repository. ","date":"2022-04-19","objectID":"/posts/candy_crush_difficulty/:6:0","tags":["Bernoulli","Python"],"title":"Leveling Difficulty in Candy Crush Saga","uri":"/posts/candy_crush_difficulty/"},{"categories":["Projects"],"content":"A/B Testing with Cookie Cats Game","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"A lot of things come to mind when we hear the name¬†Cookie Cats, and probably is not what we think since is hard to associate both words, it‚Äôs about meow-sicians (Belle, Ziggy, Smokey, Rita, Berry). Anyway, Cookie Cats is a mobile puzzle game of¬†‚Äúconnect-three‚Äù-style developed by¬†Tactile Entertainment, a mobile games developer from Copenhagen. To be in context, this game‚Äôs main objective is to align 3 cookies of the same kind to feed a cat, and in this way finish each level. Also as collectible credit, you can earn Keys to unlock gates located at certain levels. In this project, in order to embrace the actual problem that the stakeholders are facing, we are going to make use of¬†Tactical Analytics, which is a branch of user-oriented game analytics, with the purpose to¬†‚Äúaim to inform game design at the short-term, for example, an A/B test of a new game feature‚Äù¬†(A. Dranchen, 2013). Knowing this we can notice that the applicability of statistics in new fields can be considered one of the greatest advances for the game industry. Nowadays, human-machine interactions are being monitored, in a good way in most cases. The main purpose is not just to increase the company‚Äôs revenue, one of the main objectives is to give a benefit in terms of User Experience (UX) and Engagement, and this can be covered using Data Science. Looking for an interactive experience? üöÄ Download the Jupyter Notebook, available here ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:0:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"‚ö†Ô∏è Introduction to problem ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:1:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Hypothesis According to Rasmus Baath, Data Science Lead at¬†castle.io, Tactile Entertainment is planning to move Cookie Cats' time gates from level 30 to 40, but they don‚Äôt know by how much the user retention can be impacted by this decision. This sort of ‚Äútime gate‚Äù is usually seen in¬†free-to-play¬†models, and normally contains ads that can be skipped in exchange for in-game purchases. In this case the player requires to submit a specific number of ‚ÄòKeys‚Äô, which also can be skipped in exchange of¬†in-game purchases. So seeing this viewpoint, a decision like this can impact not only user retention, the expected revenue as well that‚Äôs why we are going to set the initial hypothesis as: Moving the Time Gate from Level 30 to Level 40 will decrease our user retention. Moving the Time Gate from Level 30 to Level 40 will increase our user retention. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:1:1","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Potential Stakeholders Mobile Designer \u0026 User Retention Expert: They must be aligned with the final statement of the analyst, and make a final judgment to improve user retention. Level Designer: As the scene of levels is under study, the level designers need to take action on time to guarantee the storyline of the levels has a correct sequence, and to add it in a new patch. System Designer \u0026 System Engineer: If we extend the time gate, the credits should or should not remain at the same quantity required, which also needs to be implemented in the tracking system of the user progress. Executive Game Producer: As we mentioned before, a potential change requires a redesign of the earnings strategy and an alignment in the business expectation like funding, agreements, marketing, and patching deadlines. Players community: This stakeholder can be affected by the theory of hedonic adaptation, which is according to Rasmus Baath is ‚Äúthe tendency for people to get less and less enjoyment out of a fun activity over time if that activity is undertaken continuously‚Äù, meaning that if we prolong the time gate, this can affect the engagement in an unfavorable way, which in this case require an evaluation. Note: To facilitate the understanding of the roles of the development team, I invite you to take a look at this diagram that I designed. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:1:2","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üì• About the data ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:2:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Collection process and structure Most of the time game developers work aside of¬†telemetry systems, which according to Anders Drachen et al. (one of the¬†pioneers¬†in the Game Analytics field), from an interview made with Georg Zoeller of Ubisoft Singapore, the Game Industry manages two kinds of telemetry systems: Developer-facing:¬†‚ÄúThe main goal of the system is to facilitate and improve the production process, gathering and presenting information about how developers and testers interact with the unfinished game‚Äù. Like the one mentioned in Chapter 7 of the¬†‚ÄúGame Analytics Maximizing the Value of Player Data‚Äù¬†book, like the one implemented in Bioware‚Äôs production process of¬†Dragon Age: Origins¬†in 2009. User-facing:¬†This one is¬†‚Äúcollected after a game is launched and mainly aimed at tracking, analyzing, and visualizing player behavior‚Äù¬†mentioned in Chapters 4, 5, and 6 of the same book. With the help of this kind of data-fetching system, we can create a responsive gate between the Data Analysts and the Designers. In most cases, these systems collect the data in form of logs (.txt) or dictionaries (.json), but fortunately in this case we will count with a structured CSV file. # Importing pandas import pandas as pd # Reading in the data df = pd.read_csv('datasets/cookie_cats.csv') # Showing the first few rows df.head() This dataset contains around¬†90,189¬†records of players that started the game while the telemetry system was running, according to Rasmus Baath. Among the variables collected are the next: userid¬†: unique identification of the user. version¬†: the group in which the players were measured, for a time gate at level 30 it contains a string called¬†gate_30, or for a time gate at level 40 it contains a string called¬†gate_40. sum_gamerounds¬†: number of game rounds played within the first 14 days since the first session played. retention_1¬†: Boolean that defines if the player came back 1 day after the installation. retention_7¬†: Boolean that defines if the player came back 7 days after the installation. Note:¬†An important fact to keep in mind is that in the game industry one crucial metric is¬†retention_1, since it defines if the game generate a first engagement with the first log-in of the player. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:2:1","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üîß Data Preprocessing Before starting the analysis we need to do some validations on the dataset. # Count and display the number of unique players print(\"Number of players: \\n\", df.userid.nunique(), '\\n', \"Number of records: \\n\", len(df.userid),'\\n') Number of players: 90188 Number of records: 90188 It‚Äôs not common to find this kind of data, cause as we saw the data is almost ideally sampled, where we count just with distinct records. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:3:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Data Cleaning The data doesn‚Äôt require any kind of transformation and the data types are aligned with their purpose. df.dtypes userid int64 version object sum_gamerounds int64 retention_1 bool retention_7 bool dtype: object ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:3:1","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Data Consistency The usability of the data it‚Äôs rather good, since we don‚Äôt count with ‚ÄúNAN‚Äù (Not A Number), ‚ÄúNA‚Äù (Not Available), or ‚ÄúNULL‚Äù (an empty set) values. # Function the plot the percentage of missing values def na_counter(df): print(\"NaN Values per column:\") print(\"\") for i in df.columns: percentage = 100 - ((len(df[i]) - df[i].isna().sum())/len(df[i]))*100 # Only return columns with more than 5% of NA values if percentage \u003e 5: print(i+\" has \"+ str(round(percentage)) +\"% of Null Values\") else: continue # Execute function na_counter(df) NaN Values per column: None By this way, we can conclude that there were not errors in our telemetry logs during the data collection. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:3:2","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Normalization Noticing the distribution of the quartiles and comprehending the purpose of our analysis, where we only require sum_gamerounds as numeric, we can validate that the data is comparable and doesn‚Äôt need transformations. df.describe() ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:3:3","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üîç Exploratory Analysis \u0026 In-game interpretations ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:4:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Summary statistics We got the next conclusions about their distribution and measurement: userid Interpretation: Player identifier with distinct records in the whole dataset which can be transformed as a factor Data type: Nominal Measurement type: Discrete/String version Interpretation: Just two possible values to evaluate, time gate at level 30 or level 40 Data type: Ordinal Measurement type: Discrete/String sum_gamerounds Interpretation: Number of game rounds played by the user, where 50% of the users played between 5 and 51 sessions Data type: Numerical Measurement type: Integer retention_1 Interpretation: Boolean measure to verify that the player retention was effective for 1 day at least Data type: Nominal Measurement type: Discrete/String retention_7 Interpretation: Boolean measure to verify that the player retention was effective for 7 days at least Data type: Nominal Measurement type: Discrete/String ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:4:1","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Strategy of Analysis The most accurate way to test changes is to perform¬†A/B testing¬†by targeting a specific variable, in the case¬†retention¬†(for 1 and 7 days after installation). As we mentioned before, we have two groups in the¬†version¬†variable: Control group:¬†The time gate is located at level 30. We are going to consider this one as a no-treatment group. Treatment group:¬†The company plans to move the time gate to level 40. We are going to use this as a subject of study, due to the change involved. In an advanced stage, we are going to perform a¬†bootstrapping¬†technique, to be confident about the result comparison for the retention probabilities between groups. # Counting the number of players in each AB group. players_g30 = df[df['version'] == 'gate_30'] players_g40 = df[df['version'] == 'gate_40'] print('Number of players tested at Gate 30:', str(players_g30.shape[0]), '\\n', 'Number of players tested at Gate 40:', str(players_g40.shape[0])) Number of players tested at Gate 30: 44700 Number of players tested at Gate 40: 45489 ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:4:2","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Game rounds distribution As we see the proportion of players sampled for each group is balanced, so for now, only exploring the Game Rounds data is in the queue. Let‚Äôs see the distribution of Game Rounds (The plotly¬†layout¬†created is available in vizformatter library). import matplotlib.pyplot as plt %matplotlib inline import plotly.express as px # Own layout design library from vizformatter.standards import layout_plotly # Load layout base objects sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) # Distribution Boxplot with outliers box1 = px.box(df, x=\"sum_gamerounds\", title = \"Game Rounds Overall Distribution by player\", labels = {\"sum_gamerounds\":\"Game Rounds registered\"}) box1.update_layout(layout) box1.add_annotation(sign) box1.show() For now, we see that exist clear outliers in the dataset since one user has recorded¬†49,854¬†Game rounds played in less than 14 days, meanwhile, the max recorded, excluding the outlier, is around¬†2,900. The only response to this case situation is a ‚Äúbot‚Äù, a ‚Äúbug‚Äù or a ‚Äúglitch‚Äù. Nevertheless, it‚Äôs preferable to clean it, since only affected one record. Let‚Äôs prune it. df = df[df['sum_gamerounds'] != 49854] We can make an¬†Empirical Cumulative Distribution Function, to see the real distribution of our data. Note:¬†In this case, we won‚Äôt use histograms to avoid a binning bias. import plotly.graph_objects as go # Import numpy library import numpy as np # ECDF Generator function def ecdf(data): # Generate ECDF (Empirical Cumulative Distribution Function) # for on dimension arrays n = len(data) # X axis data x = np.sort(data) # Y axis data y = np.arange(1, n+1) / n return x, y # Generate ECDF data x_rounds, y_rounds = ecdf(df['sum_gamerounds']) # Generate percentile makers percentiles = np.array([5,25,50,75,95]) ptiles = np.percentile(df['sum_gamerounds'], percentiles) # ECDF plot ecdf = go.Figure() # Add traces ecdf.add_trace(go.Scatter(x=x_rounds, y=y_rounds, mode='markers', name='Game Rounds')) ecdf.add_trace(go.Scatter(x=ptiles, y=percentiles/100, mode='markers+text', name='Percentiles', marker_line_width=2, marker_size=10, text=percentiles, textposition=\"bottom right\")) ecdf.update_layout(layout) ecdf.update_layout(title='Game Rounds Cumulative Distribution Plot', yaxis_title=\"Cumulative Probability\") ecdf.add_annotation(sign) ecdf.show() As we see 95% of our data is below 500 Game Rounds. print(\"The 95 percentile of the data is at: \", ptiles[4], \"Game Rounds\",\"\\n\", \"This means \", df[df[\"sum_gamerounds\"] \u003c= ptiles[4]].shape[0], \" players\") The 95 percentile of the data is at: 221.0 Game Rounds This means 85706 players For us, this can be considered a valuable sample. In the plot above, we saw some players that installed the game but, then never return (0 game rounds). print(\"Players inactive since installation: \", df[df[\"sum_gamerounds\"] == 0].shape[0]) Players inactive since installation: 3994 And in most cases, players just play a couple of game rounds in their first two weeks. But, we are looking for players that like the game and to get hooked, that‚Äôs one of our interests. A common metric in the video gaming industry for how fun and engaging a game is 1-day retention as we mentioned before. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:4:3","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üéÆüìä Player retention model Retention is the percentage of players that come back and plays the game one day after they have installed it. The higher 1-day retention is, the easier it is to retain players and build a large player base. According to Anders Drachen et al. (2013), these customer kind metrics¬†‚Äúare notably interesting to professionals working with marketing and management of games and game development‚Äù, also this metric is described simply as¬†‚Äúhow sticky the game is‚Äù, in other words, it‚Äôs essential. As a first step, let‚Äôs look at what 1-day retention is overall. # The % of users that came back the day after they installed prop = len(df[df['retention_1'] == True]) / len(df['retention_1']) * 100 print(\"The overall retention for 1 day is: \", str(round(prop,2)),\"%\") The overall retention for 1 day is: 44.52 % Less than half of the players come back one day after installing the game. Now that we have a benchmark, let‚Äôs look at how 1-day retention differs between the two AB groups. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:5:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üîÉ 1-day retention by A/B Group Computing the retention individually, we have the next results. # Calculating 1-day retention for each AB-group # CONTROL GROUP prop_gate30 = len(players_g30[players_g30['retention_1'] == True])/len(players_g30['retention_1']) * 100 # TREATMENT GROUP prop_gate40 = len(players_g40[players_g40['retention_1'] == True])/len(players_g40['retention_1']) * 100 print('Group 30 at 1 day retention: ',str(round(prop_gate30,2)),\"%\",\"\\n\", 'Group 40 at 1 day retention: ',str(round(prop_gate40,2)),\"%\") Group 30 at 1 day retention: 44.82 % Group 40 at 1 day retention: 44.23 % It appears that there was a slight decrease in 1-day retention when the gate was moved to level 40 (44.23%) compared to the control when it was at level 30 (44.82%). It‚Äôs a smallish change, but even small changes in retention can have a huge impact. While we are sure of the difference in the data, how confident should we be that a gate at level 40 will be more threatening in the future? For this reason, it‚Äôs important to consider bootstrapping techniques, this means¬†‚Äúa sampling with replacement from observed data to estimate the variability in a statistic of interest‚Äù. In this case, retention, and we are going to do a function for that. # Bootstrapping Function def draw_bs_reps(data,func,iterations=1): boot_Xd = [] for i in range(iterations): boot_Xd.append(func(data = np.random.choice(data, len(data)))) return boot_Xd # Retention Function def retention(data): ret = len(data[data == True])/len(data) return ret ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:6:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Control Group Bootstrapping # Bootstrapping for gate 30 btg30_1d = draw_bs_reps(players_g30['retention_1'], retention, iterations = 1000) ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:6:1","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Treatment Group Bootstrapping # Bootstrapping for gate 40 btg40_1d = draw_bs_reps(players_g40['retention_1'], retention, iterations = 1000) Now, let‚Äôs check the results import plotly.figure_factory as ff mean_g40 = np.mean(btg40_1d) mean_g30 = np.mean(btg30_1d) # A Kernel Density Estimate plot of the bootstrap distributions boot_1d = pd.DataFrame(data = {'gate_30':btg30_1d, 'gate_40':btg40_1d}, index = range(1000)) # Plotting histogram hist_1d = [boot_1d.gate_30, boot_1d.gate_40] dist_1d = ff.create_distplot(hist_1d, group_labels=[\"Gate 30 (Control)\", \"Gate 40 (Treatment)\"], show_rug=False, colors = ['#3498DB','#28B463']) dist_1d.add_vline(x=mean_g40, line_width=3, line_dash=\"dash\", line_color=\"#28B463\") dist_1d.add_vline(x=mean_g30, line_width=3, line_dash=\"dash\", line_color=\"#3498DB\") dist_1d.add_vrect(x0=mean_g30, x1=mean_g40, line_width=0, fillcolor=\"#F1C40F\", opacity=0.2) dist_1d.update_layout(layout) dist_1d.update_layout(xaxis_range=[0.43,0.46]) dist_1d.update_layout(title='1-Day Retention Bootstrapping by A/B Group', xaxis_title=\"Retention\") dist_1d.add_annotation(sign) dist_1d.show() The difference still looking close, for this reason, is preferable to zoom it by plotting the difference as an individual measure. # Adding a column with the % difference between the two AB-groups boot_1d['diff'] = ( ((boot_1d['gate_30'] - boot_1d['gate_40']) / boot_1d['gate_40']) * 100 ) # Ploting the bootstrap % difference hist_1d_diff = [boot_1d['diff']] dist_1d_diff = ff.create_distplot(hist_1d_diff, show_rug=False, colors = ['#F1C40F'], group_labels = [\"Gate 30 - Gate 40\"], show_hist=False) dist_1d_diff.add_vline(x= np.mean(boot_1d['diff']), line_width=3, line_dash=\"dash\", line_color=\"black\") dist_1d_diff.update_layout(layout) dist_1d_diff.update_layout(xaxis_range=[-3,6]) dist_1d_diff.update_layout(title='Percentage of \"1 day retention\" difference between A/B Groups', xaxis_title=\"% Difference\") dist_1d_diff.add_annotation(sign) dist_1d_diff.show() From this chart, we can see that the percentual difference is around¬†1% - 2%, and that most of the distribution is above¬†0%, in favor of a gate at level 30. But, what is the probability that the difference is above¬†0%? Let‚Äôs calculate that as well. # Calculating the probability that 1-day retention is greater when the gate is at level 30 prob = (boot_1d['diff'] \u003e 0.0).sum() / len(boot_1d['diff']) # Pretty printing the probability print('The probabilty of Group 30 (Control) having a higher \\nretention than Group 40 (Treatment) is: ', prob*100, '%') The probabilty of Group 30 (Control) having a higher retention than Group 40 (Treatment) is: 96.39999999999999 % ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:6:2","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üîÉ 7-day retention by A/B Group The bootstrap analysis tells us that there is a high probability that 1-day retention is better when the time gate is at level 30. However, since players have only been playing the game for one day, likely, most players haven‚Äôt reached level 30 yet. That is, many players won‚Äôt have been affected by the gate, even if it‚Äôs as early as level 30. But after having played for a week, more players should have reached level 40, and therefore it makes sense to also look at 7-day retention. That is:¬†What percentage of the people that installed the game also showed up a week later to play the game again? Let‚Äôs start by calculating 7-day retention for the two AB groups. # Calculating 7-day retention for both AB-groups ret30_7d = len(players_g30[players_g30['retention_7'] == True])/len(players_g30['retention_7']) * 100 ret40_7d = len(players_g40[players_g40['retention_7'] == True])/len(players_g40['retention_7']) * 100 print('Group 30 at 7 day retention: ',str(round(ret30_7d,2)),\"%\",\"\\n\", 'Group 40 at 7 day retention: ',str(round(ret40_7d,2)),\"%\") Group 30 at 7 day retention: 19.02 % Group 40 at 7 day retention: 18.2 % Like with 1-day retention, we see that 7-day retention is barely lower (18.20%) when the gate is at level 40 than when the time gate is at level 30 (19.02%). This difference is also larger than for 1-day retention. We also see that the overall 7-day retention is lower than the overall 1-day retention; fewer people play a game a week than a day after installing. But as before, let‚Äôs use bootstrap analysis to figure out how sure we can be of the difference between the AB-groups. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:7:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Control \u0026 Treatment Group Bootstrapping # Creating a list with bootstrapped means for each AB-group # Bootstrapping for CONTROL group btg30_7d = draw_bs_reps(players_g30['retention_7'], retention, iterations = 500) # Bootstrapping for TREATMENT group btg40_7d = draw_bs_reps(players_g40['retention_7'], retention, iterations = 500) boot_7d = pd.DataFrame(data = {'gate_30':btg30_7d, 'gate_40':btg40_7d}, index = range(500)) # Adding a column with the % difference between the two AB-groups boot_7d['diff'] = (boot_7d['gate_30'] - boot_7d['gate_40']) / boot_7d['gate_30'] * 100 # Ploting the bootstrap % difference hist_7d_diff = [boot_7d['diff']] dist_7d_diff = ff.create_distplot(hist_7d_diff, show_rug=False, colors = ['#FF5733'], group_labels = [\"Gate 30 - Gate 40\"], show_hist=False) dist_7d_diff.add_vline(x= np.mean(boot_7d['diff']), line_width=3, line_dash=\"dash\", line_color=\"black\") dist_7d_diff.update_layout(layout) dist_7d_diff.update_layout(xaxis_range=[-4,12]) dist_7d_diff.update_layout(title='Percentage of \"7 day retention\" difference between A/B Groups', xaxis_title=\"% Difference\") dist_7d_diff.add_annotation(sign) dist_7d_diff.show() # Calculating the probability that 7-day retention is greater when the gate is at level 30 prob = (boot_7d['diff'] \u003e 0).sum() / len(boot_7d) # Pretty printing the probability print('The probabilty of Group 30 (Control) having a higher \\nretention than Group 40 (Treatment) is: ~', prob*100, '%') The probabilty of Group 30 (Control) having a higher retention than Group 40 (Treatment) is: ~ 100.0 % ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:7:1","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"üóíÔ∏è Final thoughts \u0026 takeaways What can the stakeholders understand and take in consideration? As we underlined retention is crucial, because if we don‚Äôt retain our player base, it doesn‚Äôt matter how much money they spend in-game purchases. So, why is retention higher when the gate is positioned earlier? Normally, we could expect the opposite: The later the obstacle, the longer people get engaged with the game. But this is not what the data tells us, we explained this with the theory of hedonic adaptation. What could the stakeholders do to take action? Now we have enough statistical evidence to say that 7-day retention is higher when the gate is at level 30 than when it is at level 40, the same as we concluded for 1-day retention. If we want to keep consumer retention high, we should not move the gate from level 30 to level 40, it means we keep our Control method in the current gate system. What can stakeholders keep working on? For coming strategies the Game Designers can consider that, by pushing players to take a break when they reach a gate, the fun of the game is postponed. But, when the gate is moved to level 40, they are more likely to quit the game because they simply got bored of it. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:8:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"‚ÑπÔ∏è Additional Information About the article With acknowledgment to Rasmus Baraath for guiding this project. Which was developed for sharing knowledge while using cited sources of the material used. Thanks to you for reading as well. Related Content For more content related to the authors mentioned, I invite you to visit the next sources: ‚Äì Anders Drachen personal¬†website. ‚Äì Rasmus Baath personal¬†blog. ‚Äì Georg Zoeller personal¬†keybase. Also in case you want to share some ideas, please visit the¬†About¬†section and contact me. Datasets This project was developed with a dataset provided by Rasmus Baath, which also can be downloaded at my¬†Github repository. ","date":"2022-02-02","objectID":"/posts/ab_testing_cookiecats/:9:0","tags":["A/B Testing","Python","Bootstrapping"],"title":"A/B Testing with Cookie Cats Game","uri":"/posts/ab_testing_cookiecats/"},{"categories":["Projects"],"content":"Video Games History explained with Pandas","date":"2021-11-01","objectID":"/posts/evga/","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"The industry of video games revenues is reaching the $173.7 billion in value, with around 2.5 billion users enjoying them worldwide, with a forecasted value of $314.40 billion by 2026 according to Mordor Intelligence. Impressive facts, right? Nowadays this market is no longer considered a simple hobby for kids, it has become a constantly growing giant which attracts more and more customers as it takes advantage of the growth of streaming platforms. But this industry, as we well know, is taking more fields, born from ambitious expectations such as the Nintendo World Championships in the 90‚Äôs to what today many have adopted as a lifestyle also known as Esports. Looking for an interactive experience? üöÄ Download the Jupyter Notebook, available here We‚Äôll take a tour through the history of videogames, starting from late 70s and early the 80s. However, as a way of clarification, if you are a member of the culture, it‚Äôs important to mention that due to limitations of the scope of data available for analysis, Tomohiro Nishikado‚Äôs masterpiece, released as Space Invaders, will not be part of the analysis; and in case you‚Äôre not a member don‚Äôt worry this is for you as well. From an optimistic point of view, we will analyze quite important historical data, because is difficult to even think about getting the 70s data like Pong; and another advantage is that we can start our journey from the market revolution in the early 80s. Before starting our journey, like any exploratory data analysis we must import our libraries. # To manage Dataframes import pandas as pd # To manage number operators import numpy as np # To do interactive visualizations import plotly.express as px import plotly.graph_objects as go import plotly.figure_factory as ff # Format from vizformatter.standards import layout_plotly Now, let‚Äôs import our data. We must consider that we already prepared it, as shown in this articles‚Äôs footnote1. # Data frame of videogames df = pd.read_csv(\"data/videogames.csv\", na_values=[\"N/A\",\"\", \" \", \"nan\"], index_col=0) In addition to facilitate the management of dates in the visualizations, two extra columns will be generated, one as a Timestamp and another as a String, which will be used only if required. # Transform Year column to a timestamp df[\"Year_ts\"] = pd.to_datetime(df[\"Year_of_Release\"], format='%Y') # Transform Year column to a string df[\"Year_str\"] = df[\"Year_of_Release\"].apply(str) \\ .str.slice(stop=-2) Also we can import the layout format as a variable from my repository. sign, layout = layout_plotly(height= 720, width= 1000, font_size= 15) ","date":"2021-11-01","objectID":"/posts/evga/:0:0","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Data integrity validation First, we check our current dataset using the method .info() df.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e Int64Index: 16716 entries, 0 to 16718 Data columns (total 20 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Name 16716 non-null object 1 Year_of_Release 16447 non-null float64 2 Publisher 16662 non-null object 3 Country 9280 non-null object 4 City 9279 non-null object 5 Developer 10096 non-null object 6 Platform 16716 non-null object 7 Genre 16716 non-null object 8 NA_Sales 16716 non-null float64 9 EU_Sales 16716 non-null float64 10 JP_Sales 16716 non-null float64 11 Other_Sales 16716 non-null float64 12 Global_Sales 16716 non-null float64 13 Critic_Score 8137 non-null float64 14 Critic_Count 8137 non-null float64 15 User_Score 7590 non-null float64 16 User_Count 7590 non-null float64 17 Rating 9950 non-null object 18 Year_ts 16447 non-null datetime64[ns] 19 Year_str 16716 non-null object dtypes: datetime64[ns](1), float64(10), object(9) memory usage: 2.7+ MB To one side we find a great variety of data and attributes, to the other one we see that of the total of 16,716 records there are several attributes with a significant number of null values, which we are going to see next, in percentage terms. # Function the plot the percentage of missing values def na_counter(df): print(\"NaN Values per column:\") print(\"\") for i in df.columns: percentage = 100 - ((len(df[i]) - df[i].isna().sum())/len(df[i]))*100 # Only return columns with more than 5% of NA values if percentage \u003e 5: print(i+\" has \"+ str(round(percentage)) +\"% of Null Values\") else: continue # Execute function na_counter(df) NaN Values per column: Country has 44% of Null Values City has 44% of Null Values Developer has 40% of Null Values Critic_Score has 51% of Null Values Critic_Count has 51% of Null Values User_Score has 55% of Null Values User_Count has 55% of Null Values Rating has 40% of Null Values These correspond to the attributes that hold more than 5% of the null values considering a confidence standard, which consists of having at least 95% of the data. In a visual way, we can look at it in the following graphic. # Make a dataframe of the number of Missing Values per attribute df_na = df.isna().sum().reset_index() # Rename our dataframe columns df_na.columns = [\"Column\",\"Missing_Values\"] # Plot barchart of Missing Values barna = px.bar(df_na[df_na[\"Missing_Values\"] \u003e 0].sort_values (\"Missing_Values\", ascending = False), y=\"Missing_Values\", x=\"Column\", color=\"Missing_Values\", opacity=0.7, title = \"Total of Missing Values per attribute\", color_continuous_scale= \"teal\", labels = {\"Missing_Values\":\"Missing Values\"}) # Update layout barna.update_layout(layout) barna.update_annotations(sign) barna.show() \rWe see that there is a significant quantity of null values, predominantly in columns related to critics and their respective value (Metacritic); as well as its content Rating made by ESRB (Entertainment Software Rating Board). Still, since these are not categorical variables, they won‚Äôt have an identifier role, in which case our main interest will be ‚ÄúName‚Äù and ‚ÄúYear_of_Release‚Äù, and subsequently their elimination or omission will be evaluated if necessary. ","date":"2021-11-01","objectID":"/posts/evga/:1:0","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Exploratory Video Game Analysis (EVGA) Before starting with our expedition we should begin by understanding the behavior of the data with which our analysis will be built, for this we‚Äôll use the .describe() method. # Modify decimal number attribute pd.options.display.float_format = \"{:.2f}\".format # Print description df.describe() The numerical attributes show us that we have a total of 40 years of records (from 1980 to 2020) of sales in North America, Europe, Japan, and other parts of the world. Where the median indicates that 50% of the years recorded are less than or equal to 2007, and we did not find outliers. Also, the average sales value is higher in North America despite the fact that the average sale of the titles is around 263,000 units, but its variation is quite high, so it should be compared more exhaustively. From a historical point of view, it makes sense, that knowing that the focus of sales is North America, cause even during the 60s the head of Nintendo of America, Minoru Arakawa, decided to expand their operations in the United States starting from the world of the arcade, so we can have the hypothesis to see this as a place of opportunities for this market. ","date":"2021-11-01","objectID":"/posts/evga/:2:0","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Golden Age of Videogames 1977 ‚Äì Launch of Atari 2600\rWe will begin with the global view, I mean, the superficial perspective of the sales during this period. # Mask to subset games from 1980 to 1990 games8090 = df[\"Year_of_Release\"].isin(np.arange(1980,1991)) # Top publishers between 1980 and 1990 top_pub = df[df[\"Year_of_Release\"]\u003c=1990].groupby(\"Publisher\") \\ .sum(\"Global_Sales\") \\ .sort_values(\"Global_Sales\", ascending = False)[\"Global_Sales\"] \\ .head(10) # Dataframe for Line Plot of most frequent companies df_sales_ts = df[games8090][df[\"Publisher\"].isin(top_pub.index)] \\ .pivot_table(values = \"Global_Sales\", index = [\"Year_of_Release\", \"Year_str\", \"Year_ts\", \"Publisher\",\"Platform\"], aggfunc= np.sum) \\ .reset_index() \\ .sort_values(\"Year_of_Release\", ascending = True) \\ .groupby([\"Publisher\",\"Year_ts\"]) \\ .sum() \\ .reset_index() # Plot a lineplot gline = px.line(df_sales_ts, x=\"Year_ts\", y=\"Global_Sales\", color='Publisher', labels={\"Year_ts\": \"Years\", \"Global_Sales\": \"Millions of Units Sold\", \"total_bill\": \"Receipts\"}, title = \"Millions of units during Golden Age sold by Publisher\") # To plot markers for i in np.arange(0,10): gline.data[i].update(mode='markers+lines') # Update Layout gline.update_layout(layout) gline.update_annotations(sign) gline.show() \rAs we can see at the beginning of the decade and probably after 1977, the market was dominated by Atari Studios while Activision was its main competitor in terms of IPs, because these competitors eventually published their titles on the Atari 2600, example of this was Activision with Kaboom! or Parker Bros with Frogger. Another important fact is that in 1982 we can remember that it was one of the best times for Atari where they published titles that had a great impact such as Tod Frye‚Äôs Pac-Man. # Mask of 1982 games games82 = df[df.Year_of_Release == 1982] # Distribution column games82['Distribution'] = (games82.Global_Sales/sum(games82.Global_Sales))*100 # Extracting top titles of 1982 games82 = games82.sort_values('Distribution', ascending=False).head(10) # Fix Publisher Issue of Mario Bros., this game was originally published by # Nintendo for arcades games82.loc[games82.Name == 'Mario Bros.','Publisher'] = 'Nintendo' # Distribution bar82 = px.bar(games82, y='Distribution', text='Distribution', x='Name', color = 'Publisher', title='Distribution of total sales in 1982 by Publisher', labels={\"Distribution\":\"Market Participation distribution\", \"Name\":\"Videogame title\"}) # Adding text of percentages bar82.update_traces(texttemplate='%{text:.3s}%', textposition='outside') # Update layout bar82.update_layout(layout) bar82.update_annotations(sign) bar82.show() \rIt is evident that the adaptation of this arcade game released in 1980, had outstanding sales, once it was introduced to the world of the Atari 2600. According to the documentary ‚ÄúOunce Upon Atari‚Äù, episode 4 to be exactly, this title managed to sell more than 7 million copies, due to optimizations in the display and in the intelligence of the NPCs, compared to the original version. FYI: The version of Mario Bros in the dataset corresponds to the Atari 2600 and Arcade version are different from the success that was later introduced to the NES. 1983 ‚Äì Crisis of the Video Game industry\rUndoubtedly, the timeline above shows a clear drop in sales from 1983. And yes, I‚Äôm sure they want to know what happened here. For sure, if we had Howard Scott Warshaw talking with us, we would surely understand one of the crudest realities in this industry‚Äôs history, since he lived this in his own flesh. But in this case, I will explain. In summary, he was one of the greatest designers of that moment, who was hired to design a video game based on one of the biggest hits in cinema, E.T. the Extra-Terrestrial. At the time Steven Spielberg shares the vision of a game very similar to Pac-Man, something extremely strange, and by the way a release date is designated just a few months after this. As you may have thought, it was a complete disast","date":"2021-11-01","objectID":"/posts/evga/:2:1","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"I World Console War (WCWI) 1989 - Sega Enterprises Inc. launches worldwide Sega Megadrive Genesis\r1991 - Nintendo launches worldwide Super Nintendo Entertainment system\rAt the beginning of the 90s, after the launch of the SEGA and Nintendo consoles, the First World War of Videogames began. Mainly in two of their biggest titles, Sonic The Hedgedog and Super Mario Bros. During 1990, approximately 90% of the US market was controlled by Nintendo, until in 1992 SEGA began to push with strong marketing campaigns aimed at an older audience. One of the most remarkable fact of this period was the launch of Mortal Kombat in 1992, where Nintendo censored part of its content (blood-content) since they had an initiative to be a family friendly company, and of course this became very annoying the followers of this series. # Transform current dataframe as long format df_long = df.melt(id_vars = [\"Name\",\"Platform\",\"Year_of_Release\",\"Genre\", \"Publisher\", \"Developer\", \"Rating\", \"Year_str\", \"Year_ts\", \"Country\", \"City\",\"Critic_Score\",\"User_Score\"], value_vars = [\"NA_Sales\", \"EU_Sales\",\"JP_Sales\",\"Other_Sales\"], var_name = [\"Location\"], value_name = \"Sales\") # Giving a better format to the location's Name df_long = df_long.replace({\"Location\": {\"NA_Sales\": \"North America\", \"EU_Sales\": \"Europe\", \"JP_Sales\": \"Japan\", \"Other_Sales\": \"Rest of the World\"} }) # To delete columns without sales registry df_long = df_long[df_long[\"Sales\"] \u003e 0].dropna(subset = [\"Sales\"]) # Dataframe df_gen90 = df_long[(df_long[\"Year_of_Release\"] \u003e 1989) \u0026 (df_long[\"Year_of_Release\"] \u003c 2000)] \\ .pivot_table(values = \"Sales\", index = \"Genre\", columns = \"Location\", aggfunc = np.sum) # Image plot ima = px.imshow(df_gen90.reset_index(drop=True).T, y= [\"Europe\",\"Japan\",\"North America\",\"Rest of the Worlds\"], x= ['Action', 'Adventure', 'Fighting', 'Misc', 'Platform', 'Puzzle', 'Racing', 'Role-Playing', 'Shooter', 'Simulation', 'Sports','Strategy'], labels=dict(color=\"Total Sales in Millions\"), color_continuous_scale='RdBu_r', title = \"Heatmap of Location vs Genre during WCWI\") # Update layout ima.update_layout(layout) ima.update_annotations(sign) ima.show() \rFollowing the Mortal Kombat censorship, Nintendo was hit, noticing that fighting genres were among the most purchased during the 90s. However, the success of Nintendo IPs such as The Legend of Zelda and Super Mario, ended up destroying the SEGA console in 1998, in addition because of Nintendo grew stronger thanks to role-playing games during these years. # Dataframe with just SNES and GEN, Super Mario was removed to avoid outliers df_sn = df_long[(df_long[\"Year_of_Release\"] \u003e 1989) \u0026 (df_long[\"Year_of_Release\"] \u003c 2000) \u0026 ((df_long[\"Platform\"].isin([\"GB\",\"NES\",\"SNES\",\"GEN\",\"PC\",\"PS\",\"N64\",\"DC\"])) )].sort_values(\"Year_of_Release\", ascending=True).drop(18) # Plot of sales during 90s strip90 = px.strip(df_sn, x = \"Year_of_Release\", y = \"Sales\", color = \"Platform\", hover_name=\"Name\", labels={\"Name\":\"Title\", \"Year_of_Release\":\"Year\"}, hover_data=[\"Country\"]) # Update layout strip90.update_layout(layout) strip90.update_traces(jitter = 1) strip90.update_annotations(sign) strip90.show() \rThe first impression, when looking at this graph is that we notice the great dominance of Nintendo since the sales of the Sega Genesis collapsed in 1995, until during the Sega Dreamcast campaign, where the leadership was taken by the GameBoy and the Nintendo 64, followed by the new competitor Play Station, a topic that we will cover later. ","date":"2021-11-01","objectID":"/posts/evga/:2:2","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Role-playing game revolution One of the most characteristic events of this time was the implementation of 16-bit graphic technologies, which at the time was double what was available. Along with this, the Japanese once again made another master move, which gave a decisive turn to a genre, after the expected fall of RPGs on the PC platform. Before highlighting the Japanese originality, it is necessary to know after successes of role-playing games such as Ultima VIII: Pagan (PC), this genre had a slow development, since the CD-ROMs generated great graphic expectations for the developers, by the way prolonging the releases, and for sure this caused a lack of interest from the community, and began to move towards action games or first person shooter such as Golden Eye (1997). However, success stories continued to appear in this genre such as Diablo (1996), developed by Blizzard Entertainment. # Dataframe for Genre lineplot df90G = df_long[(df_long[\"Year_of_Release\"] \u003e 1989) \u0026 (df_long[\"Year_of_Release\"] \u003c 2000) \u0026 ((df_long[\"Genre\"] == \"Role-Playing\") | (df_long[\"Genre\"] == \"Action\") | (df_long[\"Genre\"] == \"Platform\") | (df_long[\"Genre\"] == \"Fighting\") )] \\ .groupby([\"Genre\", \"Year_ts\"]).sum(\"Sales\").reset_index() # Plot an animated lineplot linegen = px.line(df90G, x=\"Year_ts\", y=\"Sales\", color=\"Genre\", title = \"Millions of units sold during 90s by Genre \", labels={\"Sales\": \"Millions of Units Sold\", \"Year_ts\":\"Years\"}) # To plot markers for i in np.arange(0,4): linegen.data[i].update(mode='markers+lines') # Update layout linegen.update_layout(layout) linegen.update_annotations(sign) linegen.show() \rAmong all genres, the growth of RPGs over time must be underlined. The release of Pok√©mon in 1996 for GameBoy by the developer Game Freak was a success for Nintendo, which swept everything in its path, with its first generation of Pokemon Blue, Red and Yellow that was released in 1999, the latter is the fourth Japanese version. ","date":"2021-11-01","objectID":"/posts/evga/:2:3","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"A new Japanese Ruler takes the throne 1994 - Sony Computer Entertainment's PlayStation is born\rRPGs not only boosted Nintendo, but multiplatform IPs like Final Fantasy VII gave companies such as Sony Computer Entertainment a boost during the introduction of their new 32-bit console and at the same time to publicize the JRPGs within the western market. In 1995, when Sony planned their introduction of the PlayStation to America, they chose not to focus their Video Game market on a single type of genre or audience, but instead diversified their video game portfolio and memorable titles such as Crash Bandicoot, Metal Gear Solid and Tekken emerged. # Subset only PS games df_sony = df[(df[\"Year_of_Release\"].isin([1995,1996])) \u0026 (df[\"Platform\"] == \"PS\")] # Subset the columns needed df_sony = df_sony[[\"Name\",\"Year_of_Release\",\"Publisher\",\"Platform\",\"Genre\", \"Global_Sales\"]] # Pie plot piesony = px.pie(df_sony, values= \"Global_Sales\", names='Genre', color_discrete_sequence = px.colors.sequential.Blues_r, labels={\"Global_Sales\":\"Sales\"}) # Update layout piesony.update_layout(layout) piesony.update_traces({\"textinfo\":\"label+text+percent\", \"hole\":0.15}) piesony.update_annotations(sign) piesony.show() \rAs we can see in the graph, Sony‚Äôs video game distribution during its first two years in the North American market. Even if we pay attention, titles like Tekken and Mortal Kombat had a significant presence by showing the highest levels of sales by genre (referring to ‚ÄúFighting‚Äù genre). ","date":"2021-11-01","objectID":"/posts/evga/:2:4","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Content control warnings 1994 - Foundation of Entertainment Software Rating Board\rAfter titles like Doom, Wolfenstein and Mortal Kombat, an American system arises to classify the content of video games, and assign it a category depending on its potential audience maturity. It was established in 1994 by the Entertainment Software Association, the formerly called the Interactive Digital Software Association. # ESRB Rating Dataframe df_r = df[df[\"Rating\"].isna() == False] df_r = df_r.groupby([\"Rating\",\"Platform\"]).count()[\"Name\"] \\ .reset_index() \\ .pivot_table(values = \"Name\", index = \"Rating\", columns = \"Platform\", aggfunc = [np.sum]) \\ .fillna(0) # Drop empty classifications df_r = df_r.drop([\"AO\",\"EC\",\"K-A\",\"RP\"]) # Heatmap of ESRB Rating vs Consoles gesrb = px.imshow(df_r.reset_index(drop=True), x= [\"3DS\",\"DC\",\"DS\",\"GBA\",\"GC\",\"PC\",\"PS\",\"PS2\",\"PS3\",\"PS4\",\"PSP\",\"PSV\", \"Wii\",\"WiiU\",\"Xbox 360\",\"Xbox\",\"Xbox One\"], y= ['E', 'E10+', 'M', 'T'], labels=dict(x=\"Console\", y=\"ESRB Rating\", color=\"Number of Titles\"), color_continuous_scale='BuGn', title = \"Heatmap of ESRB Rating vs Consoles updated to 2016\") # Update layout gesrb.update_layout(layout) gesrb.update_annotations(sign) gesrb.show() \rBased on the classification established by the ESRB, from the data available it can be concluded that the video game console with more titles for universal use is the Nintendo DS, followed by the PS2 and then is the Wii, thus highlighting the impact they had on sales, which it will be shown later. Meanwhile, the Xbox 360 and PS3 were geared towards a more mature audience with a significant presence of M-rated titles. ","date":"2021-11-01","objectID":"/posts/evga/:2:5","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Last years of 32 bit era In the early 2000s, after the launch of the PS1, Sony continued leading the console market and diversifying its portfolio of games. On the other side of the coin SEGA, despite having launched the first console with an online system, in 2002 they retired their console from the market and dedicated itself exclusively to third-party development and Arcade, a situation that is outlined in the following graph. # Lineplot sales by platform before 2005 # Extract columns games20 = df[[\"Year_of_Release\",\"Platform\",\"Global_Sales\"]] # Subset dates games20 = games20[(games20.Year_of_Release \u003e 1998) \u0026 (games20.Year_of_Release \u003c 2005)] # Omit WonderSwan by Bandai, SEGA Saturn due low sales profiles and NDS that is not # relevant yet games20 = games20[~games20.Platform.isin([\"WS\",\"DS\",\"SAT\",\"SNES\",\"PSP\"])] # Group and summarize games20 = games20.groupby([\"Year_of_Release\",\"Platform\"]).agg(sum).reset_index()\\ .sort_values([\"Year_of_Release\",\"Platform\"], ascending=True) # Save an Array of Platforms Platforms = games20.Platform.unique() # Pivot to put in long format games20 = games20.pivot_table(values=\"Global_Sales\", index=\"Year_of_Release\", columns=\"Platform\").reset_index() # Assemble lineplot line20 = go.Figure() for platform in Platforms: line20.add_trace(go.Scatter(x = games20[\"Year_of_Release\"], y = games20[platform], name=platform, line_shape='linear')) # Update layout line20.update_layout(layout) line20.update_annotations(sign) line20.show() \rThe Japanese domain was becoming more and more determined, a situation that the software technology giant, Microsoft, takes as a challenge to enter a new market and start a contest with the PS2. 2000 - The beginning of the longest rivalry in the console market\rThis famous image of Bill Gates with Dwayne Johnson was part of a great marketing strategy carried out by Microsoft, they were willing to do whatever it took to strengthen the presence of Xbox in the market. Microsoft‚Äôs vision was to standardize the game hardware so that it was as similar as possible to a PC, so they implemented Direct X, an Intel Pentium III, a 7.3GFLOPS Nvidia GPU and an 8GB hard drive, trying to secure a significant advantage over the competitors. At this time, Nintendo announced the GameCube as a console contender, but it was not very successful, a situation that was neutralized with the sales of the Game Boy Advance within the portable market. Nevertheless, the PS2 led the first part of the decade in terms of sales, while Xbox got the second place as we saw in the last graph. And of course, that was a very expensive silver medal, according to Vladimir Cole from Joystiq, Forbes estimated around $4 billion in total lost after that trip, but at the same time they proved that they could compete with the Japanese Ruler of that time. # Mask of 2000-2004 games games2004 = (df.Year_of_Release \u003e 1999) \u0026 (df.Year_of_Release \u003c 2005) # Array to Subset publishers with hightest sales from 2000 to 2004 toppub2004 = df[games2004].groupby\\ ([\"Publisher\"])[\"Global_Sales\"].agg(sum).reset_index()\\ .sort_values(\"Global_Sales\",ascending=False).head(15) # New DF with top Titles per Publisher toppub = df[games2004 \u0026 df.Publisher.isin(toppub2004.Publisher)]\\ .sort_values([\"Publisher\",\"Name\"]) # Substitute empty scores with the mean toppub.Critic_Score = toppub.Critic_Score.fillna(toppub.Critic_Score.mean()) # Top 3 toppub3 = toppub.sort_values([\"Publisher\",\"Global_Sales\"], ascending = False)\\ .groupby(\"Publisher\")[\"Name\",\"Year_of_Release\",\"Publisher\",\"Global_Sales\", \"Critic_Score\", \"Country\",\"City\"].head(3)\\ .sort_values(\"Global_Sales\", ascending=True) # Bubble plot bubpub3 = px.scatter(toppub3, y=\"Publisher\", x=\"Global_Sales\", size=\"Critic_Score\", color=\"Critic_Score\", hover_name=\"Name\", color_continuous_scale=px.colors.sequential.Greens, labels={\"Global_Sales\":\"Millions of units sold\", \"Critic_Score\":\"Metacritic value\"}) # Add reference line bubpub3.add_vrect(x0 = 8.0, x1 = 8.98, y0= 0.32, y1=0.44, line_width=0, fillco","date":"2021-11-01","objectID":"/posts/evga/:2:6","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"The ‚ÄúNon-competitor‚Äù takes the lead 2005 - Microsoft launch the Xbox 360\r2006 - PS3 and Nintendo Wii are released\rAs Microsoft and Sony continued competing for a market with high-definition titles, online connection services like Xbox Live and PSN, and high-capacity hard drives, Nintendo chose to follow a Blue Ocean Strategy after the failure of the GameCube, who tried to compete in the big leagues. Their strategy consisted of offering something new and innovative, instead of competing to be better in the characteristics offered by the competition, becoming the fastest selling console, reaching to sell 50 million units around the world according to D. Melanson from Verizon Media, so this is the best way to describe the Wii console. # Mask of 2005-2010 games games2010 = (df.Year_of_Release \u003e 2004) \u0026 (df.Year_of_Release \u003c 2011) # Dataframe of games df2010 = df[games2010] # Excluding data to focus on new consoles df2010 = df2010[df2010.Platform.isin(['Wii','DS','X360','PS3'])]\\ .groupby([\"Platform\",\"Year_str\"])[\"Global_Sales\"].agg(sum).reset_index()\\ .sort_values([\"Year_str\",\"Global_Sales\"]) # Plot of Sales by Platform bar2010 = px.bar(df2010, color=\"Platform\", y=\"Global_Sales\", x=\"Year_str\", barmode=\"group\", labels={\"Year_str\":\"Year\", \"Global_Sales\":\"Millions of Units\"}, pattern_shape=\"Platform\", pattern_shape_sequence=[\"\", \"\", \"\", \"\", \".\"], color_discrete_sequence=[\"#00DEB7\",\"#0082C2\",\"#1333A7\",\"#5849B6\"]) # Update layout bar2010.update_layout(layout) bar2010.update_annotations(sign) bar2010.show() \rAs you can see, from the start of the Wii sales, the strategy of Nintendo began to flourish, surpassing the sales of its rivals by 4 years in a row. An interesting aspect of Nintendo among the others, is that the success of their sales was due to exclusive titles involving their unique accessories with motion sensors. Referring to sales, among the most successful titles are the following. # Dataframe for table with best-selling games table_data = df[games2010] table_data = table_data[table_data.Platform.isin(['Wii','DS','X360','PS3'])]\\ .sort_values([\"Year_str\",\"Platform\",\"Global_Sales\"], ascending=False)\\ .reset_index()\\ .groupby([\"Platform\",\"Year_str\"]).head(1)\\ .sort_values(\"Platform\", ascending = False) table_data = table_data[[\"Year_str\",\"Name\",\"Publisher\",\"Platform\",\"Global_Sales\"]] # Plot of Table table10 = go.Figure(data=[go.Table( header=dict(values=list([\"Year\",\"Game title\", \"Platform\", \"Publisher\",\"Units Sold\"]), fill_color='#5849B6', align=\"center\"), cells=dict(values=[table_data.Year_str, table_data.Name, table_data.Platform, table_data.Publisher, np.round(table_data.Global_Sales * 1000000,0)], fill_color='lavender', align=list(['center', 'left', 'center', 'left', 'right'])))]) # Update layout table10.update_layout(layout) table10.update_traces({\"header\":{\"font.color\":\"#fcfcfc\", \"font.size\":fontimg+3}}) table10.update_annotations(sign) table10.show() \rFour of Wii‚Äôs five most successful titles involve Nintendo Publishers, among its most famous IPs were Mario Kart and Wii Sports. This innovation marked an era of hardware extensions and motion sensors, a situation that Activision was able to take advantage of, when acquiring Red Octane, reaching around 13 titles of the IP known as Guitar Hero until 2009, being sold with its flagship item that imitated a Gibson SG. ","date":"2021-11-01","objectID":"/posts/evga/:2:7","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Prevalence of Social Gaming After the success of some local-gaming titles, the decade from 2010 to 2020 took a more competitive or cooperative way in certain cases, guided by a new era of interconnectivity and mobility. This reason motivated developers with extraordinary visions to create not only multiplayer, but also online content that maintains high audience rates. # Subset games from 2010 to 2015 games2010 = df[(df.Year_of_Release \u003e 2009) \u0026 (df.Year_of_Release \u003c 2016)]\\ .sort_values([\"Year_str\",\"Platform\",\"Global_Sales\"]) # Subset games with more sales from 2010 to 2015 topgames2010 = games2010.sort_values([\"Genre\",\"Global_Sales\"], ascending = False)\\ .groupby(\"Genre\").head(1).sort_values(\"Year_of_Release\", ascending = True)\\ .sort_values(\"Global_Sales\", ascending=False) topgames2010 = topgames2010[[\"Year_of_Release\",\"Name\",\"Platform\",\"Publisher\",\"Genre\", \"Global_Sales\"]] # Barplot Base bargen10 = px.bar(topgames2010, y=\"Genre\", x=\"Global_Sales\", orientation=\"h\", text=\"Name\", labels={\"Name\":\"Title\", \"Global_Sales\":\"Millions of units sold\"}, color=\"Genre\", color_discrete_sequence=[\"#8B58B0\",\"#58B081\",\"#B0B058\",\"#535353\", \"#B05858\",\"#58B09E\",\"#B05890\",\"#587FB0\", \"#B05858\",\"#58B0AA\",\"#686868\",\"#C3A149\"]) bargen10.update_traces(textposition='inside', marker_line_color='#404040', textfont = {\"color\":\"#FFFFFF\",\"family\": \"segoe ui\"}, marker_line_width=1, opacity=0.7) # Update layout bargen10.update_layout(layout) bargen10.update_annotations(sign) bargen10.show() \rBetween 2010 and 2015, the best-selling title was Kinect Adventures for Xbox 360, which had a focus on enhancing multiplayer gameplay and taking advantage of the latest technological innovation of the moment, the Microsoft‚Äôs Kinect. The second best-selling title at that time was Grand Theft Auto V for PS3, which to this day continues to prevail as one of the online systems with the largest number of users in the industry. Their vision went beyond creating an Open-World game, they had the intention of creating a dynamic online content structure, which provides seasonal content. This type of model also motivated Publishers such as Epic Games and Activision, to innovate but in this case not selling games but focusing on aesthetics, where game content is offered as an extra to the online service without having to be paid as a DLC. #pubgen # Publishers with more sales in history toppubarray = df.groupby(\"Publisher\")[\"Global_Sales\"].agg(sum).reset_index()\\ .sort_values(\"Global_Sales\", ascending= False)\\ .head(len(df.Genre.unique()))[\"Publisher\"] # Extract publisher from raw df puball = df[df.Publisher.isin(toppubarray)].groupby([\"Publisher\",\"Name\",\"Genre\"]).agg(sum) # Add a column of 1s puball[\"counter\"] = np.ones(puball.shape[0]) # Create the pivot table puball = puball.pivot_table(\"counter\", index = \"Publisher\", columns=\"Genre\", aggfunc=\"sum\") # Display rounded values pd.options.display.float_format = '{:,.0f}'.format pubmatrix = ff.create_annotated_heatmap(puball.values, x=puball.columns.tolist(), y=puball.index.tolist(), annotation_text= np.around(puball.values, decimals=0), colorscale='sunset') # Update layout pubmatrix.update_layout(layout) # Extra annotation to avoid overlapping of layers pubmatrix.add_annotation(text=author, align= \"right\", visible = True, xref=\"paper\", yref=\"paper\", x= 1, y= -0.11, showarrow=False, font={\"size\":fontimg-1}) pubmatrix.update_annotations(sign) pubmatrix.show() \rThe fact that every day more Free to Play games are announced, does not indicate that this is the exclusive focus companies will have on the industry now on. Beyond this, as we see in the previous graph, each of the most recognized Publishers in history has its own style in exclusive series, despite having titles in many genres. Even today, large companies like Microsoft offer services such as Xbox GamePass, with subscriptions that offer big catalogs of games, which even include titles from Independent Developers, supporting their growth through advertising systems, helping to in","date":"2021-11-01","objectID":"/posts/evga/:2:8","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Projects"],"content":"Summary The video game industry, beyond having started as a simple experiment at MIT, is a lifestyle for many. Like any market, it has had its moments of glory and its difficulties, but if we can rescue something, it is that the secret of its success lies in the emotional bond it generates with its customers. Through this article, my goal is to use data tools to inform the reader about curious events, which perhaps they did not know. I want to thank you for taking the time to read carefully. As a curious detail, there are no easter eggs üòâ Good luck! Additional Information About the article This infographic article was adapted to a general public, I hope you enjoyed it. In case you are interested in learning more about the development of the script, I invite you to the contact section in my About Page, and with pleasure we can connect. Related Content As a recommendation I suggest a look at this great article, published by Omri Wallach on the Visual Capitalist infographic page, where many interesting details about the industry‚Äôs history are covered. Datasets Videogames Dataset Videogames Developers Regions Indie Developers Regions Footnote: Specific datasets contain information from Publishers, which they were named in the source attribute as Developers, but not in all cases. For more details on the data transformation, please visit my Github repository.¬†‚Ü©Ô∏é ","date":"2021-11-01","objectID":"/posts/evga/:3:0","tags":["Exploratory Analysis","Python"],"title":"Video Games History explained with Pandas","uri":"/posts/evga/"},{"categories":["Articles"],"content":"Cloud Service Architecture for NLP in Google Cloud","date":"2021-10-04","objectID":"/posts/gcp_academicsolution/","tags":["Big Data","Google Cloud Services"],"title":"Cloud Service Architecture for NLP in Google Cloud","uri":"/posts/gcp_academicsolution/"},{"categories":["Articles"],"content":"Summary: This academic article was developed through my postgraduate course, DAT-03 Data Analysis in Big Data Environments, which presents a business case in which a company is interested in combining and migrating relational databases in Microsoft SQL Server and Oracle DB. Given the need to feed them with an NLP system output of social networks such as Instagram, Telegram, and Facebook Messenger. Initially, the vulnerabilities that the company has when managing its data in a Local Storage are shown, where it is compared with the advantages of implementing a Cloud Service. This service is compared with others such as Microsoft Azure and Amazon Cloud Services, showing why this solution is the one that best suits the business case. An architecture diagram was designed, covering the required APIs to the ingestion, storage, processing, and loading of the data in BI tools to ease access to the end-user. Finally, financial aspects are covered in terms of operating costs for the proposed solution. In case you want to access it, next you will find a link where it was uploaded to my LinkedIn. Click HERE to access or download. \r","date":"2021-10-04","objectID":"/posts/gcp_academicsolution/:0:1","tags":["Big Data","Google Cloud Services"],"title":"Cloud Service Architecture for NLP in Google Cloud","uri":"/posts/gcp_academicsolution/"},{"categories":["Others"],"content":"Medium Blog for Analytics and more","date":"2021-09-27","objectID":"/posts/firstmediumpost/","tags":["Medium","Blog"],"title":"Medium Blog for Analytics","uri":"/posts/firstmediumpost/"},{"categories":["Others"],"content":"Perhaps you have noticed in my about section that I have always been interested in documenting my work and write about topics of interest. However, I will use this platform to write about Data Science and Game Analytics plus other interesting topics. Source: Author\" Source: Author \rJust to be in context, I‚Äôll use Medium as side blog for Data Analytics related topics, so on this Website, I will post my projects. Now my next challenge is to figure out how to merge each of these areas, and to follow my progress, here is the Medium hyperlink and some external links from Twitter. Just published an article about how to clean your tweets, extract entities like emojis, hashtags, and mentions using data from @thegameawards 2021, using #tweepyhttps://t.co/xw0GMqRwKk ‚Äî Roberto Aguilar (@robguilarr) November 18, 2021 \rWant to learn how to make queries to the Twitter API using #tweepy? Learn how-to while fetching all those tweets you didn't want to see.https://t.co/PDQP9BjZFS ‚Äî Roberto Aguilar (@robguilarr) November 7, 2021 \rToday I published a quick example about how to fetch Twitter user data using Fork Parker profile as example. #tweepyhttps://t.co/m5eyXgpxJJ ‚Äî Roberto Aguilar (@robguilarr) November 3, 2021 \rHello world! Today I made my first Medium post about #Data Analytics. In the next few days, I'll continue uploading some content to document my path.https://t.co/Z7mi8C6bWv ‚Äî Roberto Aguilar (@robguilarr) November 1, 2021 \r","date":"2021-09-27","objectID":"/posts/firstmediumpost/:0:0","tags":["Medium","Blog"],"title":"Medium Blog for Analytics","uri":"/posts/firstmediumpost/"},{"categories":null,"content":"Contact platforms and information about the author from his background to his personal interests you can access them here","date":"2021-08-02","objectID":"/about/","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"¬†Important: Any views, material or statements expressed are mines and not those of my employer ¬†About my career \rA few years ago I started studying Industrial Engineering, meanwhile, as a hobby, I started learning how to develop mobile games in Unity, because video games have always been one of my favorite hobbies. After a year, I started to lose interest in development. Soon after, a friend told me about Data Science, but at first I wasn‚Äôt interested. However, one of my favorite subjects has always been statistics and probability, and I discovered that some people in the Software Entertainment industry were dedicated to retaining and acquiring users, and analyzing live metrics to improve UX and benefit the business itself. And most of those people have backgrounds associated with Analytics careers like Data Science, which is why I started to get motivated to pursue this path. \r ¬†Education Industrial Production Engineering at TEC Bachelor‚Äôs degree accredited by the Canadian Engineering Accreditation Board (CEAB) in the area of Industrial Engineering. Covering areas such as industrial statistics (Minitab), process simulation, computer aided design (CAD, Solid Works), computer aided manufacturing (CAM), industrial process control and programming (CNC), IT project management fundamentals (SCRUM), continuous improvement processes (Lean Six Sigma). More info here. Data Analyst at Cenfotec Postgraduate as Data Analyst, managing topics such as Data Storytelling in Python with Jupyter Notebooks, ETL with Azure Data Factory, Web Scraping \u0026 API connections with Python Selenium, Data Governance fundamentals, data modeling in Apache Hadoop using HiveQL and visualization of statistical models with D3.js, Tableau and Power BI. More info here. ¬†Experience Enterprise Data \u0026 Analytics Intern at VMware Created and migrated data analytics reports of Customer Success area in whole SaaS cycle, embracing all the Customer 360¬∞ environment. As well as the CTA predictive analysis according to the PEME needs of early warnings subscription consumption; via commercial, and financial indicators applied to Big Data. More info at LinkedIn June 2020 - September 2020 Business Intelligence Analyst at Belcorp Through prescriptive analytics applied in the field of BI within the B2C sales area, commercial sales variables were managed and reported. In order to create and consult dynamic and real-time reports / dashboards that will make timely decisions for Central American sales plans, according to the needs of sales management. More info at LinkedIn February 2021 - July 2021 Junior Data Analyst at McKinsey \u0026 Company Analyst in charge of coding/scripting (SQL, Python, etc.) for automating data transformation and loading. Also, responsible for performing data ingestion, transformation, and uploading into applications. This entailed the use of application administration tools (combination of proprietary tools), validated raw data quality (develop test cases, battery of QA scripts), and took corrective measures in case of issues. More info at LinkedIn March 2022 - Present ¬†What can I offer I‚Äôm not an expert, but a curious mind. As one of my favorite authors Austin Kleon mentions, ‚ÄúYou don‚Äôt have to be a genius ‚Ä¶. Sometimes amateurs have more to teach us than experts‚Äù, they are always willing to learn keeping the contraints out of mind. For this reason, I want to show my work, eager to get feedback from people or groups who share the same interests as me. I‚Äôm a Data Analyst with Engineering skills and a self-learning attitude. With experience in consulting, consumer goods, and virtualization technology industries. Who as an add-on has great enthusiasm for the Software Entertainment industry, in how data analytics can improve it as a business, using pre-development analytics, live metrics, and acquisition/retention analysis; for indie markets and big publishers. ¬†Personal life \u0026 Hobbies I‚Äôm a person who love to read to issues related to video games industry, especially","date":"2021-08-02","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]